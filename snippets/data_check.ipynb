{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23132425, 4) (439298, 4) (3739, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inputdir = '../data/'\n",
    "# Load the data\n",
    "train_data = pd.read_csv(inputdir + 'loop_sensor_train.csv')\n",
    "test_data = pd.read_csv(inputdir + 'loop_sensor_test_x.csv')\n",
    "metadata = pd.read_csv(inputdir + 'geo_reference.csv', delimiter=';')\n",
    "\n",
    "print(train_data.shape, test_data.shape, metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3348\n"
     ]
    }
   ],
   "source": [
    "unique_iu_ac_count = metadata['iu_ac'].nunique()\n",
    "\n",
    "print(unique_iu_ac_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有重复的 iu_ac 值及其个数:\n",
      "iu_ac\n",
      "5113    6\n",
      "5469    6\n",
      "5470    4\n",
      "5909    4\n",
      "5908    4\n",
      "       ..\n",
      "6605    2\n",
      "6846    2\n",
      "1595    2\n",
      "6090    2\n",
      "579     2\n",
      "Name: count, Length: 338, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 获取 'iu_ac' 列值的出现次数\n",
    "value_counts = metadata['iu_ac'].value_counts()\n",
    "# 筛选出出现次数大于1的值\n",
    "duplicates = value_counts[value_counts > 1]\n",
    "\n",
    "print('有重复的 iu_ac 值及其个数:')\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有 iu_ac 值为 5113 的行:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iu_ac</th>\n",
       "      <th>date_debut</th>\n",
       "      <th>date_fin</th>\n",
       "      <th>libelle</th>\n",
       "      <th>iu_nd_aval</th>\n",
       "      <th>libelle_nd_aval</th>\n",
       "      <th>iu_nd_amont</th>\n",
       "      <th>libelle_nd_amont</th>\n",
       "      <th>geo_point_2d</th>\n",
       "      <th>geo_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>5113</td>\n",
       "      <td>2005-01-01T01:00:00+00:00</td>\n",
       "      <td>2019-06-01T02:00:00+00:00</td>\n",
       "      <td>Pl_de_la_Nation</td>\n",
       "      <td>2697</td>\n",
       "      <td>Pl_de_la_Nation-Face_14</td>\n",
       "      <td>2696</td>\n",
       "      <td>Pl_de_la_Nation-Face_7</td>\n",
       "      <td>48.84866388585416, 2.3950366932731195</td>\n",
       "      <td>{\"coordinates\": [[2.395157455428554, 48.848797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>5113</td>\n",
       "      <td>2005-01-01T01:00:00+00:00</td>\n",
       "      <td>2019-06-01T02:00:00+00:00</td>\n",
       "      <td>Pl_de_la_Nation</td>\n",
       "      <td>2697</td>\n",
       "      <td>Pl_de_la_Nation-Face_14</td>\n",
       "      <td>2696</td>\n",
       "      <td>Pl_de_la_Nation-Face_7</td>\n",
       "      <td>48.848896238567235, 2.395363216280779</td>\n",
       "      <td>{\"coordinates\": [[2.3955346585419752, 48.84896...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>5113</td>\n",
       "      <td>2005-01-01T01:00:00+00:00</td>\n",
       "      <td>2019-06-01T02:00:00+00:00</td>\n",
       "      <td>Pl_de_la_Nation</td>\n",
       "      <td>2697</td>\n",
       "      <td>Pl_de_la_Nation-Face_14</td>\n",
       "      <td>2696</td>\n",
       "      <td>Pl_de_la_Nation-Face_7</td>\n",
       "      <td>48.84811422337558, 2.395019825888174</td>\n",
       "      <td>{\"coordinates\": [[2.394913407543069, 48.848251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>5113</td>\n",
       "      <td>2005-01-01T01:00:00+00:00</td>\n",
       "      <td>2019-06-01T02:00:00+00:00</td>\n",
       "      <td>Pl_de_la_Nation</td>\n",
       "      <td>2697</td>\n",
       "      <td>Pl_de_la_Nation-Face_14</td>\n",
       "      <td>2696</td>\n",
       "      <td>Pl_de_la_Nation-Face_7</td>\n",
       "      <td>48.84839118546178, 2.394903628519354</td>\n",
       "      <td>{\"coordinates\": [[2.3949030914694833, 48.84849...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>5113</td>\n",
       "      <td>1996-10-21T02:00:00+00:00</td>\n",
       "      <td>2023-01-01T01:00:00+00:00</td>\n",
       "      <td>Pl_de_la_Nation</td>\n",
       "      <td>2697</td>\n",
       "      <td>Pl_de_la_Nation-Face_14</td>\n",
       "      <td>2696</td>\n",
       "      <td>Pl_de_la_Nation-Face_7</td>\n",
       "      <td>48.84787203820565, 2.3954544144291137</td>\n",
       "      <td>{\"coordinates\": [[2.395126244233279, 48.847977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>5113</td>\n",
       "      <td>2005-01-01T01:00:00+00:00</td>\n",
       "      <td>2019-06-01T02:00:00+00:00</td>\n",
       "      <td>Pl_de_la_Nation</td>\n",
       "      <td>2697</td>\n",
       "      <td>Pl_de_la_Nation-Face_14</td>\n",
       "      <td>2696</td>\n",
       "      <td>Pl_de_la_Nation-Face_7</td>\n",
       "      <td>48.84899126306357, 2.395814300188512</td>\n",
       "      <td>{\"coordinates\": [[2.396043668646993, 48.848999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      iu_ac                 date_debut                   date_fin  \\\n",
       "550    5113  2005-01-01T01:00:00+00:00  2019-06-01T02:00:00+00:00   \n",
       "1877   5113  2005-01-01T01:00:00+00:00  2019-06-01T02:00:00+00:00   \n",
       "2216   5113  2005-01-01T01:00:00+00:00  2019-06-01T02:00:00+00:00   \n",
       "2217   5113  2005-01-01T01:00:00+00:00  2019-06-01T02:00:00+00:00   \n",
       "2717   5113  1996-10-21T02:00:00+00:00  2023-01-01T01:00:00+00:00   \n",
       "3352   5113  2005-01-01T01:00:00+00:00  2019-06-01T02:00:00+00:00   \n",
       "\n",
       "              libelle  iu_nd_aval          libelle_nd_aval  iu_nd_amont  \\\n",
       "550   Pl_de_la_Nation        2697  Pl_de_la_Nation-Face_14         2696   \n",
       "1877  Pl_de_la_Nation        2697  Pl_de_la_Nation-Face_14         2696   \n",
       "2216  Pl_de_la_Nation        2697  Pl_de_la_Nation-Face_14         2696   \n",
       "2217  Pl_de_la_Nation        2697  Pl_de_la_Nation-Face_14         2696   \n",
       "2717  Pl_de_la_Nation        2697  Pl_de_la_Nation-Face_14         2696   \n",
       "3352  Pl_de_la_Nation        2697  Pl_de_la_Nation-Face_14         2696   \n",
       "\n",
       "            libelle_nd_amont                           geo_point_2d  \\\n",
       "550   Pl_de_la_Nation-Face_7  48.84866388585416, 2.3950366932731195   \n",
       "1877  Pl_de_la_Nation-Face_7  48.848896238567235, 2.395363216280779   \n",
       "2216  Pl_de_la_Nation-Face_7   48.84811422337558, 2.395019825888174   \n",
       "2217  Pl_de_la_Nation-Face_7   48.84839118546178, 2.394903628519354   \n",
       "2717  Pl_de_la_Nation-Face_7  48.84787203820565, 2.3954544144291137   \n",
       "3352  Pl_de_la_Nation-Face_7   48.84899126306357, 2.395814300188512   \n",
       "\n",
       "                                              geo_shape  \n",
       "550   {\"coordinates\": [[2.395157455428554, 48.848797...  \n",
       "1877  {\"coordinates\": [[2.3955346585419752, 48.84896...  \n",
       "2216  {\"coordinates\": [[2.394913407543069, 48.848251...  \n",
       "2217  {\"coordinates\": [[2.3949030914694833, 48.84849...  \n",
       "2717  {\"coordinates\": [[2.395126244233279, 48.847977...  \n",
       "3352  {\"coordinates\": [[2.396043668646993, 48.848999...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选出 'iu_ac' 值为 5113 的所有行\n",
    "filtered_rows = metadata[metadata['iu_ac'] == 5113]\n",
    "\n",
    "print('所有 iu_ac 值为 5113 的行:')\n",
    "filtered_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "统计列 'iu_ac':\n",
      "iu_ac\n",
      "5113    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'date_debut':\n",
      "date_debut\n",
      "2005-01-01T01:00:00+00:00    5\n",
      "1996-10-21T02:00:00+00:00    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'date_fin':\n",
      "date_fin\n",
      "2019-06-01T02:00:00+00:00    5\n",
      "2023-01-01T01:00:00+00:00    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'libelle':\n",
      "libelle\n",
      "Pl_de_la_Nation    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'iu_nd_aval':\n",
      "iu_nd_aval\n",
      "2697    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'libelle_nd_aval':\n",
      "libelle_nd_aval\n",
      "Pl_de_la_Nation-Face_14    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'iu_nd_amont':\n",
      "iu_nd_amont\n",
      "2696    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'libelle_nd_amont':\n",
      "libelle_nd_amont\n",
      "Pl_de_la_Nation-Face_7    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'geo_point_2d':\n",
      "geo_point_2d\n",
      "48.84866388585416, 2.3950366932731195    1\n",
      "48.848896238567235, 2.395363216280779    1\n",
      "48.84811422337558, 2.395019825888174     1\n",
      "48.84839118546178, 2.394903628519354     1\n",
      "48.84787203820565, 2.3954544144291137    1\n",
      "48.84899126306357, 2.395814300188512     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "统计列 'geo_shape':\n",
      "geo_shape\n",
      "{\"coordinates\": [[2.395157455428554, 48.8487972815553], [2.394915931117685, 48.84853049015301]], \"type\": \"LineString\"}       1\n",
      "{\"coordinates\": [[2.3955346585419752, 48.848968644050075], [2.395191774019583, 48.8488238330844]], \"type\": \"LineString\"}     1\n",
      "{\"coordinates\": [[2.394913407543069, 48.84825138987332], [2.395126244233279, 48.847977056877845]], \"type\": \"LineString\"}     1\n",
      "{\"coordinates\": [[2.3949030914694833, 48.84849553028741], [2.394904165569226, 48.84828684063615]], \"type\": \"LineString\"}     1\n",
      "{\"coordinates\": [[2.395126244233279, 48.847977056877845], [2.3957825846249485, 48.84776701953345]], \"type\": \"LineString\"}    1\n",
      "{\"coordinates\": [[2.396043668646993, 48.84899997591223], [2.3955849317300304, 48.84898255021491]], \"type\": \"LineString\"}     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选出 'iu_ac' 值为 5113 的所有行\n",
    "filtered_rows = metadata[metadata['iu_ac'] == 5113]\n",
    "\n",
    "# 输出每一列的不同值及其计数，观察差异\n",
    "for column in filtered_rows.columns:\n",
    "    print(f\"统计列 '{column}':\")\n",
    "    print(filtered_rows[column].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "sw = SummaryWriter(logdir='.', flush_secs=5)\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.typing import OptTensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.transforms import LaplacianLambdaMax\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, get_laplacian\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_scatter import scatter_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphdata_channel1(graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size,\n",
    "                            shuffle=True, DEVICE = torch.device('cuda:0')):\n",
    "    '''\n",
    "    :param graph_signal_matrix_filename: str\n",
    "    :param num_of_hours: int\n",
    "    :param num_of_days: int\n",
    "    :param num_of_weeks: int\n",
    "    :param DEVICE:\n",
    "    :param batch_size: int\n",
    "    :return:\n",
    "    three DataLoaders, each dataloader contains:\n",
    "    test_x_tensor: (B, N_nodes, in_feature, T_input)\n",
    "    test_decoder_input_tensor: (B, N_nodes, T_output)\n",
    "    test_target_tensor: (B, N_nodes, T_output)\n",
    "    '''\n",
    "    file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "    filename = os.path.join('../input/processing-traffic-data-for-deep-learning-projects/', file + '_r' + str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) +'_astcgn'\n",
    "    print('load file:', filename)\n",
    "\n",
    "    file_data = np.load(filename + '.npz')\n",
    "    train_x = file_data['train_x']  # (10181, 307, 3, 12)\n",
    "    train_x = train_x[:, :, 0:1, :]\n",
    "    train_target = file_data['train_target']  # (10181, 307, 12)\n",
    "\n",
    "    val_x = file_data['val_x']\n",
    "    val_x = val_x[:, :, 0:1, :]\n",
    "    val_target = file_data['val_target']\n",
    "\n",
    "    test_x = file_data['test_x']\n",
    "    test_x = test_x[:, :, 0:1, :]\n",
    "    test_target = file_data['test_target']\n",
    "\n",
    "    mean = file_data['mean'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "    std = file_data['std'][:, :, 0:1, :]  # (1, 1, 3, 1)\n",
    "\n",
    "    # ------- train_loader -------\n",
    "    train_x_tensor = torch.from_numpy(train_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    train_target_tensor = torch.from_numpy(train_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_target_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # ------- val_loader -------\n",
    "    val_x_tensor = torch.from_numpy(val_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    val_target_tensor = torch.from_numpy(val_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_x_tensor, val_target_tensor)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ------- test_loader -------\n",
    "    test_x_tensor = torch.from_numpy(test_x).type(torch.FloatTensor).to(DEVICE)  # (B, N, F, T)\n",
    "    test_target_tensor = torch.from_numpy(test_target).type(torch.FloatTensor).to(DEVICE)  # (B, N, T)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_target_tensor)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # print\n",
    "    print('train:', train_x_tensor.size(), train_target_tensor.size())\n",
    "    print('val:', val_x_tensor.size(), val_target_tensor.size())\n",
    "    print('test:', test_x_tensor.size(), test_target_tensor.size())\n",
    "\n",
    "    return train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_signal_matrix_filename = '../input/pems-dataset/data/PEMS04/PEMS04.npz'\n",
    "batch_size = 32\n",
    "num_of_weeks = 0\n",
    "num_of_days = 0\n",
    "num_of_hours = 1\n",
    "\n",
    "train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel1(\n",
    "    graph_signal_matrix_filename, num_of_hours, num_of_days, num_of_weeks, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(distance_df_filename, num_of_vertices, id_filename=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    distance_df_filename: str, path of the csv file contains edges information\n",
    "    num_of_vertices: int, the number of vertices\n",
    "    Returns\n",
    "    ----------\n",
    "    A: np.ndarray, adjacency matrix\n",
    "    '''\n",
    "    if 'npy' in distance_df_filename:  # false\n",
    "        adj_mx = np.load(distance_df_filename)\n",
    "        return adj_mx, None\n",
    "    else:\n",
    "        \n",
    "        #--------------------------------------------- read from here\n",
    "        import csv\n",
    "        A = np.zeros((int(num_of_vertices), int(num_of_vertices)),dtype=np.float32)\n",
    "        distaneA = np.zeros((int(num_of_vertices), int(num_of_vertices)), dtype=np.float32)\n",
    "\n",
    "        #------------ Ignore\n",
    "        if id_filename: # false\n",
    "            with open(id_filename, 'r') as f:\n",
    "                id_dict = {int(i): idx for idx, i in enumerate(f.read().strip().split('\\n'))}  # 把节点id（idx）映射成从0开始的索引\n",
    "                \n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[id_dict[i], id_dict[j]] = 1\n",
    "                    distaneA[id_dict[i], id_dict[j]] = distance\n",
    "            return A, distaneA\n",
    "        else:\n",
    "         #-------------Continue reading\n",
    "            with open(distance_df_filename, 'r') as f:\n",
    "                f.readline()\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    if len(row) != 3:\n",
    "                        continue\n",
    "                    i, j, distance = int(row[0]), int(row[1]), float(row[2])\n",
    "                    A[i, j] = 1\n",
    "                    distaneA[i, j] = distance\n",
    "            return A, distaneA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_filename = None\n",
    "adj_filename = '../input/pems-dataset/data/PEMS04/PEMS04.csv'\n",
    "num_of_vertices = 307\n",
    "adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename) #  adj_mx and distance_mx (307, 307)\n",
    "\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "gr = nx.Graph()\n",
    "gr.add_edges_from(edges)\n",
    "nx.draw(gr, node_size=3)\n",
    "plt.show()\n",
    "rows, cols = np.where(adj_mx == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "edge_index_data = torch.LongTensor(np.array([rows, cols])).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.attention import ASTGCN   # For information about the architecture check the source code\n",
    "\n",
    "nb_block = 2\n",
    "in_channels = 1\n",
    "K = 3\n",
    "nb_chev_filter = 64\n",
    "nb_time_filter = 64\n",
    "time_strides = num_of_hours\n",
    "num_for_predict = 12\n",
    "len_input = 12\n",
    "#L_tilde = scaled_Laplacian(adj_mx)\n",
    "#cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "net = ASTGCN( nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, num_for_predict, len_input, num_of_vertices).to(DEVICE)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "print('Net\\'s state_dict:')\n",
    "total_param = 0\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, '\\t', net.state_dict()[param_tensor].size(), '\\t', net.state_dict()[param_tensor].device)\n",
    "    total_param += np.prod(net.state_dict()[param_tensor].size())\n",
    "print('Net\\'s total params:', total_param)\n",
    "#--------------------------------------------------\n",
    "print('Optimizer\\'s state_dict:')\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, '\\t', optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mae(preds, labels, null_val=np.nan):\n",
    "    if np.isnan(null_val):\n",
    "        mask = ~torch.isnan(labels)\n",
    "    else:\n",
    "        mask = (labels != null_val)\n",
    "    mask = mask.float()\n",
    "    mask /= torch.mean((mask))\n",
    "    mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "    loss = torch.abs(preds - labels)\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_flag=0\n",
    "criterion = nn.L1Loss().to(DEVICE)\n",
    "criterion_masked = masked_mae\n",
    "loss_function = 'mse'\n",
    "\n",
    "metric_method = 'unmask'\n",
    "missing_value=0.0\n",
    "\n",
    "\n",
    "if loss_function=='masked_mse':\n",
    "    criterion_masked = masked_mse         #nn.MSELoss().to(DEVICE)\n",
    "    masked_flag=1\n",
    "elif loss_function=='masked_mae':\n",
    "    criterion_masked = masked_mae\n",
    "    masked_flag = 1\n",
    "elif loss_function == 'mae':\n",
    "    criterion = nn.L1Loss().to(DEVICE)\n",
    "    masked_flag = 0\n",
    "elif loss_function == 'rmse':\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    masked_flag= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_loss_mstgcn(net, val_loader, criterion,  masked_flag,missing_value,sw, epoch, edge_index_data, limit=None):\n",
    "    '''\n",
    "    for rnn, compute mean loss on validation set\n",
    "    :param net: model\n",
    "    :param val_loader: torch.utils.data.utils.DataLoader\n",
    "    :param criterion: torch.nn.MSELoss\n",
    "    :param sw: tensorboardX.SummaryWriter\n",
    "    :param global_step: int, current global_step\n",
    "    :param limit: int,\n",
    "    :return: val_loss\n",
    "    '''\n",
    "    net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_loader_length = len(val_loader)  # nb of batch\n",
    "        tmp = []  # batch loss\n",
    "        for batch_index, batch_data in enumerate(val_loader):\n",
    "            encoder_inputs, labels = batch_data\n",
    "            outputs = net(encoder_inputs, edge_index_data)\n",
    "            if masked_flag:\n",
    "                loss = criterion(outputs, labels, missing_value)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            tmp.append(loss.item())\n",
    "            if batch_index % 100 == 0:\n",
    "                print('validation batch %s / %s, loss: %.2f' % (batch_index + 1, val_loader_length, loss.item()))\n",
    "            if (limit is not None) and batch_index >= limit:\n",
    "                break\n",
    "        validation_loss = sum(tmp) / len(tmp)\n",
    "        sw.add_scalar('validation_loss', validation_loss, epoch)\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_epoch = 0\n",
    "best_val_loss = np.inf\n",
    "start_time= time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(20):\n",
    "    params_filename = os.path.join('./', 'epoch_%s.params' % epoch)\n",
    "    masked_flag = 1\n",
    "    if masked_flag:\n",
    "        val_loss = compute_val_loss_mstgcn(net, val_loader, criterion_masked, masked_flag,missing_value,sw, epoch,edge_index_data)\n",
    "    else:\n",
    "        val_loss = compute_val_loss_mstgcn(net, val_loader, criterion, masked_flag, missing_value, sw, epoch,edge_index_data)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(net.state_dict(), params_filename)\n",
    "        print('save parameters to file: %s' % params_filename)\n",
    "\n",
    "    net.train()  # ensure dropout layers are in train mode\n",
    "\n",
    "    for batch_index, batch_data in enumerate(train_loader):\n",
    "        encoder_inputs, labels = batch_data   # encoder_inputs torch.Size([32, 307, 1, 12])  label torch.Size([32, 307, 12])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(encoder_inputs, edge_index_data) # torch.Size([32, 307, 12])\n",
    "\n",
    "        if masked_flag:\n",
    "            loss = criterion_masked(outputs, labels,missing_value)\n",
    "        else :\n",
    "            loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss = loss.item()\n",
    "        global_step += 1\n",
    "        sw.add_scalar('training_loss', training_loss, global_step)\n",
    "\n",
    "        if global_step % 200 == 0:\n",
    "            print('global step: %s, training loss: %.2f, time: %.2fs' % (global_step, training_loss, time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(False)  # ensure dropout layers are in evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_loader_length = len(test_loader)  # nb of batch\n",
    "    tmp = []  # batch loss\n",
    "    for batch_index, batch_data in enumerate(test_loader):\n",
    "        encoder_inputs, labels = batch_data\n",
    "        outputs = net(encoder_inputs, edge_index_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        tmp.append(loss.item())\n",
    "        if batch_index % 100 == 0:\n",
    "            print('test_loss batch %s / %s, loss: %.2f' % (batch_index + 1, test_loader_length, loss.item()))\n",
    "\n",
    "\n",
    "    test_loss = sum(tmp) / len(tmp)\n",
    "    sw.add_scalar('test_loss', test_loss, epoch)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = outputs[0]  # prediction\n",
    "sample_labels = labels[0] # truth\n",
    "print(sample_output.shape, sample_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(30,4), dpi=80)\n",
    "for i in range(50):\n",
    "    new_i = i * 12\n",
    "    plt.plot(range(0+new_i,12+new_i),sample_output[i].detach().cpu().numpy(), color = 'red')\n",
    "    plt.plot(range(0+new_i,12+new_i),sample_labels[i].cpu().numpy(), color='blue')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
