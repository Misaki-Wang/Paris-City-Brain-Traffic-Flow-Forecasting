{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "import datetime\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputdir='../data/'\n",
    "# # Load data\n",
    "# train_data = pd.read_csv(inputdir+'loop_sensor_train.csv')\n",
    "# baseline_data = pd.read_csv(inputdir+'loop_sensor_test_baseline.csv')\n",
    "# test_data_x = pd.read_csv(inputdir+'loop_sensor_test_x.csv')\n",
    "\n",
    "# # Prepare test data\n",
    "# test_data_x['t_1h'] = pd.to_datetime(test_data_x['t_1h'])\n",
    "# ref_time = pd.Timestamp('2022-01-01 00:00:00')\n",
    "# test_data_x['t_1h'] = (test_data_x['t_1h'] - ref_time).dt.total_seconds() / 3600.0\n",
    "\n",
    "# # Convert test data to tensor\n",
    "# test_features = torch.tensor(test_data_x[['iu_ac', 't_1h', 'etat_barre']].values).float().to(device)\n",
    "# # Convert time to datetime\n",
    "# train_data['t_1h'] = pd.to_datetime(train_data['t_1h'])\n",
    "# test_data_x['t_1h'] = pd.to_datetime(test_data_x['t_1h'])\n",
    "\n",
    "# # Preprocess and split data for year 2023\n",
    "# def preprocess_data(data):\n",
    "#     train = data\n",
    "#     for name, group in data[data['t_1h'].dt.year == 2023].groupby(pd.Grouper(key='t_1h', freq='29h')):\n",
    "#         train = pd.concat([train, group.iloc[:24]])\n",
    "#     return train\n",
    "\n",
    "# train     = preprocess_data(train_data)\n",
    "# eval_data = baseline_data\n",
    "# print(train.shape, eval_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = '../data/'\n",
    "# Load data\n",
    "train_data = pd.read_csv(f'{inputdir}loop_sensor_train.csv')\n",
    "eval_data = pd.read_csv(f'{inputdir}loop_sensor_eval.csv')\n",
    "test_data_x = pd.read_csv(f'{inputdir}loop_sensor_test_x.csv')\n",
    "\n",
    "def to_datetime(data):\n",
    "    data['t_1h'] = pd.to_datetime(data['t_1h'])\n",
    "\n",
    "to_datetime(train_data)\n",
    "to_datetime(eval_data)\n",
    "to_datetime(test_data_x)\n",
    "\n",
    "# Function to prepare training data for 2023\n",
    "def prepare_2023_data(data):\n",
    "    processed_data = data\n",
    "    # Split data for year 2023 into 24-hour frames and the following 5-hour as test\n",
    "    for name, group in data[data['t_1h'].dt.year == 2023].groupby(pd.Grouper(key='t_1h', freq='29h')):\n",
    "        processed_data = pd.concat([processed_data, group.iloc[:24]])\n",
    "    return processed_data\n",
    "train_data = prepare_2023_data(train_data)\n",
    "\n",
    "\n",
    "# Debugging the data shapes\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Evaluation data shape: {eval_data.shape}\")\n",
    "print(f\"Test data shape: {test_data_x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical data into one-hot vectors\n",
    "def encode_categorical(data, column_name, num_classes):\n",
    "    categories = torch.tensor(data[column_name].values)\n",
    "    return torch.nn.functional.one_hot(categories, num_classes=num_classes).float()\n",
    "\n",
    "def data_to_tensor(data, train=1):\n",
    "    print(\"Number of rows in data:\", len(data))\n",
    "    if 'id' in data.columns:\n",
    "        data.drop('id', axis=1, inplace=True)\n",
    "    # Convert 't_1h' from datetime to a float representing hours since the start of the dataset\n",
    "    ref_time = pd.Timestamp('2022-01-01 00:00:00')  # Adjust the reference time as needed\n",
    "    data['t_1h'] = (data['t_1h'] - ref_time).dt.total_seconds() / 3600.0\n",
    "    # One-hot encode 'etat_barre'\n",
    "    etat_barre_encoded = encode_categorical(data, 'etat_barre', 4)\n",
    "    # Convert to tensors\n",
    "    iu_ac = torch.tensor(data['iu_ac'].values).unsqueeze(1).float()\n",
    "    t_1h = torch.tensor(data['t_1h'].values).unsqueeze(1).float()\n",
    "    # Combine features into a single tensor\n",
    "    features = torch.cat((iu_ac, t_1h, etat_barre_encoded), dim=1)\n",
    "    \n",
    "    if train:\n",
    "        targets = torch.tensor(data['q'].values).float()\n",
    "        return TensorDataset(features, targets)\n",
    "    return TensorDataset(features)\n",
    "\n",
    "# Assuming 'train', 'eval_data', and 'test_data_x' are already loaded and preprocessed\n",
    "train_dataset = data_to_tensor(train_data)\n",
    "eval_dataset = data_to_tensor(eval_data)\n",
    "test_dataset_x = data_to_tensor(test_data_x, train=0)\n",
    "# Convert test data to tensor\n",
    "# test_features = torch.tensor(test_data_x[['iu_ac', 't_1h', 'etat_barre']].values).float().to(device)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=2048, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_x, batch_size=2048, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.regressor = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.regressor(hn.squeeze(0))\n",
    "        return out\n",
    "\n",
    "# Example with more hidden units and layers\n",
    "model = LSTMModel(input_dim=6, hidden_dim=500, num_layers=3, dropout=0.5)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, log_interval=10, checkpoint_dir='./logs/LSTM_v1'):\n",
    "    # Move model to GPU\n",
    "    device = torch.device('cuda:1')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "    start_epoch = 1\n",
    "    flag = True\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')  # specify the filename for checkpoint\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    # Check if checkpoint exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # start from next epoch\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if flag:\n",
    "                print(next(model.parameters()).is_cuda)  \n",
    "                print(inputs.is_cuda)                    \n",
    "                flag = False\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Log loss to TensorBoard\n",
    "            if batch_idx % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "            # Save checkpoint periodically\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in eval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        print(f'Evaluation Loss: {total_loss / len(eval_loader)}')\n",
    "\n",
    "evaluate_model(model, eval_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    i = 1\n",
    "    for features, in test_loader:\n",
    "        i+=1\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        predictions.extend(outputs.cpu().numpy().flatten())\n",
    "        print(predictions)\n",
    "        print(i, features.shape , len(predictions), len(outputs.cpu().numpy().flatten()))\n",
    "# Create a new DataFrame with only predictions, set index starting from 1\n",
    "predictions_df = pd.DataFrame(predictions, columns=['predicted_q'])\n",
    "predictions_df.index = predictions_df.index + 1  # Adjust index to start from 1\n",
    "\n",
    "print(len(predictions_df))\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(inputdir+'predictions_only.csv', index_label='ID')\n",
    "\n",
    "print(\"Predictions saved to 'predictions_only.csv', with IDs starting from 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
