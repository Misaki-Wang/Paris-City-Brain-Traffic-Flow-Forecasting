{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "import datetime\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir='../data/'\n",
    "# Load data\n",
    "train_data = pd.read_csv(inputdir+'loop_sensor_train.csv')\n",
    "baseline_data = pd.read_csv(inputdir+'loop_sensor_test_baseline.csv')\n",
    "test_data_x = pd.read_csv(inputdir+'loop_sensor_test_x.csv')\n",
    "\n",
    "# Prepare test data\n",
    "test_data_x['t_1h'] = pd.to_datetime(test_data_x['t_1h'])\n",
    "ref_time = pd.Timestamp('2022-01-01 00:00:00')\n",
    "test_data_x['t_1h'] = (test_data_x['t_1h'] - ref_time).dt.total_seconds() / 3600.0\n",
    "\n",
    "# Convert test data to tensor\n",
    "test_features = torch.tensor(test_data_x[['iu_ac', 't_1h', 'etat_barre']].values).float().to(device)\n",
    "# Convert time to datetime\n",
    "train_data['t_1h'] = pd.to_datetime(train_data['t_1h'])\n",
    "test_data_x['t_1h'] = pd.to_datetime(test_data_x['t_1h'])\n",
    "\n",
    "# Preprocess and split data for year 2023\n",
    "def preprocess_data(data):\n",
    "    train = data\n",
    "    for name, group in data[data['t_1h'].dt.year == 2023].groupby(pd.Grouper(key='t_1h', freq='29h')):\n",
    "        train = pd.concat([train, group.iloc[:24]])\n",
    "    return train\n",
    "\n",
    "train     = preprocess_data(train_data)\n",
    "eval_data = baseline_data\n",
    "print(train.shape, eval_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (23139649, 4)\n",
      "Evaluation data shape: (428193, 5)\n",
      "Test data shape: (439298, 4)\n"
     ]
    }
   ],
   "source": [
    "inputdir = '../data/'\n",
    "# Load data\n",
    "train_data = pd.read_csv(f'{inputdir}loop_sensor_train.csv')\n",
    "eval_data = pd.read_csv(f'{inputdir}loop_sensor_eval.csv')\n",
    "test_data_x = pd.read_csv(f'{inputdir}loop_sensor_test_x.csv')\n",
    "\n",
    "def to_datetime(data):\n",
    "    data['t_1h'] = pd.to_datetime(data['t_1h'])\n",
    "\n",
    "to_datetime(train_data)\n",
    "to_datetime(eval_data)\n",
    "to_datetime(test_data_x)\n",
    "\n",
    "# Function to prepare training data for 2023\n",
    "def prepare_2023_data(data):\n",
    "    processed_data = data\n",
    "    # Split data for year 2023 into 24-hour frames and the following 5-hour as test\n",
    "    for name, group in data[data['t_1h'].dt.year == 2023].groupby(pd.Grouper(key='t_1h', freq='29h')):\n",
    "        processed_data = pd.concat([processed_data, group.iloc[:24]])\n",
    "    return processed_data\n",
    "train_data = prepare_2023_data(train_data)\n",
    "\n",
    "\n",
    "# Debugging the data shapes\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Evaluation data shape: {eval_data.shape}\")\n",
    "print(f\"Test data shape: {test_data_x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode categorical data into one-hot vectors\n",
    "def encode_categorical(data, column_name, num_classes):\n",
    "    categories = torch.tensor(data[column_name].values)\n",
    "    return torch.nn.functional.one_hot(categories, num_classes=num_classes).float()\n",
    "\n",
    "def data_to_tensor(data, train=1):\n",
    "    # Convert 't_1h' from datetime to a float representing hours since the start of the dataset\n",
    "    ref_time = pd.Timestamp('2022-01-01 00:00:00')  # Adjust the reference time as needed\n",
    "    data['t_1h'] = (data['t_1h'] - ref_time).dt.total_seconds() / 3600.0\n",
    "    # One-hot encode 'etat_barre'\n",
    "    etat_barre_encoded = encode_categorical(data, 'etat_barre', 4)\n",
    "    # Convert to tensors\n",
    "    iu_ac = torch.tensor(data['iu_ac'].values).unsqueeze(1).float()\n",
    "    t_1h = torch.tensor(data['t_1h'].values).unsqueeze(1).float()\n",
    "    # Combine features into a single tensor\n",
    "    features = torch.cat((iu_ac, t_1h, etat_barre_encoded), dim=1)\n",
    "    \n",
    "    if train:\n",
    "        targets = torch.tensor(data['q'].values).float()\n",
    "        return TensorDataset(features, targets)\n",
    "    return TensorDataset(features)\n",
    "\n",
    "# Assuming 'train', 'eval_data', and 'test_data_x' are already loaded and preprocessed\n",
    "train_dataset = data_to_tensor(train_data)\n",
    "eval_dataset = data_to_tensor(eval_data)\n",
    "test_dataset_x = data_to_tensor(test_data_x, train=0)\n",
    "# Convert test data to tensor\n",
    "test_features = torch.tensor(test_data_x[['iu_ac', 't_1h', 'etat_barre']].values).float().to(device)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=2048, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_x, batch_size=2048, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.regressor = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.regressor(hn.squeeze(0))\n",
    "        return out\n",
    "\n",
    "# Example with more hidden units and layers\n",
    "model = LSTMModel(input_dim=5, hidden_dim=500, num_layers=3, dropout=0.5)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/rnn.py:878: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[12000, 1]' is invalid for input of size 10000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Example usage (assuming model, train_loader, criterion, optimizer are defined)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, log_interval, checkpoint_dir)\u001b[0m\n\u001b[1;32m     30\u001b[0m     flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     _, (hn, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(hn\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[12000, 1]' is invalid for input of size 10000"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, log_interval=10, checkpoint_dir='./logs'):\n",
    "    # Move model to GPU\n",
    "    device = torch.device('cuda:1')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "    start_epoch = 1\n",
    "    flag = True\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')  # specify the filename for checkpoint\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    # Check if checkpoint exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # start from next epoch\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if flag:\n",
    "                print(next(model.parameters()).is_cuda)  \n",
    "                print(inputs.is_cuda)                    \n",
    "                flag = False\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Log loss to TensorBoard\n",
    "            if batch_idx % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "            # Save checkpoint periodically\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "# Example usage (assuming model, train_loader, criterion, optimizer are defined)\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in eval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        print(f'Evaluation Loss: {total_loss / len(eval_loader)}')\n",
    "\n",
    "evaluate_model(model, eval_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for features, in test_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        predictions.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "# Create a new DataFrame with only predictions, set index starting from 1\n",
    "predictions_df = pd.DataFrame(predictions, columns=['predicted_q'])\n",
    "predictions_df.index = predictions_df.index + 1  # Adjust index to start from 1\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(inputdir+'predictions_only.csv', index_label='ID')\n",
    "\n",
    "print(\"Predictions saved to 'predictions_only.csv', with IDs starting from 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
