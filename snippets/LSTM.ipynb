{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "import datetime\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices: 2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available CUDA devices:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23139649, 4)\n"
     ]
    }
   ],
   "source": [
    "inputdir='../data/'\n",
    "# Load data\n",
    "train_data = pd.read_csv(inputdir+'loop_sensor_train.csv')\n",
    "test_data_x = pd.read_csv(inputdir+'loop_sensor_test_x.csv')\n",
    "\n",
    "# Convert time to datetime\n",
    "train_data['t_1h'] = pd.to_datetime(train_data['t_1h'])\n",
    "test_data_x['t_1h'] = pd.to_datetime(test_data_x['t_1h'])\n",
    "\n",
    "# Preprocess and split data for year 2023\n",
    "def preprocess_data(data):\n",
    "    train = data\n",
    "    test = []\n",
    "    for name, group in data[data['t_1h'].dt.year == 2023].groupby(pd.Grouper(key='t_1h', freq='29h')):\n",
    "        train = pd.concat([train, group.iloc[:24]])\n",
    "        if len(group) > 24:\n",
    "            test.append(group.iloc[24])\n",
    "    return train, pd.DataFrame(test)\n",
    "\n",
    "train, eval_data = preprocess_data(train_data)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12571565, 4)\n"
     ]
    }
   ],
   "source": [
    "# inputdir='../data/'\n",
    "# # Load data\n",
    "# train_data = pd.read_csv(inputdir+'loop_sensor_train.csv')\n",
    "# test_data_x = pd.read_csv(inputdir+'loop_sensor_test_x.csv')\n",
    "\n",
    "# # Convert time to datetime\n",
    "# train_data['t_1h'] = pd.to_datetime(train_data['t_1h'])\n",
    "# test_data_x['t_1h'] = pd.to_datetime(test_data_x['t_1h'])\n",
    "\n",
    "# # Filter data by year\n",
    "# train_2022 = train_data[train_data['t_1h'].dt.year == 2022]\n",
    "# train_2023 = train_data[train_data['t_1h'].dt.year == 2023]\n",
    "\n",
    "# # Preprocess and split data for year 2023\n",
    "# def preprocess_data(data):\n",
    "#     train = []\n",
    "#     test = []\n",
    "#     # We need to handle each 29-hour block: 24h for train, 1h for test, 4h discard\n",
    "#     grouped = data.groupby(pd.Grouper(key='t_1h', freq='29h'))\n",
    "#     for _, group in grouped:\n",
    "#         # Add the first 24 hours to train, if available\n",
    "#         if len(group) >= 24:\n",
    "#             train.append(group.iloc[:24])\n",
    "#         # Add the 25th hour to test, if available\n",
    "#         if len(group) > 24:\n",
    "#             test.append(group.iloc[24])\n",
    "#         # We automatically discard the next 4 hours by not including them\n",
    "\n",
    "#     train = pd.concat(train, ignore_index=True)\n",
    "#     test = pd.DataFrame(test, columns=data.columns)  # Ensure the DataFrame is properly formatted\n",
    "#     return train, test\n",
    "\n",
    "# # Combine the 2022 data with processed 2023 data\n",
    "# train_2023, test_2023 = preprocess_data(train_2023)\n",
    "# train_combined = pd.concat([train_2022, train_2023], ignore_index=True)\n",
    "# test_combined = pd.concat([test_data_x, test_2023], ignore_index=True)\n",
    "\n",
    "# print(train_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TensorDataset(features)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Assuming 'train', 'eval_data', and 'test_data_x' are already loaded and preprocessed\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m data_to_tensor(eval_data)\n\u001b[1;32m     26\u001b[0m test_dataset_x \u001b[38;5;241m=\u001b[39m data_to_tensor(test_data_x, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mdata_to_tensor\u001b[0;34m(data, train)\u001b[0m\n\u001b[1;32m      9\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_1h\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_1h\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m ref_time)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600.0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# One-hot encode 'etat_barre'\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m etat_barre_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mencode_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43metat_barre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert to tensors\u001b[39;00m\n\u001b[1;32m     13\u001b[0m iu_ac \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miu_ac\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mencode_categorical\u001b[0;34m(data, column_name, num_classes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_categorical\u001b[39m(data, column_name, num_classes):\n\u001b[1;32m      3\u001b[0m     categories \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data[column_name]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "# Function to encode categorical data into one-hot vectors\n",
    "def encode_categorical(data, column_name, num_classes):\n",
    "    categories = torch.tensor(data[column_name].values)\n",
    "    return torch.nn.functional.one_hot(categories, num_classes=num_classes).float()\n",
    "\n",
    "def data_to_tensor(data, train=1):\n",
    "    # Convert 't_1h' from datetime to a float representing hours since the start of the dataset\n",
    "    ref_time = pd.Timestamp('2022-01-01 00:00:00')  # Adjust the reference time as needed\n",
    "    data['t_1h'] = (data['t_1h'] - ref_time).dt.total_seconds() / 3600.0\n",
    "    # One-hot encode 'etat_barre'\n",
    "    etat_barre_encoded = encode_categorical(data, 'etat_barre', 4)\n",
    "    # Convert to tensors\n",
    "    iu_ac = torch.tensor(data['iu_ac'].values).unsqueeze(1).float()\n",
    "    t_1h = torch.tensor(data['t_1h'].values).unsqueeze(1).float()\n",
    "    # Combine features into a single tensor\n",
    "    features = torch.cat((iu_ac, t_1h, etat_barre_encoded), dim=1)\n",
    "    \n",
    "    if train:\n",
    "        targets = torch.tensor(data['q'].values).float()\n",
    "        return TensorDataset(features, targets)\n",
    "    return TensorDataset(features)\n",
    "\n",
    "# Assuming 'train', 'eval_data', and 'test_data_x' are already loaded and preprocessed\n",
    "train_dataset = data_to_tensor(train)\n",
    "eval_dataset = data_to_tensor(eval_data)\n",
    "test_dataset_x = data_to_tensor(test_data_x, train=0)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=2048, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset_x, batch_size=2048, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define LSTM model\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "#         self.regressor = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         _, (hn, _) = self.lstm(x)\n",
    "#         out = self.regressor(hn.squeeze(0))\n",
    "#         return out\n",
    "\n",
    "# model = LSTMModel(input_dim=3, hidden_dim=50, num_layers=2)\n",
    "# model.to(device)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.regressor = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.regressor(hn.squeeze(0))\n",
    "        return out\n",
    "\n",
    "# Example with more hidden units and layers\n",
    "model = LSTMModel(input_dim=5, hidden_dim=500, num_layers=3, dropout=0.5)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "True\n",
      "True\n",
      "Epoch 4, Batch 1, Loss: 520.0232543945312\n",
      "Epoch 4, Batch 2, Loss: 526.5364379882812\n",
      "Epoch 4, Batch 3, Loss: 522.0922241210938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([4096])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 4, Loss: 495.80145263671875\n",
      "Epoch 4, Batch 5, Loss: 502.6297302246094\n",
      "Epoch 4, Batch 6, Loss: 506.8919372558594\n",
      "Epoch 4, Batch 7, Loss: 487.9410095214844\n",
      "Epoch 4, Batch 8, Loss: 498.7703857421875\n",
      "Epoch 4, Batch 9, Loss: 501.1858825683594\n",
      "Epoch 4, Batch 10, Loss: 521.5285034179688\n",
      "Epoch 4, Batch 11, Loss: 514.3475952148438\n",
      "Epoch 4, Batch 12, Loss: 523.0536499023438\n",
      "Epoch 4, Batch 13, Loss: 487.61322021484375\n",
      "Epoch 4, Batch 14, Loss: 541.3139038085938\n",
      "Epoch 4, Batch 15, Loss: 522.6865234375\n",
      "Epoch 4, Batch 16, Loss: 555.7149047851562\n",
      "Epoch 4, Batch 17, Loss: 554.218017578125\n",
      "Epoch 4, Batch 18, Loss: 534.621826171875\n",
      "Epoch 4, Batch 19, Loss: 521.9705810546875\n",
      "Epoch 4, Batch 20, Loss: 533.052734375\n",
      "Epoch 4, Batch 21, Loss: 526.8267822265625\n",
      "Epoch 4, Batch 22, Loss: 537.8046875\n",
      "Epoch 4, Batch 23, Loss: 523.1343994140625\n",
      "Epoch 4, Batch 24, Loss: 533.2232055664062\n",
      "Epoch 4, Batch 25, Loss: 523.561279296875\n",
      "Epoch 4, Batch 26, Loss: 531.1241455078125\n",
      "Epoch 4, Batch 27, Loss: 510.52117919921875\n",
      "Epoch 4, Batch 28, Loss: 523.1213989257812\n",
      "Epoch 4, Batch 29, Loss: 491.9349060058594\n",
      "Epoch 4, Batch 30, Loss: 548.9752807617188\n",
      "Epoch 4, Batch 31, Loss: 517.935546875\n",
      "Epoch 4, Batch 32, Loss: 522.5360107421875\n",
      "Epoch 4, Batch 33, Loss: 489.0721130371094\n",
      "Epoch 4, Batch 34, Loss: 508.1773376464844\n",
      "Epoch 4, Batch 35, Loss: 503.5517578125\n",
      "Epoch 4, Batch 36, Loss: 519.1097412109375\n",
      "Epoch 4, Batch 37, Loss: 519.0541381835938\n",
      "Epoch 4, Batch 38, Loss: 550.400390625\n",
      "Epoch 4, Batch 39, Loss: 519.7186279296875\n",
      "Epoch 4, Batch 40, Loss: 531.1063232421875\n",
      "Epoch 4, Batch 41, Loss: 526.400390625\n",
      "Epoch 4, Batch 42, Loss: 489.98388671875\n",
      "Epoch 4, Batch 43, Loss: 501.9410095214844\n",
      "Epoch 4, Batch 44, Loss: 530.3822021484375\n",
      "Epoch 4, Batch 45, Loss: 529.9389038085938\n",
      "Epoch 4, Batch 46, Loss: 482.5352783203125\n",
      "Epoch 4, Batch 47, Loss: 532.316650390625\n",
      "Epoch 4, Batch 48, Loss: 512.1790771484375\n",
      "Epoch 4, Batch 49, Loss: 517.2464599609375\n",
      "Epoch 4, Batch 50, Loss: 505.794677734375\n",
      "Epoch 4, Batch 51, Loss: 523.7066650390625\n",
      "Epoch 4, Batch 52, Loss: 505.1259765625\n",
      "Epoch 4, Batch 53, Loss: 489.90936279296875\n",
      "Epoch 4, Batch 54, Loss: 499.67626953125\n",
      "Epoch 4, Batch 55, Loss: 515.777099609375\n",
      "Epoch 4, Batch 56, Loss: 515.1826782226562\n",
      "Epoch 4, Batch 57, Loss: 510.63714599609375\n",
      "Epoch 4, Batch 58, Loss: 522.0774536132812\n",
      "Epoch 4, Batch 59, Loss: 548.3660888671875\n",
      "Epoch 4, Batch 60, Loss: 529.7657470703125\n",
      "Epoch 4, Batch 61, Loss: 492.82257080078125\n",
      "Epoch 4, Batch 62, Loss: 509.423583984375\n",
      "Epoch 4, Batch 63, Loss: 501.5191345214844\n",
      "Epoch 4, Batch 64, Loss: 543.94091796875\n",
      "Epoch 4, Batch 65, Loss: 503.952880859375\n",
      "Epoch 4, Batch 66, Loss: 514.677490234375\n",
      "Epoch 4, Batch 67, Loss: 530.1764526367188\n",
      "Epoch 4, Batch 68, Loss: 511.1256103515625\n",
      "Epoch 4, Batch 69, Loss: 507.72698974609375\n",
      "Epoch 4, Batch 70, Loss: 534.2103881835938\n",
      "Epoch 4, Batch 71, Loss: 529.239013671875\n",
      "Epoch 4, Batch 72, Loss: 535.2545166015625\n",
      "Epoch 4, Batch 73, Loss: 501.2858581542969\n",
      "Epoch 4, Batch 74, Loss: 521.3236694335938\n",
      "Epoch 4, Batch 75, Loss: 511.1623229980469\n",
      "Epoch 4, Batch 76, Loss: 519.1473999023438\n",
      "Epoch 4, Batch 77, Loss: 510.8081970214844\n",
      "Epoch 4, Batch 78, Loss: 521.6707763671875\n",
      "Epoch 4, Batch 79, Loss: 482.19647216796875\n",
      "Epoch 4, Batch 80, Loss: 519.29541015625\n",
      "Epoch 4, Batch 81, Loss: 533.6866455078125\n",
      "Epoch 4, Batch 82, Loss: 516.2685546875\n",
      "Epoch 4, Batch 83, Loss: 521.3173828125\n",
      "Epoch 4, Batch 84, Loss: 490.12652587890625\n",
      "Epoch 4, Batch 85, Loss: 484.98358154296875\n",
      "Epoch 4, Batch 86, Loss: 525.2333984375\n",
      "Epoch 4, Batch 87, Loss: 517.9140625\n",
      "Epoch 4, Batch 88, Loss: 504.6153259277344\n",
      "Epoch 4, Batch 89, Loss: 506.022216796875\n",
      "Epoch 4, Batch 90, Loss: 521.4901123046875\n",
      "Epoch 4, Batch 91, Loss: 564.7452392578125\n",
      "Epoch 4, Batch 92, Loss: 520.5553588867188\n",
      "Epoch 4, Batch 93, Loss: 525.8307495117188\n",
      "Epoch 4, Batch 94, Loss: 524.1256103515625\n",
      "Epoch 4, Batch 95, Loss: 522.5572509765625\n",
      "Epoch 4, Batch 96, Loss: 526.067626953125\n",
      "Epoch 4, Batch 97, Loss: 541.14306640625\n",
      "Epoch 4, Batch 98, Loss: 517.330322265625\n",
      "Epoch 4, Batch 99, Loss: 522.061767578125\n",
      "Epoch 4, Batch 100, Loss: 515.9285278320312\n",
      "Epoch 4, Batch 101, Loss: 497.62451171875\n",
      "Epoch 4, Batch 102, Loss: 531.5963134765625\n",
      "Epoch 4, Batch 103, Loss: 511.98260498046875\n",
      "Epoch 4, Batch 104, Loss: 522.30712890625\n",
      "Epoch 4, Batch 105, Loss: 516.1357421875\n",
      "Epoch 4, Batch 106, Loss: 507.5548095703125\n",
      "Epoch 4, Batch 107, Loss: 496.9455261230469\n",
      "Epoch 4, Batch 108, Loss: 492.6358337402344\n",
      "Epoch 4, Batch 109, Loss: 482.392578125\n",
      "Epoch 4, Batch 110, Loss: 513.0137939453125\n",
      "Epoch 4, Batch 111, Loss: 511.6148681640625\n",
      "Epoch 4, Batch 112, Loss: 494.2371826171875\n",
      "Epoch 4, Batch 113, Loss: 522.5787353515625\n",
      "Epoch 4, Batch 114, Loss: 505.458251953125\n",
      "Epoch 4, Batch 115, Loss: 531.5726318359375\n",
      "Epoch 4, Batch 116, Loss: 527.3309936523438\n",
      "Epoch 4, Batch 117, Loss: 529.1488037109375\n",
      "Epoch 4, Batch 118, Loss: 517.2451782226562\n",
      "Epoch 4, Batch 119, Loss: 494.4446716308594\n",
      "Epoch 4, Batch 120, Loss: 526.37939453125\n",
      "Epoch 4, Batch 121, Loss: 530.165283203125\n",
      "Epoch 4, Batch 122, Loss: 500.5548095703125\n",
      "Epoch 4, Batch 123, Loss: 540.2633666992188\n",
      "Epoch 4, Batch 124, Loss: 531.8480834960938\n",
      "Epoch 4, Batch 125, Loss: 520.50390625\n",
      "Epoch 4, Batch 126, Loss: 517.4757690429688\n",
      "Epoch 4, Batch 127, Loss: 496.92156982421875\n",
      "Epoch 4, Batch 128, Loss: 537.6214599609375\n",
      "Epoch 4, Batch 129, Loss: 533.5979614257812\n",
      "Epoch 4, Batch 130, Loss: 505.9411315917969\n",
      "Epoch 4, Batch 131, Loss: 518.1143798828125\n",
      "Epoch 4, Batch 132, Loss: 519.240234375\n",
      "Epoch 4, Batch 133, Loss: 481.23883056640625\n",
      "Epoch 4, Batch 134, Loss: 494.869140625\n",
      "Epoch 4, Batch 135, Loss: 531.9508666992188\n",
      "Epoch 4, Batch 136, Loss: 506.132568359375\n",
      "Epoch 4, Batch 137, Loss: 538.3643188476562\n",
      "Epoch 4, Batch 138, Loss: 514.304931640625\n",
      "Epoch 4, Batch 139, Loss: 497.4571228027344\n",
      "Epoch 4, Batch 140, Loss: 521.4104614257812\n",
      "Epoch 4, Batch 141, Loss: 521.8289794921875\n",
      "Epoch 4, Batch 142, Loss: 520.7994384765625\n",
      "Epoch 4, Batch 143, Loss: 497.13812255859375\n",
      "Epoch 4, Batch 144, Loss: 505.6752624511719\n",
      "Epoch 4, Batch 145, Loss: 522.857666015625\n",
      "Epoch 4, Batch 146, Loss: 544.5291137695312\n",
      "Epoch 4, Batch 147, Loss: 525.26708984375\n",
      "Epoch 4, Batch 148, Loss: 503.54229736328125\n",
      "Epoch 4, Batch 149, Loss: 478.1253356933594\n",
      "Epoch 4, Batch 150, Loss: 537.021728515625\n",
      "Epoch 4, Batch 151, Loss: 555.7311401367188\n",
      "Epoch 4, Batch 152, Loss: 515.457763671875\n",
      "Epoch 4, Batch 153, Loss: 516.8687744140625\n",
      "Epoch 4, Batch 154, Loss: 525.97314453125\n",
      "Epoch 4, Batch 155, Loss: 530.7789306640625\n",
      "Epoch 4, Batch 156, Loss: 489.9463806152344\n",
      "Epoch 4, Batch 157, Loss: 541.3031616210938\n",
      "Epoch 4, Batch 158, Loss: 531.9143676757812\n",
      "Epoch 4, Batch 159, Loss: 512.3615112304688\n",
      "Epoch 4, Batch 160, Loss: 528.2003784179688\n",
      "Epoch 4, Batch 161, Loss: 518.6510009765625\n",
      "Epoch 4, Batch 162, Loss: 501.7952575683594\n",
      "Epoch 4, Batch 163, Loss: 516.90625\n",
      "Epoch 4, Batch 164, Loss: 504.866455078125\n",
      "Epoch 4, Batch 165, Loss: 504.6077575683594\n",
      "Epoch 4, Batch 166, Loss: 547.982421875\n",
      "Epoch 4, Batch 167, Loss: 532.8553466796875\n",
      "Epoch 4, Batch 168, Loss: 496.64678955078125\n",
      "Epoch 4, Batch 169, Loss: 535.40087890625\n",
      "Epoch 4, Batch 170, Loss: 503.90020751953125\n",
      "Epoch 4, Batch 171, Loss: 533.3119506835938\n",
      "Epoch 4, Batch 172, Loss: 506.9390563964844\n",
      "Epoch 4, Batch 173, Loss: 527.2701416015625\n",
      "Epoch 4, Batch 174, Loss: 490.779296875\n",
      "Epoch 4, Batch 175, Loss: 483.5955505371094\n",
      "Epoch 4, Batch 176, Loss: 514.3362426757812\n",
      "Epoch 4, Batch 177, Loss: 542.6220703125\n",
      "Epoch 4, Batch 178, Loss: 537.8788452148438\n",
      "Epoch 4, Batch 179, Loss: 519.912353515625\n",
      "Epoch 4, Batch 180, Loss: 509.58062744140625\n",
      "Epoch 4, Batch 181, Loss: 510.3993835449219\n",
      "Epoch 4, Batch 182, Loss: 533.186767578125\n",
      "Epoch 4, Batch 183, Loss: 535.699462890625\n",
      "Epoch 4, Batch 184, Loss: 509.11761474609375\n",
      "Epoch 4, Batch 185, Loss: 523.954833984375\n",
      "Epoch 4, Batch 186, Loss: 521.4156494140625\n",
      "Epoch 4, Batch 187, Loss: 511.57049560546875\n",
      "Epoch 4, Batch 188, Loss: 536.7511596679688\n",
      "Epoch 4, Batch 189, Loss: 519.791748046875\n",
      "Epoch 4, Batch 190, Loss: 553.0107421875\n",
      "Epoch 4, Batch 191, Loss: 474.47869873046875\n",
      "Epoch 4, Batch 192, Loss: 540.7174682617188\n",
      "Epoch 4, Batch 193, Loss: 518.2516479492188\n",
      "Epoch 4, Batch 194, Loss: 503.2969665527344\n",
      "Epoch 4, Batch 195, Loss: 527.1331787109375\n",
      "Epoch 4, Batch 196, Loss: 516.8953247070312\n",
      "Epoch 4, Batch 197, Loss: 552.5726318359375\n",
      "Epoch 4, Batch 198, Loss: 497.3875427246094\n",
      "Epoch 4, Batch 199, Loss: 499.39404296875\n",
      "Epoch 4, Batch 200, Loss: 481.45697021484375\n",
      "Epoch 4, Batch 201, Loss: 485.3479919433594\n",
      "Epoch 4, Batch 202, Loss: 501.91015625\n",
      "Epoch 4, Batch 203, Loss: 518.6414794921875\n",
      "Epoch 4, Batch 204, Loss: 505.40399169921875\n",
      "Epoch 4, Batch 205, Loss: 502.505859375\n",
      "Epoch 4, Batch 206, Loss: 537.0828857421875\n",
      "Epoch 4, Batch 207, Loss: 492.1473083496094\n",
      "Epoch 4, Batch 208, Loss: 571.7807006835938\n",
      "Epoch 4, Batch 209, Loss: 487.0101013183594\n",
      "Epoch 4, Batch 210, Loss: 525.550537109375\n",
      "Epoch 4, Batch 211, Loss: 520.5665893554688\n",
      "Epoch 4, Batch 212, Loss: 532.4647216796875\n",
      "Epoch 4, Batch 213, Loss: 522.5353393554688\n",
      "Epoch 4, Batch 214, Loss: 518.9467163085938\n",
      "Epoch 4, Batch 215, Loss: 511.6116943359375\n",
      "Epoch 4, Batch 216, Loss: 511.7211608886719\n",
      "Epoch 4, Batch 217, Loss: 513.7439575195312\n",
      "Epoch 4, Batch 218, Loss: 509.9407653808594\n",
      "Epoch 4, Batch 219, Loss: 553.0753173828125\n",
      "Epoch 4, Batch 220, Loss: 512.330322265625\n",
      "Epoch 4, Batch 221, Loss: 518.7935791015625\n",
      "Epoch 4, Batch 222, Loss: 517.0277099609375\n",
      "Epoch 4, Batch 223, Loss: 516.4196166992188\n",
      "Epoch 4, Batch 224, Loss: 508.10064697265625\n",
      "Epoch 4, Batch 225, Loss: 506.6715087890625\n",
      "Epoch 4, Batch 226, Loss: 538.8492431640625\n",
      "Epoch 4, Batch 227, Loss: 492.91473388671875\n",
      "Epoch 4, Batch 228, Loss: 512.002685546875\n",
      "Epoch 4, Batch 229, Loss: 486.59136962890625\n",
      "Epoch 4, Batch 230, Loss: 502.5714111328125\n",
      "Epoch 4, Batch 231, Loss: 501.65643310546875\n",
      "Epoch 4, Batch 232, Loss: 519.006591796875\n",
      "Epoch 4, Batch 233, Loss: 536.784912109375\n",
      "Epoch 4, Batch 234, Loss: 513.5779418945312\n",
      "Epoch 4, Batch 235, Loss: 512.1806640625\n",
      "Epoch 4, Batch 236, Loss: 543.7740478515625\n",
      "Epoch 4, Batch 237, Loss: 524.4525146484375\n",
      "Epoch 4, Batch 238, Loss: 481.8136901855469\n",
      "Epoch 4, Batch 239, Loss: 525.2020874023438\n",
      "Epoch 4, Batch 240, Loss: 512.1102294921875\n",
      "Epoch 4, Batch 241, Loss: 527.5768432617188\n",
      "Epoch 4, Batch 242, Loss: 520.7015380859375\n",
      "Epoch 4, Batch 243, Loss: 510.5003356933594\n",
      "Epoch 4, Batch 244, Loss: 524.3407592773438\n",
      "Epoch 4, Batch 245, Loss: 504.211669921875\n",
      "Epoch 4, Batch 246, Loss: 502.2572937011719\n",
      "Epoch 4, Batch 247, Loss: 534.2452392578125\n",
      "Epoch 4, Batch 248, Loss: 513.746337890625\n",
      "Epoch 4, Batch 249, Loss: 540.47509765625\n",
      "Epoch 4, Batch 250, Loss: 503.94403076171875\n",
      "Epoch 4, Batch 251, Loss: 490.4378356933594\n",
      "Epoch 4, Batch 252, Loss: 527.343017578125\n",
      "Epoch 4, Batch 253, Loss: 525.3161010742188\n",
      "Epoch 4, Batch 254, Loss: 511.7554626464844\n",
      "Epoch 4, Batch 255, Loss: 495.20526123046875\n",
      "Epoch 4, Batch 256, Loss: 525.9733276367188\n",
      "Epoch 4, Batch 257, Loss: 547.5328979492188\n",
      "Epoch 4, Batch 258, Loss: 519.8394775390625\n",
      "Epoch 4, Batch 259, Loss: 529.5146484375\n",
      "Epoch 4, Batch 260, Loss: 534.581787109375\n",
      "Epoch 4, Batch 261, Loss: 513.2127075195312\n",
      "Epoch 4, Batch 262, Loss: 554.0093994140625\n",
      "Epoch 4, Batch 263, Loss: 508.718017578125\n",
      "Epoch 4, Batch 264, Loss: 514.1195068359375\n",
      "Epoch 4, Batch 265, Loss: 502.0015869140625\n",
      "Epoch 4, Batch 266, Loss: 542.8109741210938\n",
      "Epoch 4, Batch 267, Loss: 517.2752685546875\n",
      "Epoch 4, Batch 268, Loss: 517.419921875\n",
      "Epoch 4, Batch 269, Loss: 519.0936279296875\n",
      "Epoch 4, Batch 270, Loss: 503.47064208984375\n",
      "Epoch 4, Batch 271, Loss: 506.7456970214844\n",
      "Epoch 4, Batch 272, Loss: 494.63250732421875\n",
      "Epoch 4, Batch 273, Loss: 537.3040771484375\n",
      "Epoch 4, Batch 274, Loss: 494.1429138183594\n",
      "Epoch 4, Batch 275, Loss: 504.3968505859375\n",
      "Epoch 4, Batch 276, Loss: 526.6636352539062\n",
      "Epoch 4, Batch 277, Loss: 510.66033935546875\n",
      "Epoch 4, Batch 278, Loss: 521.3390502929688\n",
      "Epoch 4, Batch 279, Loss: 511.93231201171875\n",
      "Epoch 4, Batch 280, Loss: 528.2227783203125\n",
      "Epoch 4, Batch 281, Loss: 520.046875\n",
      "Epoch 4, Batch 282, Loss: 508.711181640625\n",
      "Epoch 4, Batch 283, Loss: 500.251953125\n",
      "Epoch 4, Batch 284, Loss: 508.9749450683594\n",
      "Epoch 4, Batch 285, Loss: 495.2857360839844\n",
      "Epoch 4, Batch 286, Loss: 536.965087890625\n",
      "Epoch 4, Batch 287, Loss: 523.858154296875\n",
      "Epoch 4, Batch 288, Loss: 481.3323974609375\n",
      "Epoch 4, Batch 289, Loss: 511.93670654296875\n",
      "Epoch 4, Batch 290, Loss: 535.10107421875\n",
      "Epoch 4, Batch 291, Loss: 498.31982421875\n",
      "Epoch 4, Batch 292, Loss: 537.2001953125\n",
      "Epoch 4, Batch 293, Loss: 528.3443603515625\n",
      "Epoch 4, Batch 294, Loss: 518.5536499023438\n",
      "Epoch 4, Batch 295, Loss: 537.7113037109375\n",
      "Epoch 4, Batch 296, Loss: 509.06024169921875\n",
      "Epoch 4, Batch 297, Loss: 520.7232666015625\n",
      "Epoch 4, Batch 298, Loss: 536.5015869140625\n",
      "Epoch 4, Batch 299, Loss: 531.2164306640625\n",
      "Epoch 4, Batch 300, Loss: 515.5744018554688\n",
      "Epoch 4, Batch 301, Loss: 521.646484375\n",
      "Epoch 4, Batch 302, Loss: 539.319091796875\n",
      "Epoch 4, Batch 303, Loss: 516.284423828125\n",
      "Epoch 4, Batch 304, Loss: 516.184326171875\n",
      "Epoch 4, Batch 305, Loss: 530.980224609375\n",
      "Epoch 4, Batch 306, Loss: 482.615966796875\n",
      "Epoch 4, Batch 307, Loss: 515.7991943359375\n",
      "Epoch 4, Batch 308, Loss: 530.325927734375\n",
      "Epoch 4, Batch 309, Loss: 523.85498046875\n",
      "Epoch 4, Batch 310, Loss: 505.0508728027344\n",
      "Epoch 4, Batch 311, Loss: 534.256103515625\n",
      "Epoch 4, Batch 312, Loss: 512.0003662109375\n",
      "Epoch 4, Batch 313, Loss: 536.5640258789062\n",
      "Epoch 4, Batch 314, Loss: 507.169189453125\n",
      "Epoch 4, Batch 315, Loss: 495.045654296875\n",
      "Epoch 4, Batch 316, Loss: 511.384765625\n",
      "Epoch 4, Batch 317, Loss: 506.7182312011719\n",
      "Epoch 4, Batch 318, Loss: 534.396484375\n",
      "Epoch 4, Batch 319, Loss: 521.6048583984375\n",
      "Epoch 4, Batch 320, Loss: 494.28533935546875\n",
      "Epoch 4, Batch 321, Loss: 501.50579833984375\n",
      "Epoch 4, Batch 322, Loss: 528.665771484375\n",
      "Epoch 4, Batch 323, Loss: 517.0761108398438\n",
      "Epoch 4, Batch 324, Loss: 504.7117919921875\n",
      "Epoch 4, Batch 325, Loss: 491.3695068359375\n",
      "Epoch 4, Batch 326, Loss: 511.64501953125\n",
      "Epoch 4, Batch 327, Loss: 506.71453857421875\n",
      "Epoch 4, Batch 328, Loss: 496.885009765625\n",
      "Epoch 4, Batch 329, Loss: 513.8184814453125\n",
      "Epoch 4, Batch 330, Loss: 497.5452575683594\n",
      "Epoch 4, Batch 331, Loss: 500.55194091796875\n",
      "Epoch 4, Batch 332, Loss: 521.0020751953125\n",
      "Epoch 4, Batch 333, Loss: 525.6312255859375\n",
      "Epoch 4, Batch 334, Loss: 501.5785827636719\n",
      "Epoch 4, Batch 335, Loss: 534.1363525390625\n",
      "Epoch 4, Batch 336, Loss: 513.0780029296875\n",
      "Epoch 4, Batch 337, Loss: 515.30029296875\n",
      "Epoch 4, Batch 338, Loss: 525.8348388671875\n",
      "Epoch 4, Batch 339, Loss: 509.8944091796875\n",
      "Epoch 4, Batch 340, Loss: 538.493896484375\n",
      "Epoch 4, Batch 341, Loss: 530.480224609375\n",
      "Epoch 4, Batch 342, Loss: 501.8982849121094\n",
      "Epoch 4, Batch 343, Loss: 512.8562622070312\n",
      "Epoch 4, Batch 344, Loss: 503.6995544433594\n",
      "Epoch 4, Batch 345, Loss: 498.42962646484375\n",
      "Epoch 4, Batch 346, Loss: 521.7593383789062\n",
      "Epoch 4, Batch 347, Loss: 534.4464111328125\n",
      "Epoch 4, Batch 348, Loss: 530.6612548828125\n",
      "Epoch 4, Batch 349, Loss: 511.78009033203125\n",
      "Epoch 4, Batch 350, Loss: 517.190673828125\n",
      "Epoch 4, Batch 351, Loss: 518.8450317382812\n",
      "Epoch 4, Batch 352, Loss: 539.6468505859375\n",
      "Epoch 4, Batch 353, Loss: 509.13128662109375\n",
      "Epoch 4, Batch 354, Loss: 528.3239135742188\n",
      "Epoch 4, Batch 355, Loss: 504.8352966308594\n",
      "Epoch 4, Batch 356, Loss: 520.1036376953125\n",
      "Epoch 4, Batch 357, Loss: 535.897705078125\n",
      "Epoch 4, Batch 358, Loss: 520.8833618164062\n",
      "Epoch 4, Batch 359, Loss: 513.0027465820312\n",
      "Epoch 4, Batch 360, Loss: 527.7554931640625\n",
      "Epoch 4, Batch 361, Loss: 513.2213134765625\n",
      "Epoch 4, Batch 362, Loss: 539.9241943359375\n",
      "Epoch 4, Batch 363, Loss: 524.761962890625\n",
      "Epoch 4, Batch 364, Loss: 529.3847045898438\n",
      "Epoch 4, Batch 365, Loss: 551.9116821289062\n",
      "Epoch 4, Batch 366, Loss: 538.7266845703125\n",
      "Epoch 4, Batch 367, Loss: 524.9264526367188\n",
      "Epoch 4, Batch 368, Loss: 526.8323974609375\n",
      "Epoch 4, Batch 369, Loss: 523.3969116210938\n",
      "Epoch 4, Batch 370, Loss: 524.07470703125\n",
      "Epoch 4, Batch 371, Loss: 511.7366638183594\n",
      "Epoch 4, Batch 372, Loss: 494.3968505859375\n",
      "Epoch 4, Batch 373, Loss: 523.0580444335938\n",
      "Epoch 4, Batch 374, Loss: 521.2443237304688\n",
      "Epoch 4, Batch 375, Loss: 552.409912109375\n",
      "Epoch 4, Batch 376, Loss: 542.224365234375\n",
      "Epoch 4, Batch 377, Loss: 506.166015625\n",
      "Epoch 4, Batch 378, Loss: 492.8885192871094\n",
      "Epoch 4, Batch 379, Loss: 514.348876953125\n",
      "Epoch 4, Batch 380, Loss: 511.770263671875\n",
      "Epoch 4, Batch 381, Loss: 512.7650146484375\n",
      "Epoch 4, Batch 382, Loss: 507.79473876953125\n",
      "Epoch 4, Batch 383, Loss: 519.412109375\n",
      "Epoch 4, Batch 384, Loss: 510.6788024902344\n",
      "Epoch 4, Batch 385, Loss: 544.0213623046875\n",
      "Epoch 4, Batch 386, Loss: 506.36761474609375\n",
      "Epoch 4, Batch 387, Loss: 529.3713989257812\n",
      "Epoch 4, Batch 388, Loss: 515.5594482421875\n",
      "Epoch 4, Batch 389, Loss: 490.70086669921875\n",
      "Epoch 4, Batch 390, Loss: 499.9258728027344\n",
      "Epoch 4, Batch 391, Loss: 506.9715270996094\n",
      "Epoch 4, Batch 392, Loss: 501.9019470214844\n",
      "Epoch 4, Batch 393, Loss: 501.03411865234375\n",
      "Epoch 4, Batch 394, Loss: 550.6537475585938\n",
      "Epoch 4, Batch 395, Loss: 501.88336181640625\n",
      "Epoch 4, Batch 396, Loss: 505.188232421875\n",
      "Epoch 4, Batch 397, Loss: 517.392578125\n",
      "Epoch 4, Batch 398, Loss: 525.7682495117188\n",
      "Epoch 4, Batch 399, Loss: 507.6527099609375\n",
      "Epoch 4, Batch 400, Loss: 493.34771728515625\n",
      "Epoch 4, Batch 401, Loss: 548.189453125\n",
      "Epoch 4, Batch 402, Loss: 539.5932006835938\n",
      "Epoch 4, Batch 403, Loss: 518.1803588867188\n",
      "Epoch 4, Batch 404, Loss: 517.77880859375\n",
      "Epoch 4, Batch 405, Loss: 527.6373901367188\n",
      "Epoch 4, Batch 406, Loss: 505.2408142089844\n",
      "Epoch 4, Batch 407, Loss: 521.8062133789062\n",
      "Epoch 4, Batch 408, Loss: 522.7989501953125\n",
      "Epoch 4, Batch 409, Loss: 529.2615966796875\n",
      "Epoch 4, Batch 410, Loss: 538.0629272460938\n",
      "Epoch 4, Batch 411, Loss: 542.280517578125\n",
      "Epoch 4, Batch 412, Loss: 506.4532165527344\n",
      "Epoch 4, Batch 413, Loss: 529.101806640625\n",
      "Epoch 4, Batch 414, Loss: 502.6809997558594\n",
      "Epoch 4, Batch 415, Loss: 490.4519958496094\n",
      "Epoch 4, Batch 416, Loss: 524.8276977539062\n",
      "Epoch 4, Batch 417, Loss: 536.840576171875\n",
      "Epoch 4, Batch 418, Loss: 536.399169921875\n",
      "Epoch 4, Batch 419, Loss: 509.9602966308594\n",
      "Epoch 4, Batch 420, Loss: 516.3668212890625\n",
      "Epoch 4, Batch 421, Loss: 510.28021240234375\n",
      "Epoch 4, Batch 422, Loss: 513.6475219726562\n",
      "Epoch 4, Batch 423, Loss: 510.133056640625\n",
      "Epoch 4, Batch 424, Loss: 510.6666259765625\n",
      "Epoch 4, Batch 425, Loss: 509.7876892089844\n",
      "Epoch 4, Batch 426, Loss: 521.8204956054688\n",
      "Epoch 4, Batch 427, Loss: 506.326904296875\n",
      "Epoch 4, Batch 428, Loss: 499.9091796875\n",
      "Epoch 4, Batch 429, Loss: 504.6194763183594\n",
      "Epoch 4, Batch 430, Loss: 520.6058959960938\n",
      "Epoch 4, Batch 431, Loss: 498.27679443359375\n",
      "Epoch 4, Batch 432, Loss: 520.0211181640625\n",
      "Epoch 4, Batch 433, Loss: 478.1939697265625\n",
      "Epoch 4, Batch 434, Loss: 511.3621826171875\n",
      "Epoch 4, Batch 435, Loss: 500.58172607421875\n",
      "Epoch 4, Batch 436, Loss: 535.256591796875\n",
      "Epoch 4, Batch 437, Loss: 507.8755798339844\n",
      "Epoch 4, Batch 438, Loss: 495.71624755859375\n",
      "Epoch 4, Batch 439, Loss: 529.2135009765625\n",
      "Epoch 4, Batch 440, Loss: 506.02239990234375\n",
      "Epoch 4, Batch 441, Loss: 505.53424072265625\n",
      "Epoch 4, Batch 442, Loss: 521.3812866210938\n",
      "Epoch 4, Batch 443, Loss: 507.3729248046875\n",
      "Epoch 4, Batch 444, Loss: 517.5469970703125\n",
      "Epoch 4, Batch 445, Loss: 520.2755126953125\n",
      "Epoch 4, Batch 446, Loss: 521.7828369140625\n",
      "Epoch 4, Batch 447, Loss: 503.86053466796875\n",
      "Epoch 4, Batch 448, Loss: 503.35150146484375\n",
      "Epoch 4, Batch 449, Loss: 489.8859558105469\n",
      "Epoch 4, Batch 450, Loss: 527.272705078125\n",
      "Epoch 4, Batch 451, Loss: 507.34246826171875\n",
      "Epoch 4, Batch 452, Loss: 548.4682006835938\n",
      "Epoch 4, Batch 453, Loss: 551.2551879882812\n",
      "Epoch 4, Batch 454, Loss: 526.1520385742188\n",
      "Epoch 4, Batch 455, Loss: 539.509033203125\n",
      "Epoch 4, Batch 456, Loss: 503.3207702636719\n",
      "Epoch 4, Batch 457, Loss: 525.6722412109375\n",
      "Epoch 4, Batch 458, Loss: 494.11248779296875\n",
      "Epoch 4, Batch 459, Loss: 510.0916442871094\n",
      "Epoch 4, Batch 460, Loss: 512.819091796875\n",
      "Epoch 4, Batch 461, Loss: 543.779541015625\n",
      "Epoch 4, Batch 462, Loss: 534.90234375\n",
      "Epoch 4, Batch 463, Loss: 496.4208984375\n",
      "Epoch 4, Batch 464, Loss: 495.266357421875\n",
      "Epoch 4, Batch 465, Loss: 524.1217041015625\n",
      "Epoch 4, Batch 466, Loss: 515.9444580078125\n",
      "Epoch 4, Batch 467, Loss: 532.190185546875\n",
      "Epoch 4, Batch 468, Loss: 523.61865234375\n",
      "Epoch 4, Batch 469, Loss: 470.2718200683594\n",
      "Epoch 4, Batch 470, Loss: 508.9067077636719\n",
      "Epoch 4, Batch 471, Loss: 521.0512084960938\n",
      "Epoch 4, Batch 472, Loss: 516.5118408203125\n",
      "Epoch 4, Batch 473, Loss: 524.7022705078125\n",
      "Epoch 4, Batch 474, Loss: 525.0958251953125\n",
      "Epoch 4, Batch 475, Loss: 524.636474609375\n",
      "Epoch 4, Batch 476, Loss: 506.6265869140625\n",
      "Epoch 4, Batch 477, Loss: 523.8797607421875\n",
      "Epoch 4, Batch 478, Loss: 514.59130859375\n",
      "Epoch 4, Batch 479, Loss: 513.9859619140625\n",
      "Epoch 4, Batch 480, Loss: 516.9086303710938\n",
      "Epoch 4, Batch 481, Loss: 484.76837158203125\n",
      "Epoch 4, Batch 482, Loss: 529.49072265625\n",
      "Epoch 4, Batch 483, Loss: 514.3314208984375\n",
      "Epoch 4, Batch 484, Loss: 522.4964599609375\n",
      "Epoch 4, Batch 485, Loss: 513.1219482421875\n",
      "Epoch 4, Batch 486, Loss: 490.6423034667969\n",
      "Epoch 4, Batch 487, Loss: 527.1424560546875\n",
      "Epoch 4, Batch 488, Loss: 517.5990600585938\n",
      "Epoch 4, Batch 489, Loss: 514.003662109375\n",
      "Epoch 4, Batch 490, Loss: 509.87158203125\n",
      "Epoch 4, Batch 491, Loss: 506.8754577636719\n",
      "Epoch 4, Batch 492, Loss: 528.9030151367188\n",
      "Epoch 4, Batch 493, Loss: 526.451416015625\n",
      "Epoch 4, Batch 494, Loss: 522.7918701171875\n",
      "Epoch 4, Batch 495, Loss: 544.5411987304688\n",
      "Epoch 4, Batch 496, Loss: 515.410888671875\n",
      "Epoch 4, Batch 497, Loss: 507.8421325683594\n",
      "Epoch 4, Batch 498, Loss: 514.97900390625\n",
      "Epoch 4, Batch 499, Loss: 519.4312744140625\n",
      "Epoch 4, Batch 500, Loss: 499.0895690917969\n",
      "Epoch 4, Batch 501, Loss: 489.619873046875\n",
      "Epoch 4, Batch 502, Loss: 514.8107299804688\n",
      "Epoch 4, Batch 503, Loss: 508.75726318359375\n",
      "Epoch 4, Batch 504, Loss: 498.07000732421875\n",
      "Epoch 4, Batch 505, Loss: 539.6390380859375\n",
      "Epoch 4, Batch 506, Loss: 509.96075439453125\n",
      "Epoch 4, Batch 507, Loss: 505.71502685546875\n",
      "Epoch 4, Batch 508, Loss: 520.0908203125\n",
      "Epoch 4, Batch 509, Loss: 514.852294921875\n",
      "Epoch 4, Batch 510, Loss: 516.1441650390625\n",
      "Epoch 4, Batch 511, Loss: 548.9694213867188\n",
      "Epoch 4, Batch 512, Loss: 486.4126892089844\n",
      "Epoch 4, Batch 513, Loss: 522.6334228515625\n",
      "Epoch 4, Batch 514, Loss: 530.0225830078125\n",
      "Epoch 4, Batch 515, Loss: 516.70654296875\n",
      "Epoch 4, Batch 516, Loss: 514.6624755859375\n",
      "Epoch 4, Batch 517, Loss: 503.4983825683594\n",
      "Epoch 4, Batch 518, Loss: 519.6678466796875\n",
      "Epoch 4, Batch 519, Loss: 503.22320556640625\n",
      "Epoch 4, Batch 520, Loss: 516.1871948242188\n",
      "Epoch 4, Batch 521, Loss: 536.6764526367188\n",
      "Epoch 4, Batch 522, Loss: 545.775634765625\n",
      "Epoch 4, Batch 523, Loss: 523.0006103515625\n",
      "Epoch 4, Batch 524, Loss: 509.29071044921875\n",
      "Epoch 4, Batch 525, Loss: 489.29595947265625\n",
      "Epoch 4, Batch 526, Loss: 508.09393310546875\n",
      "Epoch 4, Batch 527, Loss: 518.6973876953125\n",
      "Epoch 4, Batch 528, Loss: 529.3082275390625\n",
      "Epoch 4, Batch 529, Loss: 535.078125\n",
      "Epoch 4, Batch 530, Loss: 523.4073486328125\n",
      "Epoch 4, Batch 531, Loss: 492.88427734375\n",
      "Epoch 4, Batch 532, Loss: 519.8258666992188\n",
      "Epoch 4, Batch 533, Loss: 516.7228393554688\n",
      "Epoch 4, Batch 534, Loss: 522.1122436523438\n",
      "Epoch 4, Batch 535, Loss: 527.2140502929688\n",
      "Epoch 4, Batch 536, Loss: 486.8687438964844\n",
      "Epoch 4, Batch 537, Loss: 529.9368896484375\n",
      "Epoch 4, Batch 538, Loss: 528.3109130859375\n",
      "Epoch 4, Batch 539, Loss: 506.6282958984375\n",
      "Epoch 4, Batch 540, Loss: 520.94482421875\n",
      "Epoch 4, Batch 541, Loss: 517.0941162109375\n",
      "Epoch 4, Batch 542, Loss: 511.9018249511719\n",
      "Epoch 4, Batch 543, Loss: 512.2116088867188\n",
      "Epoch 4, Batch 544, Loss: 523.65380859375\n",
      "Epoch 4, Batch 545, Loss: 490.5635681152344\n",
      "Epoch 4, Batch 546, Loss: 510.85565185546875\n",
      "Epoch 4, Batch 547, Loss: 494.93475341796875\n",
      "Epoch 4, Batch 548, Loss: 522.0338745117188\n",
      "Epoch 4, Batch 549, Loss: 488.67529296875\n",
      "Epoch 4, Batch 550, Loss: 505.6724548339844\n",
      "Epoch 4, Batch 551, Loss: 509.2886657714844\n",
      "Epoch 4, Batch 552, Loss: 522.9274291992188\n",
      "Epoch 4, Batch 553, Loss: 525.4835815429688\n",
      "Epoch 4, Batch 554, Loss: 509.2057800292969\n",
      "Epoch 4, Batch 555, Loss: 526.3927001953125\n",
      "Epoch 4, Batch 556, Loss: 499.2703857421875\n",
      "Epoch 4, Batch 557, Loss: 528.8056640625\n",
      "Epoch 4, Batch 558, Loss: 528.463623046875\n",
      "Epoch 4, Batch 559, Loss: 505.7361755371094\n",
      "Epoch 4, Batch 560, Loss: 517.996826171875\n",
      "Epoch 4, Batch 561, Loss: 510.9893798828125\n",
      "Epoch 4, Batch 562, Loss: 543.9774780273438\n",
      "Epoch 4, Batch 563, Loss: 511.6454772949219\n",
      "Epoch 4, Batch 564, Loss: 519.880859375\n",
      "Epoch 4, Batch 565, Loss: 502.2919921875\n",
      "Epoch 4, Batch 566, Loss: 503.7376403808594\n",
      "Epoch 4, Batch 567, Loss: 527.7452392578125\n",
      "Epoch 4, Batch 568, Loss: 521.989990234375\n",
      "Epoch 4, Batch 569, Loss: 507.23382568359375\n",
      "Epoch 4, Batch 570, Loss: 499.45635986328125\n",
      "Epoch 4, Batch 571, Loss: 497.820556640625\n",
      "Epoch 4, Batch 572, Loss: 511.42596435546875\n",
      "Epoch 4, Batch 573, Loss: 542.3243408203125\n",
      "Epoch 4, Batch 574, Loss: 526.051025390625\n",
      "Epoch 4, Batch 575, Loss: 495.4488220214844\n",
      "Epoch 4, Batch 576, Loss: 524.46337890625\n",
      "Epoch 4, Batch 577, Loss: 515.4259643554688\n",
      "Epoch 4, Batch 578, Loss: 514.5568237304688\n",
      "Epoch 4, Batch 579, Loss: 506.803955078125\n",
      "Epoch 4, Batch 580, Loss: 494.8265075683594\n",
      "Epoch 4, Batch 581, Loss: 519.446044921875\n",
      "Epoch 4, Batch 582, Loss: 528.2830810546875\n",
      "Epoch 4, Batch 583, Loss: 509.47119140625\n",
      "Epoch 4, Batch 584, Loss: 525.8253784179688\n",
      "Epoch 4, Batch 585, Loss: 529.457275390625\n",
      "Epoch 4, Batch 586, Loss: 520.9176635742188\n",
      "Epoch 4, Batch 587, Loss: 507.6385192871094\n",
      "Epoch 4, Batch 588, Loss: 517.9288940429688\n",
      "Epoch 4, Batch 589, Loss: 546.771240234375\n",
      "Epoch 4, Batch 590, Loss: 502.6202087402344\n",
      "Epoch 4, Batch 591, Loss: 524.5089721679688\n",
      "Epoch 4, Batch 592, Loss: 486.84490966796875\n",
      "Epoch 4, Batch 593, Loss: 537.8431396484375\n",
      "Epoch 4, Batch 594, Loss: 518.6425170898438\n",
      "Epoch 4, Batch 595, Loss: 520.718017578125\n",
      "Epoch 4, Batch 596, Loss: 502.56231689453125\n",
      "Epoch 4, Batch 597, Loss: 509.25341796875\n",
      "Epoch 4, Batch 598, Loss: 527.5238037109375\n",
      "Epoch 4, Batch 599, Loss: 528.809814453125\n",
      "Epoch 4, Batch 600, Loss: 553.2796630859375\n",
      "Epoch 4, Batch 601, Loss: 508.47161865234375\n",
      "Epoch 4, Batch 602, Loss: 530.4598999023438\n",
      "Epoch 4, Batch 603, Loss: 508.7453308105469\n",
      "Epoch 4, Batch 604, Loss: 518.9362182617188\n",
      "Epoch 4, Batch 605, Loss: 510.123046875\n",
      "Epoch 4, Batch 606, Loss: 541.6160888671875\n",
      "Epoch 4, Batch 607, Loss: 501.0796813964844\n",
      "Epoch 4, Batch 608, Loss: 531.2913818359375\n",
      "Epoch 4, Batch 609, Loss: 523.9525146484375\n",
      "Epoch 4, Batch 610, Loss: 514.70068359375\n",
      "Epoch 4, Batch 611, Loss: 517.8681640625\n",
      "Epoch 4, Batch 612, Loss: 512.991943359375\n",
      "Epoch 4, Batch 613, Loss: 513.598388671875\n",
      "Epoch 4, Batch 614, Loss: 523.6701049804688\n",
      "Epoch 4, Batch 615, Loss: 489.4295349121094\n",
      "Epoch 4, Batch 616, Loss: 508.6266784667969\n",
      "Epoch 4, Batch 617, Loss: 532.41943359375\n",
      "Epoch 4, Batch 618, Loss: 545.1686401367188\n",
      "Epoch 4, Batch 619, Loss: 513.1967163085938\n",
      "Epoch 4, Batch 620, Loss: 541.8925170898438\n",
      "Epoch 4, Batch 621, Loss: 532.9352416992188\n",
      "Epoch 4, Batch 622, Loss: 529.5322875976562\n",
      "Epoch 4, Batch 623, Loss: 535.2827758789062\n",
      "Epoch 4, Batch 624, Loss: 539.344482421875\n",
      "Epoch 4, Batch 625, Loss: 495.7106628417969\n",
      "Epoch 4, Batch 626, Loss: 520.9404296875\n",
      "Epoch 4, Batch 627, Loss: 543.0724487304688\n",
      "Epoch 4, Batch 628, Loss: 545.0841064453125\n",
      "Epoch 4, Batch 629, Loss: 506.226318359375\n",
      "Epoch 4, Batch 630, Loss: 515.61865234375\n",
      "Epoch 4, Batch 631, Loss: 521.3846435546875\n",
      "Epoch 4, Batch 632, Loss: 508.9437255859375\n",
      "Epoch 4, Batch 633, Loss: 513.6557006835938\n",
      "Epoch 4, Batch 634, Loss: 517.1984252929688\n",
      "Epoch 4, Batch 635, Loss: 500.1253356933594\n",
      "Epoch 4, Batch 636, Loss: 527.8153686523438\n",
      "Epoch 4, Batch 637, Loss: 536.6761474609375\n",
      "Epoch 4, Batch 638, Loss: 526.6936645507812\n",
      "Epoch 4, Batch 639, Loss: 534.0950317382812\n",
      "Epoch 4, Batch 640, Loss: 511.1720886230469\n",
      "Epoch 4, Batch 641, Loss: 505.4277038574219\n",
      "Epoch 4, Batch 642, Loss: 528.486083984375\n",
      "Epoch 4, Batch 643, Loss: 510.1888427734375\n",
      "Epoch 4, Batch 644, Loss: 524.7646484375\n",
      "Epoch 4, Batch 645, Loss: 524.4541015625\n",
      "Epoch 4, Batch 646, Loss: 503.0589294433594\n",
      "Epoch 4, Batch 647, Loss: 484.8970642089844\n",
      "Epoch 4, Batch 648, Loss: 495.3958740234375\n",
      "Epoch 4, Batch 649, Loss: 499.38726806640625\n",
      "Epoch 4, Batch 650, Loss: 517.5155029296875\n",
      "Epoch 4, Batch 651, Loss: 509.80950927734375\n",
      "Epoch 4, Batch 652, Loss: 523.7206420898438\n",
      "Epoch 4, Batch 653, Loss: 548.6630249023438\n",
      "Epoch 4, Batch 654, Loss: 537.1098022460938\n",
      "Epoch 4, Batch 655, Loss: 517.4588623046875\n",
      "Epoch 4, Batch 656, Loss: 550.6085205078125\n",
      "Epoch 4, Batch 657, Loss: 527.2144165039062\n",
      "Epoch 4, Batch 658, Loss: 518.1346435546875\n",
      "Epoch 4, Batch 659, Loss: 498.0259704589844\n",
      "Epoch 4, Batch 660, Loss: 538.034912109375\n",
      "Epoch 4, Batch 661, Loss: 502.2254333496094\n",
      "Epoch 4, Batch 662, Loss: 510.8033142089844\n",
      "Epoch 4, Batch 663, Loss: 525.4417114257812\n",
      "Epoch 4, Batch 664, Loss: 530.5403442382812\n",
      "Epoch 4, Batch 665, Loss: 496.50946044921875\n",
      "Epoch 4, Batch 666, Loss: 532.1832275390625\n",
      "Epoch 4, Batch 667, Loss: 479.5188903808594\n",
      "Epoch 4, Batch 668, Loss: 494.921630859375\n",
      "Epoch 4, Batch 669, Loss: 524.3493041992188\n",
      "Epoch 4, Batch 670, Loss: 523.6654052734375\n",
      "Epoch 4, Batch 671, Loss: 507.38873291015625\n",
      "Epoch 4, Batch 672, Loss: 547.3807983398438\n",
      "Epoch 4, Batch 673, Loss: 502.6202087402344\n",
      "Epoch 4, Batch 674, Loss: 530.0462646484375\n",
      "Epoch 4, Batch 675, Loss: 537.7323608398438\n",
      "Epoch 4, Batch 676, Loss: 515.6578979492188\n",
      "Epoch 4, Batch 677, Loss: 514.1629638671875\n",
      "Epoch 4, Batch 678, Loss: 528.5761108398438\n",
      "Epoch 4, Batch 679, Loss: 521.046630859375\n",
      "Epoch 4, Batch 680, Loss: 509.35882568359375\n",
      "Epoch 4, Batch 681, Loss: 509.0877685546875\n",
      "Epoch 4, Batch 682, Loss: 516.96435546875\n",
      "Epoch 4, Batch 683, Loss: 517.7137451171875\n",
      "Epoch 4, Batch 684, Loss: 538.7886962890625\n",
      "Epoch 4, Batch 685, Loss: 530.6861572265625\n",
      "Epoch 4, Batch 686, Loss: 496.21221923828125\n",
      "Epoch 4, Batch 687, Loss: 496.6444091796875\n",
      "Epoch 4, Batch 688, Loss: 506.76934814453125\n",
      "Epoch 4, Batch 689, Loss: 499.753173828125\n",
      "Epoch 4, Batch 690, Loss: 524.6998901367188\n",
      "Epoch 4, Batch 691, Loss: 520.3267822265625\n",
      "Epoch 4, Batch 692, Loss: 533.4611206054688\n",
      "Epoch 4, Batch 693, Loss: 513.6988525390625\n",
      "Epoch 4, Batch 694, Loss: 522.4193725585938\n",
      "Epoch 4, Batch 695, Loss: 505.50091552734375\n",
      "Epoch 4, Batch 696, Loss: 492.2996826171875\n",
      "Epoch 4, Batch 697, Loss: 523.9130859375\n",
      "Epoch 4, Batch 698, Loss: 529.1865234375\n",
      "Epoch 4, Batch 699, Loss: 497.23944091796875\n",
      "Epoch 4, Batch 700, Loss: 511.1495361328125\n",
      "Epoch 4, Batch 701, Loss: 506.06494140625\n",
      "Epoch 4, Batch 702, Loss: 499.0081481933594\n",
      "Epoch 4, Batch 703, Loss: 537.6063842773438\n",
      "Epoch 4, Batch 704, Loss: 497.245361328125\n",
      "Epoch 4, Batch 705, Loss: 517.052734375\n",
      "Epoch 4, Batch 706, Loss: 518.1605834960938\n",
      "Epoch 4, Batch 707, Loss: 513.2213134765625\n",
      "Epoch 4, Batch 708, Loss: 528.4586791992188\n",
      "Epoch 4, Batch 709, Loss: 526.527099609375\n",
      "Epoch 4, Batch 710, Loss: 525.4325561523438\n",
      "Epoch 4, Batch 711, Loss: 519.37158203125\n",
      "Epoch 4, Batch 712, Loss: 516.1412353515625\n",
      "Epoch 4, Batch 713, Loss: 512.743896484375\n",
      "Epoch 4, Batch 714, Loss: 495.7588806152344\n",
      "Epoch 4, Batch 715, Loss: 514.547119140625\n",
      "Epoch 4, Batch 716, Loss: 535.05615234375\n",
      "Epoch 4, Batch 717, Loss: 525.09765625\n",
      "Epoch 4, Batch 718, Loss: 521.8231811523438\n",
      "Epoch 4, Batch 719, Loss: 531.828369140625\n",
      "Epoch 4, Batch 720, Loss: 530.5443115234375\n",
      "Epoch 4, Batch 721, Loss: 523.6205444335938\n",
      "Epoch 4, Batch 722, Loss: 533.1927490234375\n",
      "Epoch 4, Batch 723, Loss: 509.6820068359375\n",
      "Epoch 4, Batch 724, Loss: 544.4124755859375\n",
      "Epoch 4, Batch 725, Loss: 511.9192810058594\n",
      "Epoch 4, Batch 726, Loss: 499.09906005859375\n",
      "Epoch 4, Batch 727, Loss: 542.5526733398438\n",
      "Epoch 4, Batch 728, Loss: 507.3332214355469\n",
      "Epoch 4, Batch 729, Loss: 495.6490478515625\n",
      "Epoch 4, Batch 730, Loss: 497.66357421875\n",
      "Epoch 4, Batch 731, Loss: 522.485595703125\n",
      "Epoch 4, Batch 732, Loss: 504.6004333496094\n",
      "Epoch 4, Batch 733, Loss: 509.10498046875\n",
      "Epoch 4, Batch 734, Loss: 506.1855163574219\n",
      "Epoch 4, Batch 735, Loss: 524.89990234375\n",
      "Epoch 4, Batch 736, Loss: 519.0349731445312\n",
      "Epoch 4, Batch 737, Loss: 520.201904296875\n",
      "Epoch 4, Batch 738, Loss: 512.2537841796875\n",
      "Epoch 4, Batch 739, Loss: 506.53143310546875\n",
      "Epoch 4, Batch 740, Loss: 502.31402587890625\n",
      "Epoch 4, Batch 741, Loss: 503.4283447265625\n",
      "Epoch 4, Batch 742, Loss: 487.282470703125\n",
      "Epoch 4, Batch 743, Loss: 536.9003295898438\n",
      "Epoch 4, Batch 744, Loss: 530.5922241210938\n",
      "Epoch 4, Batch 745, Loss: 533.4320068359375\n",
      "Epoch 4, Batch 746, Loss: 498.579833984375\n",
      "Epoch 4, Batch 747, Loss: 498.2889099121094\n",
      "Epoch 4, Batch 748, Loss: 522.5717163085938\n",
      "Epoch 4, Batch 749, Loss: 489.04876708984375\n",
      "Epoch 4, Batch 750, Loss: 515.7601318359375\n",
      "Epoch 4, Batch 751, Loss: 542.8389892578125\n",
      "Epoch 4, Batch 752, Loss: 536.9940185546875\n",
      "Epoch 4, Batch 753, Loss: 540.7613525390625\n",
      "Epoch 4, Batch 754, Loss: 504.35260009765625\n",
      "Epoch 4, Batch 755, Loss: 562.0064697265625\n",
      "Epoch 4, Batch 756, Loss: 513.1246337890625\n",
      "Epoch 4, Batch 757, Loss: 537.21728515625\n",
      "Epoch 4, Batch 758, Loss: 526.5714111328125\n",
      "Epoch 4, Batch 759, Loss: 529.0750122070312\n",
      "Epoch 4, Batch 760, Loss: 531.1251220703125\n",
      "Epoch 4, Batch 761, Loss: 539.7064208984375\n",
      "Epoch 4, Batch 762, Loss: 522.8715209960938\n",
      "Epoch 4, Batch 763, Loss: 490.8973083496094\n",
      "Epoch 4, Batch 764, Loss: 522.6123046875\n",
      "Epoch 4, Batch 765, Loss: 510.6268005371094\n",
      "Epoch 4, Batch 766, Loss: 542.8789672851562\n",
      "Epoch 4, Batch 767, Loss: 508.75189208984375\n",
      "Epoch 4, Batch 768, Loss: 526.4235229492188\n",
      "Epoch 4, Batch 769, Loss: 501.7001953125\n",
      "Epoch 4, Batch 770, Loss: 517.3602294921875\n",
      "Epoch 4, Batch 771, Loss: 517.2232666015625\n",
      "Epoch 4, Batch 772, Loss: 518.9669189453125\n",
      "Epoch 4, Batch 773, Loss: 505.43377685546875\n",
      "Epoch 4, Batch 774, Loss: 537.6900634765625\n",
      "Epoch 4, Batch 775, Loss: 510.22894287109375\n",
      "Epoch 4, Batch 776, Loss: 527.58740234375\n",
      "Epoch 4, Batch 777, Loss: 512.07666015625\n",
      "Epoch 4, Batch 778, Loss: 496.29644775390625\n",
      "Epoch 4, Batch 779, Loss: 523.4645385742188\n",
      "Epoch 4, Batch 780, Loss: 510.16357421875\n",
      "Epoch 4, Batch 781, Loss: 513.3585815429688\n",
      "Epoch 4, Batch 782, Loss: 525.5687866210938\n",
      "Epoch 4, Batch 783, Loss: 502.103759765625\n",
      "Epoch 4, Batch 784, Loss: 522.6693115234375\n",
      "Epoch 4, Batch 785, Loss: 496.812744140625\n",
      "Epoch 4, Batch 786, Loss: 497.3076171875\n",
      "Epoch 4, Batch 787, Loss: 502.33489990234375\n",
      "Epoch 4, Batch 788, Loss: 526.3131103515625\n",
      "Epoch 4, Batch 789, Loss: 542.512939453125\n",
      "Epoch 4, Batch 790, Loss: 504.3154296875\n",
      "Epoch 4, Batch 791, Loss: 505.59356689453125\n",
      "Epoch 4, Batch 792, Loss: 519.0023193359375\n",
      "Epoch 4, Batch 793, Loss: 501.12335205078125\n",
      "Epoch 4, Batch 794, Loss: 498.32855224609375\n",
      "Epoch 4, Batch 795, Loss: 547.5250244140625\n",
      "Epoch 4, Batch 796, Loss: 504.814208984375\n",
      "Epoch 4, Batch 797, Loss: 529.6302490234375\n",
      "Epoch 4, Batch 798, Loss: 528.7535400390625\n",
      "Epoch 4, Batch 799, Loss: 524.057861328125\n",
      "Epoch 4, Batch 800, Loss: 509.47442626953125\n",
      "Epoch 4, Batch 801, Loss: 537.5438232421875\n",
      "Epoch 4, Batch 802, Loss: 515.3695068359375\n",
      "Epoch 4, Batch 803, Loss: 484.9834899902344\n",
      "Epoch 4, Batch 804, Loss: 540.894287109375\n",
      "Epoch 4, Batch 805, Loss: 516.1881713867188\n",
      "Epoch 4, Batch 806, Loss: 516.8916625976562\n",
      "Epoch 4, Batch 807, Loss: 504.8013916015625\n",
      "Epoch 4, Batch 808, Loss: 527.2867431640625\n",
      "Epoch 4, Batch 809, Loss: 537.0714111328125\n",
      "Epoch 4, Batch 810, Loss: 531.7792358398438\n",
      "Epoch 4, Batch 811, Loss: 500.9820251464844\n",
      "Epoch 4, Batch 812, Loss: 528.4251708984375\n",
      "Epoch 4, Batch 813, Loss: 524.88671875\n",
      "Epoch 4, Batch 814, Loss: 524.7402954101562\n",
      "Epoch 4, Batch 815, Loss: 517.048095703125\n",
      "Epoch 4, Batch 816, Loss: 523.56298828125\n",
      "Epoch 4, Batch 817, Loss: 511.6679382324219\n",
      "Epoch 4, Batch 818, Loss: 504.42669677734375\n",
      "Epoch 4, Batch 819, Loss: 503.1657409667969\n",
      "Epoch 4, Batch 820, Loss: 509.32708740234375\n",
      "Epoch 4, Batch 821, Loss: 518.8994140625\n",
      "Epoch 4, Batch 822, Loss: 498.90576171875\n",
      "Epoch 4, Batch 823, Loss: 554.002197265625\n",
      "Epoch 4, Batch 824, Loss: 504.1108093261719\n",
      "Epoch 4, Batch 825, Loss: 501.1756286621094\n",
      "Epoch 4, Batch 826, Loss: 509.46209716796875\n",
      "Epoch 4, Batch 827, Loss: 504.0906677246094\n",
      "Epoch 4, Batch 828, Loss: 562.2420654296875\n",
      "Epoch 4, Batch 829, Loss: 534.6856079101562\n",
      "Epoch 4, Batch 830, Loss: 503.4290466308594\n",
      "Epoch 4, Batch 831, Loss: 535.5045166015625\n",
      "Epoch 4, Batch 832, Loss: 501.254638671875\n",
      "Epoch 4, Batch 833, Loss: 502.67578125\n",
      "Epoch 4, Batch 834, Loss: 521.1937255859375\n",
      "Epoch 4, Batch 835, Loss: 511.07415771484375\n",
      "Epoch 4, Batch 836, Loss: 532.1633911132812\n",
      "Epoch 4, Batch 837, Loss: 519.50048828125\n",
      "Epoch 4, Batch 838, Loss: 546.7479248046875\n",
      "Epoch 4, Batch 839, Loss: 518.17138671875\n",
      "Epoch 4, Batch 840, Loss: 551.333984375\n",
      "Epoch 4, Batch 841, Loss: 528.995849609375\n",
      "Epoch 4, Batch 842, Loss: 500.57269287109375\n",
      "Epoch 4, Batch 843, Loss: 514.7877807617188\n",
      "Epoch 4, Batch 844, Loss: 509.7673034667969\n",
      "Epoch 4, Batch 845, Loss: 523.2028198242188\n",
      "Epoch 4, Batch 846, Loss: 497.7333984375\n",
      "Epoch 4, Batch 847, Loss: 553.9533081054688\n",
      "Epoch 4, Batch 848, Loss: 532.3433227539062\n",
      "Epoch 4, Batch 849, Loss: 500.2508544921875\n",
      "Epoch 4, Batch 850, Loss: 494.86212158203125\n",
      "Epoch 4, Batch 851, Loss: 533.31689453125\n",
      "Epoch 4, Batch 852, Loss: 532.5407104492188\n",
      "Epoch 4, Batch 853, Loss: 520.4117431640625\n",
      "Epoch 4, Batch 854, Loss: 536.874267578125\n",
      "Epoch 4, Batch 855, Loss: 518.5753784179688\n",
      "Epoch 4, Batch 856, Loss: 501.30828857421875\n",
      "Epoch 4, Batch 857, Loss: 521.070068359375\n",
      "Epoch 4, Batch 858, Loss: 520.4620361328125\n",
      "Epoch 4, Batch 859, Loss: 539.0057373046875\n",
      "Epoch 4, Batch 860, Loss: 534.1636962890625\n",
      "Epoch 4, Batch 861, Loss: 534.2681884765625\n",
      "Epoch 4, Batch 862, Loss: 527.07373046875\n",
      "Epoch 4, Batch 863, Loss: 522.76708984375\n",
      "Epoch 4, Batch 864, Loss: 499.474365234375\n",
      "Epoch 4, Batch 865, Loss: 516.272216796875\n",
      "Epoch 4, Batch 866, Loss: 527.5552978515625\n",
      "Epoch 4, Batch 867, Loss: 518.790283203125\n",
      "Epoch 4, Batch 868, Loss: 520.4352416992188\n",
      "Epoch 4, Batch 869, Loss: 524.968017578125\n",
      "Epoch 4, Batch 870, Loss: 501.54473876953125\n",
      "Epoch 4, Batch 871, Loss: 539.7404174804688\n",
      "Epoch 4, Batch 872, Loss: 527.1138305664062\n",
      "Epoch 4, Batch 873, Loss: 526.6121826171875\n",
      "Epoch 4, Batch 874, Loss: 542.7305297851562\n",
      "Epoch 4, Batch 875, Loss: 514.9342041015625\n",
      "Epoch 4, Batch 876, Loss: 506.49786376953125\n",
      "Epoch 4, Batch 877, Loss: 524.892822265625\n",
      "Epoch 4, Batch 878, Loss: 500.95306396484375\n",
      "Epoch 4, Batch 879, Loss: 537.9986572265625\n",
      "Epoch 4, Batch 880, Loss: 513.2732543945312\n",
      "Epoch 4, Batch 881, Loss: 498.885498046875\n",
      "Epoch 4, Batch 882, Loss: 526.3365478515625\n",
      "Epoch 4, Batch 883, Loss: 520.9107666015625\n",
      "Epoch 4, Batch 884, Loss: 513.4192504882812\n",
      "Epoch 4, Batch 885, Loss: 521.8544311523438\n",
      "Epoch 4, Batch 886, Loss: 539.995361328125\n",
      "Epoch 4, Batch 887, Loss: 535.6937866210938\n",
      "Epoch 4, Batch 888, Loss: 518.242919921875\n",
      "Epoch 4, Batch 889, Loss: 529.4781494140625\n",
      "Epoch 4, Batch 890, Loss: 519.4522705078125\n",
      "Epoch 4, Batch 891, Loss: 524.6077880859375\n",
      "Epoch 4, Batch 892, Loss: 512.4229736328125\n",
      "Epoch 4, Batch 893, Loss: 515.1615600585938\n",
      "Epoch 4, Batch 894, Loss: 510.452880859375\n",
      "Epoch 4, Batch 895, Loss: 529.7445678710938\n",
      "Epoch 4, Batch 896, Loss: 524.4623413085938\n",
      "Epoch 4, Batch 897, Loss: 536.9625854492188\n",
      "Epoch 4, Batch 898, Loss: 523.4716796875\n",
      "Epoch 4, Batch 899, Loss: 513.1085205078125\n",
      "Epoch 4, Batch 900, Loss: 562.0905151367188\n",
      "Epoch 4, Batch 901, Loss: 499.0634765625\n",
      "Epoch 4, Batch 902, Loss: 516.5011596679688\n",
      "Epoch 4, Batch 903, Loss: 491.25067138671875\n",
      "Epoch 4, Batch 904, Loss: 535.4232177734375\n",
      "Epoch 4, Batch 905, Loss: 515.1256103515625\n",
      "Epoch 4, Batch 906, Loss: 507.14874267578125\n",
      "Epoch 4, Batch 907, Loss: 508.04473876953125\n",
      "Epoch 4, Batch 908, Loss: 550.7587280273438\n",
      "Epoch 4, Batch 909, Loss: 534.1994018554688\n",
      "Epoch 4, Batch 910, Loss: 494.12518310546875\n",
      "Epoch 4, Batch 911, Loss: 531.5641479492188\n",
      "Epoch 4, Batch 912, Loss: 532.8153686523438\n",
      "Epoch 4, Batch 913, Loss: 506.17987060546875\n",
      "Epoch 4, Batch 914, Loss: 518.682373046875\n",
      "Epoch 4, Batch 915, Loss: 522.7109375\n",
      "Epoch 4, Batch 916, Loss: 540.5782470703125\n",
      "Epoch 4, Batch 917, Loss: 532.9586181640625\n",
      "Epoch 4, Batch 918, Loss: 504.96356201171875\n",
      "Epoch 4, Batch 919, Loss: 526.6260986328125\n",
      "Epoch 4, Batch 920, Loss: 513.6197509765625\n",
      "Epoch 4, Batch 921, Loss: 495.8789978027344\n",
      "Epoch 4, Batch 922, Loss: 542.0606689453125\n",
      "Epoch 4, Batch 923, Loss: 518.634033203125\n",
      "Epoch 4, Batch 924, Loss: 511.74700927734375\n",
      "Epoch 4, Batch 925, Loss: 516.34228515625\n",
      "Epoch 4, Batch 926, Loss: 533.8028564453125\n",
      "Epoch 4, Batch 927, Loss: 507.62469482421875\n",
      "Epoch 4, Batch 928, Loss: 517.0756225585938\n",
      "Epoch 4, Batch 929, Loss: 525.158935546875\n",
      "Epoch 4, Batch 930, Loss: 522.2802734375\n",
      "Epoch 4, Batch 931, Loss: 524.4337158203125\n",
      "Epoch 4, Batch 932, Loss: 507.64697265625\n",
      "Epoch 4, Batch 933, Loss: 516.44775390625\n",
      "Epoch 4, Batch 934, Loss: 521.2059936523438\n",
      "Epoch 4, Batch 935, Loss: 516.2578125\n",
      "Epoch 4, Batch 936, Loss: 531.6648559570312\n",
      "Epoch 4, Batch 937, Loss: 517.1407470703125\n",
      "Epoch 4, Batch 938, Loss: 498.350830078125\n",
      "Epoch 4, Batch 939, Loss: 519.50732421875\n",
      "Epoch 4, Batch 940, Loss: 544.7183837890625\n",
      "Epoch 4, Batch 941, Loss: 530.489501953125\n",
      "Epoch 4, Batch 942, Loss: 520.1721801757812\n",
      "Epoch 4, Batch 943, Loss: 515.150634765625\n",
      "Epoch 4, Batch 944, Loss: 501.580078125\n",
      "Epoch 4, Batch 945, Loss: 537.0379028320312\n",
      "Epoch 4, Batch 946, Loss: 502.3497009277344\n",
      "Epoch 4, Batch 947, Loss: 530.0418701171875\n",
      "Epoch 4, Batch 948, Loss: 526.5537109375\n",
      "Epoch 4, Batch 949, Loss: 536.68994140625\n",
      "Epoch 4, Batch 950, Loss: 484.19384765625\n",
      "Epoch 4, Batch 951, Loss: 524.1651611328125\n",
      "Epoch 4, Batch 952, Loss: 514.5579833984375\n",
      "Epoch 4, Batch 953, Loss: 542.2255859375\n",
      "Epoch 4, Batch 954, Loss: 484.71954345703125\n",
      "Epoch 4, Batch 955, Loss: 513.4650268554688\n",
      "Epoch 4, Batch 956, Loss: 498.5450134277344\n",
      "Epoch 4, Batch 957, Loss: 493.12005615234375\n",
      "Epoch 4, Batch 958, Loss: 513.6795043945312\n",
      "Epoch 4, Batch 959, Loss: 510.785888671875\n",
      "Epoch 4, Batch 960, Loss: 542.2100830078125\n",
      "Epoch 4, Batch 961, Loss: 508.0367126464844\n",
      "Epoch 4, Batch 962, Loss: 545.562744140625\n",
      "Epoch 4, Batch 963, Loss: 500.596435546875\n",
      "Epoch 4, Batch 964, Loss: 524.7081909179688\n",
      "Epoch 4, Batch 965, Loss: 499.77911376953125\n",
      "Epoch 4, Batch 966, Loss: 485.68109130859375\n",
      "Epoch 4, Batch 967, Loss: 517.3162841796875\n",
      "Epoch 4, Batch 968, Loss: 543.341552734375\n",
      "Epoch 4, Batch 969, Loss: 533.6751708984375\n",
      "Epoch 4, Batch 970, Loss: 530.794189453125\n",
      "Epoch 4, Batch 971, Loss: 497.40399169921875\n",
      "Epoch 4, Batch 972, Loss: 520.6146850585938\n",
      "Epoch 4, Batch 973, Loss: 514.8364868164062\n",
      "Epoch 4, Batch 974, Loss: 506.0372009277344\n",
      "Epoch 4, Batch 975, Loss: 530.7459716796875\n",
      "Epoch 4, Batch 976, Loss: 519.8077392578125\n",
      "Epoch 4, Batch 977, Loss: 507.276123046875\n",
      "Epoch 4, Batch 978, Loss: 525.033447265625\n",
      "Epoch 4, Batch 979, Loss: 516.9749755859375\n",
      "Epoch 4, Batch 980, Loss: 503.3438415527344\n",
      "Epoch 4, Batch 981, Loss: 534.241455078125\n",
      "Epoch 4, Batch 982, Loss: 522.29150390625\n",
      "Epoch 4, Batch 983, Loss: 515.6373291015625\n",
      "Epoch 4, Batch 984, Loss: 525.5722045898438\n",
      "Epoch 4, Batch 985, Loss: 503.330078125\n",
      "Epoch 4, Batch 986, Loss: 524.0941162109375\n",
      "Epoch 4, Batch 987, Loss: 533.3319091796875\n",
      "Epoch 4, Batch 988, Loss: 522.87939453125\n",
      "Epoch 4, Batch 989, Loss: 531.04296875\n",
      "Epoch 4, Batch 990, Loss: 544.0863647460938\n",
      "Epoch 4, Batch 991, Loss: 526.378662109375\n",
      "Epoch 4, Batch 992, Loss: 507.10882568359375\n",
      "Epoch 4, Batch 993, Loss: 532.3815307617188\n",
      "Epoch 4, Batch 994, Loss: 533.5025024414062\n",
      "Epoch 4, Batch 995, Loss: 533.5983276367188\n",
      "Epoch 4, Batch 996, Loss: 526.6430053710938\n",
      "Epoch 4, Batch 997, Loss: 541.4418334960938\n",
      "Epoch 4, Batch 998, Loss: 508.6838073730469\n",
      "Epoch 4, Batch 999, Loss: 524.444580078125\n",
      "Epoch 4, Batch 1000, Loss: 532.3450927734375\n",
      "Epoch 4, Batch 1001, Loss: 532.38671875\n",
      "Epoch 4, Batch 1002, Loss: 522.249755859375\n",
      "Epoch 4, Batch 1003, Loss: 527.00244140625\n",
      "Epoch 4, Batch 1004, Loss: 530.57861328125\n",
      "Epoch 4, Batch 1005, Loss: 554.6067504882812\n",
      "Epoch 4, Batch 1006, Loss: 508.98284912109375\n",
      "Epoch 4, Batch 1007, Loss: 522.0721435546875\n",
      "Epoch 4, Batch 1008, Loss: 486.8995056152344\n",
      "Epoch 4, Batch 1009, Loss: 531.6987915039062\n",
      "Epoch 4, Batch 1010, Loss: 543.2139282226562\n",
      "Epoch 4, Batch 1011, Loss: 537.4561767578125\n",
      "Epoch 4, Batch 1012, Loss: 512.0978393554688\n",
      "Epoch 4, Batch 1013, Loss: 512.4500122070312\n",
      "Epoch 4, Batch 1014, Loss: 531.1925659179688\n",
      "Epoch 4, Batch 1015, Loss: 497.39886474609375\n",
      "Epoch 4, Batch 1016, Loss: 527.7127685546875\n",
      "Epoch 4, Batch 1017, Loss: 508.2668151855469\n",
      "Epoch 4, Batch 1018, Loss: 497.2288513183594\n",
      "Epoch 4, Batch 1019, Loss: 512.2767944335938\n",
      "Epoch 4, Batch 1020, Loss: 519.6689453125\n",
      "Epoch 4, Batch 1021, Loss: 543.0477905273438\n",
      "Epoch 4, Batch 1022, Loss: 513.4873046875\n",
      "Epoch 4, Batch 1023, Loss: 539.1290283203125\n",
      "Epoch 4, Batch 1024, Loss: 507.2820739746094\n",
      "Epoch 4, Batch 1025, Loss: 518.0582275390625\n",
      "Epoch 4, Batch 1026, Loss: 542.6358642578125\n",
      "Epoch 4, Batch 1027, Loss: 531.0079345703125\n",
      "Epoch 4, Batch 1028, Loss: 546.1515502929688\n",
      "Epoch 4, Batch 1029, Loss: 499.84466552734375\n",
      "Epoch 4, Batch 1030, Loss: 497.312744140625\n",
      "Epoch 4, Batch 1031, Loss: 538.8905029296875\n",
      "Epoch 4, Batch 1032, Loss: 532.9364624023438\n",
      "Epoch 4, Batch 1033, Loss: 497.9449157714844\n",
      "Epoch 4, Batch 1034, Loss: 540.539794921875\n",
      "Epoch 4, Batch 1035, Loss: 491.97796630859375\n",
      "Epoch 4, Batch 1036, Loss: 480.01336669921875\n",
      "Epoch 4, Batch 1037, Loss: 513.0181884765625\n",
      "Epoch 4, Batch 1038, Loss: 530.153564453125\n",
      "Epoch 4, Batch 1039, Loss: 541.5911254882812\n",
      "Epoch 4, Batch 1040, Loss: 532.47802734375\n",
      "Epoch 4, Batch 1041, Loss: 511.4113464355469\n",
      "Epoch 4, Batch 1042, Loss: 535.9893798828125\n",
      "Epoch 4, Batch 1043, Loss: 489.61187744140625\n",
      "Epoch 4, Batch 1044, Loss: 511.67041015625\n",
      "Epoch 4, Batch 1045, Loss: 494.1893310546875\n",
      "Epoch 4, Batch 1046, Loss: 513.722412109375\n",
      "Epoch 4, Batch 1047, Loss: 494.2187194824219\n",
      "Epoch 4, Batch 1048, Loss: 540.37158203125\n",
      "Epoch 4, Batch 1049, Loss: 541.5186767578125\n",
      "Epoch 4, Batch 1050, Loss: 554.1788330078125\n",
      "Epoch 4, Batch 1051, Loss: 485.73211669921875\n",
      "Epoch 4, Batch 1052, Loss: 508.272216796875\n",
      "Epoch 4, Batch 1053, Loss: 510.5660095214844\n",
      "Epoch 4, Batch 1054, Loss: 514.0270385742188\n",
      "Epoch 4, Batch 1055, Loss: 520.5331420898438\n",
      "Epoch 4, Batch 1056, Loss: 494.60321044921875\n",
      "Epoch 4, Batch 1057, Loss: 513.4813232421875\n",
      "Epoch 4, Batch 1058, Loss: 516.84521484375\n",
      "Epoch 4, Batch 1059, Loss: 506.75634765625\n",
      "Epoch 4, Batch 1060, Loss: 512.4805908203125\n",
      "Epoch 4, Batch 1061, Loss: 516.9498901367188\n",
      "Epoch 4, Batch 1062, Loss: 530.1110229492188\n",
      "Epoch 4, Batch 1063, Loss: 518.3046875\n",
      "Epoch 4, Batch 1064, Loss: 522.8085327148438\n",
      "Epoch 4, Batch 1065, Loss: 531.5355834960938\n",
      "Epoch 4, Batch 1066, Loss: 525.40234375\n",
      "Epoch 4, Batch 1067, Loss: 509.6534118652344\n",
      "Epoch 4, Batch 1068, Loss: 496.492919921875\n",
      "Epoch 4, Batch 1069, Loss: 505.0731201171875\n",
      "Epoch 4, Batch 1070, Loss: 524.2060546875\n",
      "Epoch 4, Batch 1071, Loss: 496.579833984375\n",
      "Epoch 4, Batch 1072, Loss: 562.3429565429688\n",
      "Epoch 4, Batch 1073, Loss: 512.7216186523438\n",
      "Epoch 4, Batch 1074, Loss: 515.1981201171875\n",
      "Epoch 4, Batch 1075, Loss: 510.7837829589844\n",
      "Epoch 4, Batch 1076, Loss: 522.0743408203125\n",
      "Epoch 4, Batch 1077, Loss: 550.0858154296875\n",
      "Epoch 4, Batch 1078, Loss: 539.3184204101562\n",
      "Epoch 4, Batch 1079, Loss: 532.6087646484375\n",
      "Epoch 4, Batch 1080, Loss: 525.278076171875\n",
      "Epoch 4, Batch 1081, Loss: 531.38671875\n",
      "Epoch 4, Batch 1082, Loss: 516.8195190429688\n",
      "Epoch 4, Batch 1083, Loss: 504.1015625\n",
      "Epoch 4, Batch 1084, Loss: 550.673828125\n",
      "Epoch 4, Batch 1085, Loss: 501.21923828125\n",
      "Epoch 4, Batch 1086, Loss: 501.2978515625\n",
      "Epoch 4, Batch 1087, Loss: 508.03564453125\n",
      "Epoch 4, Batch 1088, Loss: 531.9708862304688\n",
      "Epoch 4, Batch 1089, Loss: 518.9237060546875\n",
      "Epoch 4, Batch 1090, Loss: 523.60693359375\n",
      "Epoch 4, Batch 1091, Loss: 514.2080688476562\n",
      "Epoch 4, Batch 1092, Loss: 500.25958251953125\n",
      "Epoch 4, Batch 1093, Loss: 518.8851318359375\n",
      "Epoch 4, Batch 1094, Loss: 517.8300170898438\n",
      "Epoch 4, Batch 1095, Loss: 501.0632019042969\n",
      "Epoch 4, Batch 1096, Loss: 486.57037353515625\n",
      "Epoch 4, Batch 1097, Loss: 510.963134765625\n",
      "Epoch 4, Batch 1098, Loss: 531.08935546875\n",
      "Epoch 4, Batch 1099, Loss: 505.47869873046875\n",
      "Epoch 4, Batch 1100, Loss: 500.24334716796875\n",
      "Epoch 4, Batch 1101, Loss: 517.5792236328125\n",
      "Epoch 4, Batch 1102, Loss: 533.36328125\n",
      "Epoch 4, Batch 1103, Loss: 529.622802734375\n",
      "Epoch 4, Batch 1104, Loss: 529.341064453125\n",
      "Epoch 4, Batch 1105, Loss: 507.84228515625\n",
      "Epoch 4, Batch 1106, Loss: 517.04248046875\n",
      "Epoch 4, Batch 1107, Loss: 505.09771728515625\n",
      "Epoch 4, Batch 1108, Loss: 509.3442077636719\n",
      "Epoch 4, Batch 1109, Loss: 550.6014404296875\n",
      "Epoch 4, Batch 1110, Loss: 559.3912963867188\n",
      "Epoch 4, Batch 1111, Loss: 507.01171875\n",
      "Epoch 4, Batch 1112, Loss: 549.0950317382812\n",
      "Epoch 4, Batch 1113, Loss: 497.0867614746094\n",
      "Epoch 4, Batch 1114, Loss: 503.88885498046875\n",
      "Epoch 4, Batch 1115, Loss: 513.1370849609375\n",
      "Epoch 4, Batch 1116, Loss: 516.30029296875\n",
      "Epoch 4, Batch 1117, Loss: 525.0634765625\n",
      "Epoch 4, Batch 1118, Loss: 547.0096435546875\n",
      "Epoch 4, Batch 1119, Loss: 542.881591796875\n",
      "Epoch 4, Batch 1120, Loss: 533.3739013671875\n",
      "Epoch 4, Batch 1121, Loss: 510.9173278808594\n",
      "Epoch 4, Batch 1122, Loss: 522.6451416015625\n",
      "Epoch 4, Batch 1123, Loss: 523.138916015625\n",
      "Epoch 4, Batch 1124, Loss: 498.8020935058594\n",
      "Epoch 4, Batch 1125, Loss: 531.436279296875\n",
      "Epoch 4, Batch 1126, Loss: 535.1803588867188\n",
      "Epoch 4, Batch 1127, Loss: 541.0254516601562\n",
      "Epoch 4, Batch 1128, Loss: 525.9564208984375\n",
      "Epoch 4, Batch 1129, Loss: 529.6536865234375\n",
      "Epoch 4, Batch 1130, Loss: 490.78033447265625\n",
      "Epoch 4, Batch 1131, Loss: 525.9280395507812\n",
      "Epoch 4, Batch 1132, Loss: 547.6016845703125\n",
      "Epoch 4, Batch 1133, Loss: 497.185302734375\n",
      "Epoch 4, Batch 1134, Loss: 519.733642578125\n",
      "Epoch 4, Batch 1135, Loss: 504.4817810058594\n",
      "Epoch 4, Batch 1136, Loss: 516.899169921875\n",
      "Epoch 4, Batch 1137, Loss: 536.3123779296875\n",
      "Epoch 4, Batch 1138, Loss: 543.1879272460938\n",
      "Epoch 4, Batch 1139, Loss: 534.4569702148438\n",
      "Epoch 4, Batch 1140, Loss: 522.177001953125\n",
      "Epoch 4, Batch 1141, Loss: 519.1009521484375\n",
      "Epoch 4, Batch 1142, Loss: 537.9271240234375\n",
      "Epoch 4, Batch 1143, Loss: 512.1771240234375\n",
      "Epoch 4, Batch 1144, Loss: 505.441650390625\n",
      "Epoch 4, Batch 1145, Loss: 523.1215209960938\n",
      "Epoch 4, Batch 1146, Loss: 516.1013793945312\n",
      "Epoch 4, Batch 1147, Loss: 560.9246826171875\n",
      "Epoch 4, Batch 1148, Loss: 499.790771484375\n",
      "Epoch 4, Batch 1149, Loss: 518.4261474609375\n",
      "Epoch 4, Batch 1150, Loss: 534.1101684570312\n",
      "Epoch 4, Batch 1151, Loss: 502.6340026855469\n",
      "Epoch 4, Batch 1152, Loss: 513.91796875\n",
      "Epoch 4, Batch 1153, Loss: 514.918701171875\n",
      "Epoch 4, Batch 1154, Loss: 510.42230224609375\n",
      "Epoch 4, Batch 1155, Loss: 500.3753662109375\n",
      "Epoch 4, Batch 1156, Loss: 518.9577026367188\n",
      "Epoch 4, Batch 1157, Loss: 497.16473388671875\n",
      "Epoch 4, Batch 1158, Loss: 543.66455078125\n",
      "Epoch 4, Batch 1159, Loss: 525.2506103515625\n",
      "Epoch 4, Batch 1160, Loss: 540.2510986328125\n",
      "Epoch 4, Batch 1161, Loss: 509.5669860839844\n",
      "Epoch 4, Batch 1162, Loss: 531.0626831054688\n",
      "Epoch 4, Batch 1163, Loss: 507.944091796875\n",
      "Epoch 4, Batch 1164, Loss: 531.058349609375\n",
      "Epoch 4, Batch 1165, Loss: 505.56781005859375\n",
      "Epoch 4, Batch 1166, Loss: 526.66796875\n",
      "Epoch 4, Batch 1167, Loss: 531.1349487304688\n",
      "Epoch 4, Batch 1168, Loss: 516.5050048828125\n",
      "Epoch 4, Batch 1169, Loss: 542.386962890625\n",
      "Epoch 4, Batch 1170, Loss: 514.360595703125\n",
      "Epoch 4, Batch 1171, Loss: 532.5929565429688\n",
      "Epoch 4, Batch 1172, Loss: 523.3886108398438\n",
      "Epoch 4, Batch 1173, Loss: 517.303466796875\n",
      "Epoch 4, Batch 1174, Loss: 522.181640625\n",
      "Epoch 4, Batch 1175, Loss: 522.333984375\n",
      "Epoch 4, Batch 1176, Loss: 540.757568359375\n",
      "Epoch 4, Batch 1177, Loss: 492.85455322265625\n",
      "Epoch 4, Batch 1178, Loss: 526.5759887695312\n",
      "Epoch 4, Batch 1179, Loss: 531.7033081054688\n",
      "Epoch 4, Batch 1180, Loss: 524.140625\n",
      "Epoch 4, Batch 1181, Loss: 535.6329345703125\n",
      "Epoch 4, Batch 1182, Loss: 525.951904296875\n",
      "Epoch 4, Batch 1183, Loss: 494.4517822265625\n",
      "Epoch 4, Batch 1184, Loss: 517.1624755859375\n",
      "Epoch 4, Batch 1185, Loss: 499.99212646484375\n",
      "Epoch 4, Batch 1186, Loss: 537.72509765625\n",
      "Epoch 4, Batch 1187, Loss: 521.2487182617188\n",
      "Epoch 4, Batch 1188, Loss: 517.319091796875\n",
      "Epoch 4, Batch 1189, Loss: 513.9379272460938\n",
      "Epoch 4, Batch 1190, Loss: 550.209716796875\n",
      "Epoch 4, Batch 1191, Loss: 529.19970703125\n",
      "Epoch 4, Batch 1192, Loss: 509.00726318359375\n",
      "Epoch 4, Batch 1193, Loss: 506.8565368652344\n",
      "Epoch 4, Batch 1194, Loss: 522.4067993164062\n",
      "Epoch 4, Batch 1195, Loss: 527.6898803710938\n",
      "Epoch 4, Batch 1196, Loss: 506.630126953125\n",
      "Epoch 4, Batch 1197, Loss: 498.77386474609375\n",
      "Epoch 4, Batch 1198, Loss: 515.3856201171875\n",
      "Epoch 4, Batch 1199, Loss: 491.71044921875\n",
      "Epoch 4, Batch 1200, Loss: 522.7265625\n",
      "Epoch 4, Batch 1201, Loss: 551.135498046875\n",
      "Epoch 4, Batch 1202, Loss: 523.1495971679688\n",
      "Epoch 4, Batch 1203, Loss: 525.93701171875\n",
      "Epoch 4, Batch 1204, Loss: 523.8942260742188\n",
      "Epoch 4, Batch 1205, Loss: 529.0193481445312\n",
      "Epoch 4, Batch 1206, Loss: 499.5703125\n",
      "Epoch 4, Batch 1207, Loss: 493.9560546875\n",
      "Epoch 4, Batch 1208, Loss: 540.5872802734375\n",
      "Epoch 4, Batch 1209, Loss: 518.1502685546875\n",
      "Epoch 4, Batch 1210, Loss: 511.45245361328125\n",
      "Epoch 4, Batch 1211, Loss: 543.2685546875\n",
      "Epoch 4, Batch 1212, Loss: 531.0194091796875\n",
      "Epoch 4, Batch 1213, Loss: 526.3078002929688\n",
      "Epoch 4, Batch 1214, Loss: 499.6473083496094\n",
      "Epoch 4, Batch 1215, Loss: 521.5219116210938\n",
      "Epoch 4, Batch 1216, Loss: 515.9893798828125\n",
      "Epoch 4, Batch 1217, Loss: 514.5100708007812\n",
      "Epoch 4, Batch 1218, Loss: 521.1728515625\n",
      "Epoch 4, Batch 1219, Loss: 521.241943359375\n",
      "Epoch 4, Batch 1220, Loss: 503.6617126464844\n",
      "Epoch 4, Batch 1221, Loss: 537.9429931640625\n",
      "Epoch 4, Batch 1222, Loss: 506.1465759277344\n",
      "Epoch 4, Batch 1223, Loss: 511.3173828125\n",
      "Epoch 4, Batch 1224, Loss: 497.3650817871094\n",
      "Epoch 4, Batch 1225, Loss: 511.9465026855469\n",
      "Epoch 4, Batch 1226, Loss: 510.57037353515625\n",
      "Epoch 4, Batch 1227, Loss: 493.55047607421875\n",
      "Epoch 4, Batch 1228, Loss: 513.1661987304688\n",
      "Epoch 4, Batch 1229, Loss: 494.35003662109375\n",
      "Epoch 4, Batch 1230, Loss: 524.6666259765625\n",
      "Epoch 4, Batch 1231, Loss: 538.2215576171875\n",
      "Epoch 4, Batch 1232, Loss: 495.57958984375\n",
      "Epoch 4, Batch 1233, Loss: 514.9755249023438\n",
      "Epoch 4, Batch 1234, Loss: 513.581298828125\n",
      "Epoch 4, Batch 1235, Loss: 512.6322021484375\n",
      "Epoch 4, Batch 1236, Loss: 501.32391357421875\n",
      "Epoch 4, Batch 1237, Loss: 504.2998046875\n",
      "Epoch 4, Batch 1238, Loss: 526.9068603515625\n",
      "Epoch 4, Batch 1239, Loss: 499.99072265625\n",
      "Epoch 4, Batch 1240, Loss: 532.2884521484375\n",
      "Epoch 4, Batch 1241, Loss: 514.1336669921875\n",
      "Epoch 4, Batch 1242, Loss: 512.8473510742188\n",
      "Epoch 4, Batch 1243, Loss: 512.8259887695312\n",
      "Epoch 4, Batch 1244, Loss: 493.2787780761719\n",
      "Epoch 4, Batch 1245, Loss: 536.317626953125\n",
      "Epoch 4, Batch 1246, Loss: 532.1642456054688\n",
      "Epoch 4, Batch 1247, Loss: 516.8319091796875\n",
      "Epoch 4, Batch 1248, Loss: 512.2280883789062\n",
      "Epoch 4, Batch 1249, Loss: 488.21160888671875\n",
      "Epoch 4, Batch 1250, Loss: 521.9114990234375\n",
      "Epoch 4, Batch 1251, Loss: 534.9996337890625\n",
      "Epoch 4, Batch 1252, Loss: 529.856201171875\n",
      "Epoch 4, Batch 1253, Loss: 504.3360290527344\n",
      "Epoch 4, Batch 1254, Loss: 525.795654296875\n",
      "Epoch 4, Batch 1255, Loss: 502.56390380859375\n",
      "Epoch 4, Batch 1256, Loss: 518.7252807617188\n",
      "Epoch 4, Batch 1257, Loss: 536.4198608398438\n",
      "Epoch 4, Batch 1258, Loss: 548.03369140625\n",
      "Epoch 4, Batch 1259, Loss: 493.60540771484375\n",
      "Epoch 4, Batch 1260, Loss: 512.90576171875\n",
      "Epoch 4, Batch 1261, Loss: 534.9562377929688\n",
      "Epoch 4, Batch 1262, Loss: 495.28656005859375\n",
      "Epoch 4, Batch 1263, Loss: 527.23193359375\n",
      "Epoch 4, Batch 1264, Loss: 506.2087097167969\n",
      "Epoch 4, Batch 1265, Loss: 488.31317138671875\n",
      "Epoch 4, Batch 1266, Loss: 505.58441162109375\n",
      "Epoch 4, Batch 1267, Loss: 522.2000732421875\n",
      "Epoch 4, Batch 1268, Loss: 532.2784423828125\n",
      "Epoch 4, Batch 1269, Loss: 504.0325622558594\n",
      "Epoch 4, Batch 1270, Loss: 503.7054138183594\n",
      "Epoch 4, Batch 1271, Loss: 518.6825561523438\n",
      "Epoch 4, Batch 1272, Loss: 539.192138671875\n",
      "Epoch 4, Batch 1273, Loss: 555.7372436523438\n",
      "Epoch 4, Batch 1274, Loss: 539.5853271484375\n",
      "Epoch 4, Batch 1275, Loss: 524.5850219726562\n",
      "Epoch 4, Batch 1276, Loss: 518.892578125\n",
      "Epoch 4, Batch 1277, Loss: 530.1058349609375\n",
      "Epoch 4, Batch 1278, Loss: 510.762451171875\n",
      "Epoch 4, Batch 1279, Loss: 544.943603515625\n",
      "Epoch 4, Batch 1280, Loss: 504.131103515625\n",
      "Epoch 4, Batch 1281, Loss: 512.154541015625\n",
      "Epoch 4, Batch 1282, Loss: 499.3140563964844\n",
      "Epoch 4, Batch 1283, Loss: 530.3218994140625\n",
      "Epoch 4, Batch 1284, Loss: 518.9085693359375\n",
      "Epoch 4, Batch 1285, Loss: 520.7578125\n",
      "Epoch 4, Batch 1286, Loss: 523.9603881835938\n",
      "Epoch 4, Batch 1287, Loss: 538.6812744140625\n",
      "Epoch 4, Batch 1288, Loss: 521.60888671875\n",
      "Epoch 4, Batch 1289, Loss: 523.9678955078125\n",
      "Epoch 4, Batch 1290, Loss: 509.9567565917969\n",
      "Epoch 4, Batch 1291, Loss: 499.63427734375\n",
      "Epoch 4, Batch 1292, Loss: 538.5072021484375\n",
      "Epoch 4, Batch 1293, Loss: 530.4952392578125\n",
      "Epoch 4, Batch 1294, Loss: 535.6289672851562\n",
      "Epoch 4, Batch 1295, Loss: 512.5882568359375\n",
      "Epoch 4, Batch 1296, Loss: 515.94580078125\n",
      "Epoch 4, Batch 1297, Loss: 513.465576171875\n",
      "Epoch 4, Batch 1298, Loss: 535.8707275390625\n",
      "Epoch 4, Batch 1299, Loss: 540.5574951171875\n",
      "Epoch 4, Batch 1300, Loss: 483.138427734375\n",
      "Epoch 4, Batch 1301, Loss: 500.5423278808594\n",
      "Epoch 4, Batch 1302, Loss: 512.0253295898438\n",
      "Epoch 4, Batch 1303, Loss: 522.6024169921875\n",
      "Epoch 4, Batch 1304, Loss: 517.407470703125\n",
      "Epoch 4, Batch 1305, Loss: 506.64599609375\n",
      "Epoch 4, Batch 1306, Loss: 505.7003173828125\n",
      "Epoch 4, Batch 1307, Loss: 511.35699462890625\n",
      "Epoch 4, Batch 1308, Loss: 508.36810302734375\n",
      "Epoch 4, Batch 1309, Loss: 519.8646850585938\n",
      "Epoch 4, Batch 1310, Loss: 499.86578369140625\n",
      "Epoch 4, Batch 1311, Loss: 535.519287109375\n",
      "Epoch 4, Batch 1312, Loss: 510.7526550292969\n",
      "Epoch 4, Batch 1313, Loss: 539.0639038085938\n",
      "Epoch 4, Batch 1314, Loss: 510.1895446777344\n",
      "Epoch 4, Batch 1315, Loss: 522.783447265625\n",
      "Epoch 4, Batch 1316, Loss: 535.0552368164062\n",
      "Epoch 4, Batch 1317, Loss: 535.028564453125\n",
      "Epoch 4, Batch 1318, Loss: 527.203857421875\n",
      "Epoch 4, Batch 1319, Loss: 518.88671875\n",
      "Epoch 4, Batch 1320, Loss: 532.8980712890625\n",
      "Epoch 4, Batch 1321, Loss: 496.15057373046875\n",
      "Epoch 4, Batch 1322, Loss: 516.3111572265625\n",
      "Epoch 4, Batch 1323, Loss: 520.488037109375\n",
      "Epoch 4, Batch 1324, Loss: 496.008544921875\n",
      "Epoch 4, Batch 1325, Loss: 523.0361328125\n",
      "Epoch 4, Batch 1326, Loss: 522.0805053710938\n",
      "Epoch 4, Batch 1327, Loss: 506.30242919921875\n",
      "Epoch 4, Batch 1328, Loss: 522.619140625\n",
      "Epoch 4, Batch 1329, Loss: 487.7629089355469\n",
      "Epoch 4, Batch 1330, Loss: 535.1192016601562\n",
      "Epoch 4, Batch 1331, Loss: 509.737060546875\n",
      "Epoch 4, Batch 1332, Loss: 513.0396118164062\n",
      "Epoch 4, Batch 1333, Loss: 518.8239135742188\n",
      "Epoch 4, Batch 1334, Loss: 525.47705078125\n",
      "Epoch 4, Batch 1335, Loss: 507.18084716796875\n",
      "Epoch 4, Batch 1336, Loss: 528.5245971679688\n",
      "Epoch 4, Batch 1337, Loss: 505.771728515625\n",
      "Epoch 4, Batch 1338, Loss: 532.8218383789062\n",
      "Epoch 4, Batch 1339, Loss: 526.0404052734375\n",
      "Epoch 4, Batch 1340, Loss: 553.6476440429688\n",
      "Epoch 4, Batch 1341, Loss: 491.0413513183594\n",
      "Epoch 4, Batch 1342, Loss: 508.3995056152344\n",
      "Epoch 4, Batch 1343, Loss: 535.096435546875\n",
      "Epoch 4, Batch 1344, Loss: 499.8023681640625\n",
      "Epoch 4, Batch 1345, Loss: 508.9488525390625\n",
      "Epoch 4, Batch 1346, Loss: 546.2400512695312\n",
      "Epoch 4, Batch 1347, Loss: 531.1151733398438\n",
      "Epoch 4, Batch 1348, Loss: 527.0997314453125\n",
      "Epoch 4, Batch 1349, Loss: 547.5782470703125\n",
      "Epoch 4, Batch 1350, Loss: 513.0010375976562\n",
      "Epoch 4, Batch 1351, Loss: 539.0406494140625\n",
      "Epoch 4, Batch 1352, Loss: 516.0181274414062\n",
      "Epoch 4, Batch 1353, Loss: 513.20361328125\n",
      "Epoch 4, Batch 1354, Loss: 517.27197265625\n",
      "Epoch 4, Batch 1355, Loss: 520.7650756835938\n",
      "Epoch 4, Batch 1356, Loss: 501.5296325683594\n",
      "Epoch 4, Batch 1357, Loss: 494.9244384765625\n",
      "Epoch 4, Batch 1358, Loss: 507.51043701171875\n",
      "Epoch 4, Batch 1359, Loss: 529.5670166015625\n",
      "Epoch 4, Batch 1360, Loss: 515.3132934570312\n",
      "Epoch 4, Batch 1361, Loss: 509.64178466796875\n",
      "Epoch 4, Batch 1362, Loss: 512.060302734375\n",
      "Epoch 4, Batch 1363, Loss: 530.68896484375\n",
      "Epoch 4, Batch 1364, Loss: 493.8893737792969\n",
      "Epoch 4, Batch 1365, Loss: 540.0187377929688\n",
      "Epoch 4, Batch 1366, Loss: 510.449462890625\n",
      "Epoch 4, Batch 1367, Loss: 502.242919921875\n",
      "Epoch 4, Batch 1368, Loss: 531.5929565429688\n",
      "Epoch 4, Batch 1369, Loss: 503.545166015625\n",
      "Epoch 4, Batch 1370, Loss: 550.041015625\n",
      "Epoch 4, Batch 1371, Loss: 478.0174255371094\n",
      "Epoch 4, Batch 1372, Loss: 513.4784545898438\n",
      "Epoch 4, Batch 1373, Loss: 524.0936279296875\n",
      "Epoch 4, Batch 1374, Loss: 518.4583740234375\n",
      "Epoch 4, Batch 1375, Loss: 530.4110107421875\n",
      "Epoch 4, Batch 1376, Loss: 510.7196960449219\n",
      "Epoch 4, Batch 1377, Loss: 525.9237670898438\n",
      "Epoch 4, Batch 1378, Loss: 520.3369140625\n",
      "Epoch 4, Batch 1379, Loss: 525.330810546875\n",
      "Epoch 4, Batch 1380, Loss: 503.4495849609375\n",
      "Epoch 4, Batch 1381, Loss: 517.990234375\n",
      "Epoch 4, Batch 1382, Loss: 527.3311767578125\n",
      "Epoch 4, Batch 1383, Loss: 516.5582885742188\n",
      "Epoch 4, Batch 1384, Loss: 529.9424438476562\n",
      "Epoch 4, Batch 1385, Loss: 510.22161865234375\n",
      "Epoch 4, Batch 1386, Loss: 517.5990600585938\n",
      "Epoch 4, Batch 1387, Loss: 541.0967407226562\n",
      "Epoch 4, Batch 1388, Loss: 505.75341796875\n",
      "Epoch 4, Batch 1389, Loss: 522.38232421875\n",
      "Epoch 4, Batch 1390, Loss: 506.51239013671875\n",
      "Epoch 4, Batch 1391, Loss: 512.1876220703125\n",
      "Epoch 4, Batch 1392, Loss: 499.4464111328125\n",
      "Epoch 4, Batch 1393, Loss: 537.251220703125\n",
      "Epoch 4, Batch 1394, Loss: 500.47698974609375\n",
      "Epoch 4, Batch 1395, Loss: 503.36297607421875\n",
      "Epoch 4, Batch 1396, Loss: 514.5188598632812\n",
      "Epoch 4, Batch 1397, Loss: 531.937744140625\n",
      "Epoch 4, Batch 1398, Loss: 541.3907470703125\n",
      "Epoch 4, Batch 1399, Loss: 529.54345703125\n",
      "Epoch 4, Batch 1400, Loss: 505.4966735839844\n",
      "Epoch 4, Batch 1401, Loss: 508.63055419921875\n",
      "Epoch 4, Batch 1402, Loss: 509.9683837890625\n",
      "Epoch 4, Batch 1403, Loss: 519.345703125\n",
      "Epoch 4, Batch 1404, Loss: 501.2366638183594\n",
      "Epoch 4, Batch 1405, Loss: 502.2759704589844\n",
      "Epoch 4, Batch 1406, Loss: 517.7284545898438\n",
      "Epoch 4, Batch 1407, Loss: 531.8521728515625\n",
      "Epoch 4, Batch 1408, Loss: 527.653076171875\n",
      "Epoch 4, Batch 1409, Loss: 512.1123046875\n",
      "Epoch 4, Batch 1410, Loss: 500.2048034667969\n",
      "Epoch 4, Batch 1411, Loss: 531.9615478515625\n",
      "Epoch 4, Batch 1412, Loss: 510.1067810058594\n",
      "Epoch 4, Batch 1413, Loss: 506.7984313964844\n",
      "Epoch 4, Batch 1414, Loss: 515.2916259765625\n",
      "Epoch 4, Batch 1415, Loss: 511.86224365234375\n",
      "Epoch 4, Batch 1416, Loss: 553.738037109375\n",
      "Epoch 4, Batch 1417, Loss: 537.4989624023438\n",
      "Epoch 4, Batch 1418, Loss: 523.7723999023438\n",
      "Epoch 4, Batch 1419, Loss: 513.3323974609375\n",
      "Epoch 4, Batch 1420, Loss: 507.6495056152344\n",
      "Epoch 4, Batch 1421, Loss: 534.7635498046875\n",
      "Epoch 4, Batch 1422, Loss: 530.4781494140625\n",
      "Epoch 4, Batch 1423, Loss: 519.7675170898438\n",
      "Epoch 4, Batch 1424, Loss: 496.28155517578125\n",
      "Epoch 4, Batch 1425, Loss: 510.0694580078125\n",
      "Epoch 4, Batch 1426, Loss: 531.3316650390625\n",
      "Epoch 4, Batch 1427, Loss: 535.6168823242188\n",
      "Epoch 4, Batch 1428, Loss: 535.939453125\n",
      "Epoch 4, Batch 1429, Loss: 512.5948486328125\n",
      "Epoch 4, Batch 1430, Loss: 547.329833984375\n",
      "Epoch 4, Batch 1431, Loss: 491.9804382324219\n",
      "Epoch 4, Batch 1432, Loss: 494.3392028808594\n",
      "Epoch 4, Batch 1433, Loss: 530.0220336914062\n",
      "Epoch 4, Batch 1434, Loss: 526.1895141601562\n",
      "Epoch 4, Batch 1435, Loss: 515.1964111328125\n",
      "Epoch 4, Batch 1436, Loss: 515.885498046875\n",
      "Epoch 4, Batch 1437, Loss: 490.73583984375\n",
      "Epoch 4, Batch 1438, Loss: 510.3654479980469\n",
      "Epoch 4, Batch 1439, Loss: 520.4329833984375\n",
      "Epoch 4, Batch 1440, Loss: 523.629638671875\n",
      "Epoch 4, Batch 1441, Loss: 523.956298828125\n",
      "Epoch 4, Batch 1442, Loss: 507.19354248046875\n",
      "Epoch 4, Batch 1443, Loss: 513.36474609375\n",
      "Epoch 4, Batch 1444, Loss: 549.4649658203125\n",
      "Epoch 4, Batch 1445, Loss: 531.26416015625\n",
      "Epoch 4, Batch 1446, Loss: 547.593017578125\n",
      "Epoch 4, Batch 1447, Loss: 492.939453125\n",
      "Epoch 4, Batch 1448, Loss: 515.379638671875\n",
      "Epoch 4, Batch 1449, Loss: 544.5038452148438\n",
      "Epoch 4, Batch 1450, Loss: 518.5640869140625\n",
      "Epoch 4, Batch 1451, Loss: 530.3405151367188\n",
      "Epoch 4, Batch 1452, Loss: 528.155517578125\n",
      "Epoch 4, Batch 1453, Loss: 518.9747314453125\n",
      "Epoch 4, Batch 1454, Loss: 525.1522827148438\n",
      "Epoch 4, Batch 1455, Loss: 499.5800476074219\n",
      "Epoch 4, Batch 1456, Loss: 488.34368896484375\n",
      "Epoch 4, Batch 1457, Loss: 527.420166015625\n",
      "Epoch 4, Batch 1458, Loss: 516.1307373046875\n",
      "Epoch 4, Batch 1459, Loss: 539.3314819335938\n",
      "Epoch 4, Batch 1460, Loss: 526.6744995117188\n",
      "Epoch 4, Batch 1461, Loss: 552.78076171875\n",
      "Epoch 4, Batch 1462, Loss: 528.870849609375\n",
      "Epoch 4, Batch 1463, Loss: 497.47283935546875\n",
      "Epoch 4, Batch 1464, Loss: 534.7511596679688\n",
      "Epoch 4, Batch 1465, Loss: 542.8078002929688\n",
      "Epoch 4, Batch 1466, Loss: 523.5529174804688\n",
      "Epoch 4, Batch 1467, Loss: 510.720947265625\n",
      "Epoch 4, Batch 1468, Loss: 511.755859375\n",
      "Epoch 4, Batch 1469, Loss: 485.30096435546875\n",
      "Epoch 4, Batch 1470, Loss: 540.9640502929688\n",
      "Epoch 4, Batch 1471, Loss: 505.71331787109375\n",
      "Epoch 4, Batch 1472, Loss: 494.119873046875\n",
      "Epoch 4, Batch 1473, Loss: 510.0730895996094\n",
      "Epoch 4, Batch 1474, Loss: 518.1927490234375\n",
      "Epoch 4, Batch 1475, Loss: 548.7531127929688\n",
      "Epoch 4, Batch 1476, Loss: 517.9899291992188\n",
      "Epoch 4, Batch 1477, Loss: 528.932373046875\n",
      "Epoch 4, Batch 1478, Loss: 549.9508666992188\n",
      "Epoch 4, Batch 1479, Loss: 522.7857666015625\n",
      "Epoch 4, Batch 1480, Loss: 520.722412109375\n",
      "Epoch 4, Batch 1481, Loss: 512.2993774414062\n",
      "Epoch 4, Batch 1482, Loss: 509.997802734375\n",
      "Epoch 4, Batch 1483, Loss: 528.8028564453125\n",
      "Epoch 4, Batch 1484, Loss: 488.3095703125\n",
      "Epoch 4, Batch 1485, Loss: 537.898193359375\n",
      "Epoch 4, Batch 1486, Loss: 533.1976318359375\n",
      "Epoch 4, Batch 1487, Loss: 488.3909912109375\n",
      "Epoch 4, Batch 1488, Loss: 511.19671630859375\n",
      "Epoch 4, Batch 1489, Loss: 523.1227416992188\n",
      "Epoch 4, Batch 1490, Loss: 523.797607421875\n",
      "Epoch 4, Batch 1491, Loss: 539.6256103515625\n",
      "Epoch 4, Batch 1492, Loss: 505.42584228515625\n",
      "Epoch 4, Batch 1493, Loss: 516.928955078125\n",
      "Epoch 4, Batch 1494, Loss: 524.8958740234375\n",
      "Epoch 4, Batch 1495, Loss: 509.14715576171875\n",
      "Epoch 4, Batch 1496, Loss: 508.509521484375\n",
      "Epoch 4, Batch 1497, Loss: 544.513671875\n",
      "Epoch 4, Batch 1498, Loss: 518.770751953125\n",
      "Epoch 4, Batch 1499, Loss: 521.029296875\n",
      "Epoch 4, Batch 1500, Loss: 522.5468139648438\n",
      "Epoch 4, Batch 1501, Loss: 521.5294799804688\n",
      "Epoch 4, Batch 1502, Loss: 551.3494873046875\n",
      "Epoch 4, Batch 1503, Loss: 499.19183349609375\n",
      "Epoch 4, Batch 1504, Loss: 506.73931884765625\n",
      "Epoch 4, Batch 1505, Loss: 530.416259765625\n",
      "Epoch 4, Batch 1506, Loss: 490.24267578125\n",
      "Epoch 4, Batch 1507, Loss: 516.329833984375\n",
      "Epoch 4, Batch 1508, Loss: 504.85467529296875\n",
      "Epoch 4, Batch 1509, Loss: 526.84912109375\n",
      "Epoch 4, Batch 1510, Loss: 537.4417724609375\n",
      "Epoch 4, Batch 1511, Loss: 504.7736511230469\n",
      "Epoch 4, Batch 1512, Loss: 530.0188598632812\n",
      "Epoch 4, Batch 1513, Loss: 526.4825439453125\n",
      "Epoch 4, Batch 1514, Loss: 507.761962890625\n",
      "Epoch 4, Batch 1515, Loss: 524.588623046875\n",
      "Epoch 4, Batch 1516, Loss: 497.68731689453125\n",
      "Epoch 4, Batch 1517, Loss: 534.5703125\n",
      "Epoch 4, Batch 1518, Loss: 500.7696228027344\n",
      "Epoch 4, Batch 1519, Loss: 541.2710571289062\n",
      "Epoch 4, Batch 1520, Loss: 517.0623779296875\n",
      "Epoch 4, Batch 1521, Loss: 541.300048828125\n",
      "Epoch 4, Batch 1522, Loss: 497.7818298339844\n",
      "Epoch 4, Batch 1523, Loss: 519.8602294921875\n",
      "Epoch 4, Batch 1524, Loss: 505.130615234375\n",
      "Epoch 4, Batch 1525, Loss: 529.0355834960938\n",
      "Epoch 4, Batch 1526, Loss: 515.056640625\n",
      "Epoch 4, Batch 1527, Loss: 469.1123046875\n",
      "Epoch 4, Batch 1528, Loss: 505.4530029296875\n",
      "Epoch 4, Batch 1529, Loss: 524.5836791992188\n",
      "Epoch 4, Batch 1530, Loss: 516.1739501953125\n",
      "Epoch 4, Batch 1531, Loss: 505.87109375\n",
      "Epoch 4, Batch 1532, Loss: 529.6769409179688\n",
      "Epoch 4, Batch 1533, Loss: 504.87005615234375\n",
      "Epoch 4, Batch 1534, Loss: 517.2107543945312\n",
      "Epoch 4, Batch 1535, Loss: 518.3601684570312\n",
      "Epoch 4, Batch 1536, Loss: 512.126708984375\n",
      "Epoch 4, Batch 1537, Loss: 494.69744873046875\n",
      "Epoch 4, Batch 1538, Loss: 521.468994140625\n",
      "Epoch 4, Batch 1539, Loss: 498.112060546875\n",
      "Epoch 4, Batch 1540, Loss: 546.981689453125\n",
      "Epoch 4, Batch 1541, Loss: 535.7738647460938\n",
      "Epoch 4, Batch 1542, Loss: 497.7961120605469\n",
      "Epoch 4, Batch 1543, Loss: 523.5325927734375\n",
      "Epoch 4, Batch 1544, Loss: 513.8955078125\n",
      "Epoch 4, Batch 1545, Loss: 524.1719970703125\n",
      "Epoch 4, Batch 1546, Loss: 508.3226013183594\n",
      "Epoch 4, Batch 1547, Loss: 512.4141235351562\n",
      "Epoch 4, Batch 1548, Loss: 528.116455078125\n",
      "Epoch 4, Batch 1549, Loss: 515.0111083984375\n",
      "Epoch 4, Batch 1550, Loss: 537.694091796875\n",
      "Epoch 4, Batch 1551, Loss: 518.4457397460938\n",
      "Epoch 4, Batch 1552, Loss: 522.0284423828125\n",
      "Epoch 4, Batch 1553, Loss: 527.647216796875\n",
      "Epoch 4, Batch 1554, Loss: 518.340576171875\n",
      "Epoch 4, Batch 1555, Loss: 535.2347412109375\n",
      "Epoch 4, Batch 1556, Loss: 495.17535400390625\n",
      "Epoch 4, Batch 1557, Loss: 513.302978515625\n",
      "Epoch 4, Batch 1558, Loss: 531.123291015625\n",
      "Epoch 4, Batch 1559, Loss: 505.089111328125\n",
      "Epoch 4, Batch 1560, Loss: 526.202880859375\n",
      "Epoch 4, Batch 1561, Loss: 519.7588500976562\n",
      "Epoch 4, Batch 1562, Loss: 505.97784423828125\n",
      "Epoch 4, Batch 1563, Loss: 508.3167419433594\n",
      "Epoch 4, Batch 1564, Loss: 502.41302490234375\n",
      "Epoch 4, Batch 1565, Loss: 526.0626220703125\n",
      "Epoch 4, Batch 1566, Loss: 496.4637451171875\n",
      "Epoch 4, Batch 1567, Loss: 514.1339111328125\n",
      "Epoch 4, Batch 1568, Loss: 484.5093994140625\n",
      "Epoch 4, Batch 1569, Loss: 497.6143798828125\n",
      "Epoch 4, Batch 1570, Loss: 540.1550903320312\n",
      "Epoch 4, Batch 1571, Loss: 507.935546875\n",
      "Epoch 4, Batch 1572, Loss: 532.80029296875\n",
      "Epoch 4, Batch 1573, Loss: 505.9241638183594\n",
      "Epoch 4, Batch 1574, Loss: 511.63671875\n",
      "Epoch 4, Batch 1575, Loss: 526.7308959960938\n",
      "Epoch 4, Batch 1576, Loss: 500.246337890625\n",
      "Epoch 4, Batch 1577, Loss: 492.944580078125\n",
      "Epoch 4, Batch 1578, Loss: 483.237548828125\n",
      "Epoch 4, Batch 1579, Loss: 525.1420288085938\n",
      "Epoch 4, Batch 1580, Loss: 523.5098266601562\n",
      "Epoch 4, Batch 1581, Loss: 493.2051696777344\n",
      "Epoch 4, Batch 1582, Loss: 505.08221435546875\n",
      "Epoch 4, Batch 1583, Loss: 512.212890625\n",
      "Epoch 4, Batch 1584, Loss: 501.00726318359375\n",
      "Epoch 4, Batch 1585, Loss: 488.62200927734375\n",
      "Epoch 4, Batch 1586, Loss: 511.699951171875\n",
      "Epoch 4, Batch 1587, Loss: 550.7213134765625\n",
      "Epoch 4, Batch 1588, Loss: 508.85296630859375\n",
      "Epoch 4, Batch 1589, Loss: 505.36041259765625\n",
      "Epoch 4, Batch 1590, Loss: 551.0829467773438\n",
      "Epoch 4, Batch 1591, Loss: 501.3753356933594\n",
      "Epoch 4, Batch 1592, Loss: 542.8405151367188\n",
      "Epoch 4, Batch 1593, Loss: 513.9743041992188\n",
      "Epoch 4, Batch 1594, Loss: 507.62353515625\n",
      "Epoch 4, Batch 1595, Loss: 523.3680419921875\n",
      "Epoch 4, Batch 1596, Loss: 549.1901245117188\n",
      "Epoch 4, Batch 1597, Loss: 535.3155517578125\n",
      "Epoch 4, Batch 1598, Loss: 522.5897216796875\n",
      "Epoch 4, Batch 1599, Loss: 517.242431640625\n",
      "Epoch 4, Batch 1600, Loss: 521.2916870117188\n",
      "Epoch 4, Batch 1601, Loss: 517.7628784179688\n",
      "Epoch 4, Batch 1602, Loss: 526.14990234375\n",
      "Epoch 4, Batch 1603, Loss: 513.1257934570312\n",
      "Epoch 4, Batch 1604, Loss: 511.6373291015625\n",
      "Epoch 4, Batch 1605, Loss: 513.248046875\n",
      "Epoch 4, Batch 1606, Loss: 486.0003356933594\n",
      "Epoch 4, Batch 1607, Loss: 516.4871826171875\n",
      "Epoch 4, Batch 1608, Loss: 542.1378784179688\n",
      "Epoch 4, Batch 1609, Loss: 509.55682373046875\n",
      "Epoch 4, Batch 1610, Loss: 519.0069580078125\n",
      "Epoch 4, Batch 1611, Loss: 505.3095703125\n",
      "Epoch 4, Batch 1612, Loss: 529.8599243164062\n",
      "Epoch 4, Batch 1613, Loss: 519.0846557617188\n",
      "Epoch 4, Batch 1614, Loss: 513.9989624023438\n",
      "Epoch 4, Batch 1615, Loss: 498.6419372558594\n",
      "Epoch 4, Batch 1616, Loss: 525.5916748046875\n",
      "Epoch 4, Batch 1617, Loss: 513.248779296875\n",
      "Epoch 4, Batch 1618, Loss: 536.0098876953125\n",
      "Epoch 4, Batch 1619, Loss: 511.51483154296875\n",
      "Epoch 4, Batch 1620, Loss: 518.3314208984375\n",
      "Epoch 4, Batch 1621, Loss: 500.40545654296875\n",
      "Epoch 4, Batch 1622, Loss: 540.0584716796875\n",
      "Epoch 4, Batch 1623, Loss: 518.010986328125\n",
      "Epoch 4, Batch 1624, Loss: 507.92205810546875\n",
      "Epoch 4, Batch 1625, Loss: 530.705078125\n",
      "Epoch 4, Batch 1626, Loss: 526.1461181640625\n",
      "Epoch 4, Batch 1627, Loss: 541.584716796875\n",
      "Epoch 4, Batch 1628, Loss: 506.293701171875\n",
      "Epoch 4, Batch 1629, Loss: 517.4638061523438\n",
      "Epoch 4, Batch 1630, Loss: 546.0892333984375\n",
      "Epoch 4, Batch 1631, Loss: 520.1620483398438\n",
      "Epoch 4, Batch 1632, Loss: 529.7724609375\n",
      "Epoch 4, Batch 1633, Loss: 517.303466796875\n",
      "Epoch 4, Batch 1634, Loss: 501.3089294433594\n",
      "Epoch 4, Batch 1635, Loss: 550.4445190429688\n",
      "Epoch 4, Batch 1636, Loss: 509.323486328125\n",
      "Epoch 4, Batch 1637, Loss: 540.9503784179688\n",
      "Epoch 4, Batch 1638, Loss: 533.1488037109375\n",
      "Epoch 4, Batch 1639, Loss: 515.068603515625\n",
      "Epoch 4, Batch 1640, Loss: 504.5818786621094\n",
      "Epoch 4, Batch 1641, Loss: 521.59033203125\n",
      "Epoch 4, Batch 1642, Loss: 514.2893676757812\n",
      "Epoch 4, Batch 1643, Loss: 521.8614501953125\n",
      "Epoch 4, Batch 1644, Loss: 552.170166015625\n",
      "Epoch 4, Batch 1645, Loss: 545.248291015625\n",
      "Epoch 4, Batch 1646, Loss: 513.4757690429688\n",
      "Epoch 4, Batch 1647, Loss: 517.120361328125\n",
      "Epoch 4, Batch 1648, Loss: 512.0916748046875\n",
      "Epoch 4, Batch 1649, Loss: 541.5859375\n",
      "Epoch 4, Batch 1650, Loss: 484.6399841308594\n",
      "Epoch 4, Batch 1651, Loss: 552.7088012695312\n",
      "Epoch 4, Batch 1652, Loss: 514.8772583007812\n",
      "Epoch 4, Batch 1653, Loss: 499.80120849609375\n",
      "Epoch 4, Batch 1654, Loss: 501.83673095703125\n",
      "Epoch 4, Batch 1655, Loss: 519.7144775390625\n",
      "Epoch 4, Batch 1656, Loss: 509.7102966308594\n",
      "Epoch 4, Batch 1657, Loss: 534.4705200195312\n",
      "Epoch 4, Batch 1658, Loss: 527.9405517578125\n",
      "Epoch 4, Batch 1659, Loss: 508.475830078125\n",
      "Epoch 4, Batch 1660, Loss: 554.4281005859375\n",
      "Epoch 4, Batch 1661, Loss: 528.4119873046875\n",
      "Epoch 4, Batch 1662, Loss: 542.14697265625\n",
      "Epoch 4, Batch 1663, Loss: 502.9036560058594\n",
      "Epoch 4, Batch 1664, Loss: 520.4700927734375\n",
      "Epoch 4, Batch 1665, Loss: 501.62811279296875\n",
      "Epoch 4, Batch 1666, Loss: 526.0800170898438\n",
      "Epoch 4, Batch 1667, Loss: 530.803955078125\n",
      "Epoch 4, Batch 1668, Loss: 532.5628662109375\n",
      "Epoch 4, Batch 1669, Loss: 495.7268981933594\n",
      "Epoch 4, Batch 1670, Loss: 497.08758544921875\n",
      "Epoch 4, Batch 1671, Loss: 522.8799438476562\n",
      "Epoch 4, Batch 1672, Loss: 543.580078125\n",
      "Epoch 4, Batch 1673, Loss: 535.5994262695312\n",
      "Epoch 4, Batch 1674, Loss: 524.9219970703125\n",
      "Epoch 4, Batch 1675, Loss: 505.853759765625\n",
      "Epoch 4, Batch 1676, Loss: 508.4021911621094\n",
      "Epoch 4, Batch 1677, Loss: 505.5498962402344\n",
      "Epoch 4, Batch 1678, Loss: 494.99310302734375\n",
      "Epoch 4, Batch 1679, Loss: 546.7555541992188\n",
      "Epoch 4, Batch 1680, Loss: 492.52362060546875\n",
      "Epoch 4, Batch 1681, Loss: 530.1109619140625\n",
      "Epoch 4, Batch 1682, Loss: 513.1539306640625\n",
      "Epoch 4, Batch 1683, Loss: 534.5040893554688\n",
      "Epoch 4, Batch 1684, Loss: 502.659912109375\n",
      "Epoch 4, Batch 1685, Loss: 522.6776123046875\n",
      "Epoch 4, Batch 1686, Loss: 522.5912475585938\n",
      "Epoch 4, Batch 1687, Loss: 506.0705261230469\n",
      "Epoch 4, Batch 1688, Loss: 506.56829833984375\n",
      "Epoch 4, Batch 1689, Loss: 495.5486755371094\n",
      "Epoch 4, Batch 1690, Loss: 519.0650634765625\n",
      "Epoch 4, Batch 1691, Loss: 505.7189636230469\n",
      "Epoch 4, Batch 1692, Loss: 540.98095703125\n",
      "Epoch 4, Batch 1693, Loss: 497.7116394042969\n",
      "Epoch 4, Batch 1694, Loss: 537.3728637695312\n",
      "Epoch 4, Batch 1695, Loss: 512.9214477539062\n",
      "Epoch 4, Batch 1696, Loss: 512.4447021484375\n",
      "Epoch 4, Batch 1697, Loss: 522.4580078125\n",
      "Epoch 4, Batch 1698, Loss: 517.87744140625\n",
      "Epoch 4, Batch 1699, Loss: 519.935302734375\n",
      "Epoch 4, Batch 1700, Loss: 516.022216796875\n",
      "Epoch 4, Batch 1701, Loss: 518.4622802734375\n",
      "Epoch 4, Batch 1702, Loss: 505.88360595703125\n",
      "Epoch 4, Batch 1703, Loss: 533.618896484375\n",
      "Epoch 4, Batch 1704, Loss: 507.3639831542969\n",
      "Epoch 4, Batch 1705, Loss: 517.210693359375\n",
      "Epoch 4, Batch 1706, Loss: 523.9195556640625\n",
      "Epoch 4, Batch 1707, Loss: 526.9483642578125\n",
      "Epoch 4, Batch 1708, Loss: 507.6963806152344\n",
      "Epoch 4, Batch 1709, Loss: 521.361083984375\n",
      "Epoch 4, Batch 1710, Loss: 498.62548828125\n",
      "Epoch 4, Batch 1711, Loss: 506.34393310546875\n",
      "Epoch 4, Batch 1712, Loss: 520.7974853515625\n",
      "Epoch 4, Batch 1713, Loss: 526.006103515625\n",
      "Epoch 4, Batch 1714, Loss: 514.0535888671875\n",
      "Epoch 4, Batch 1715, Loss: 527.9549560546875\n",
      "Epoch 4, Batch 1716, Loss: 491.91741943359375\n",
      "Epoch 4, Batch 1717, Loss: 533.3038330078125\n",
      "Epoch 4, Batch 1718, Loss: 495.78387451171875\n",
      "Epoch 4, Batch 1719, Loss: 526.0131225585938\n",
      "Epoch 4, Batch 1720, Loss: 505.24505615234375\n",
      "Epoch 4, Batch 1721, Loss: 502.6370544433594\n",
      "Epoch 4, Batch 1722, Loss: 521.439208984375\n",
      "Epoch 4, Batch 1723, Loss: 508.5249938964844\n",
      "Epoch 4, Batch 1724, Loss: 511.19781494140625\n",
      "Epoch 4, Batch 1725, Loss: 514.1076049804688\n",
      "Epoch 4, Batch 1726, Loss: 495.09423828125\n",
      "Epoch 4, Batch 1727, Loss: 481.78411865234375\n",
      "Epoch 4, Batch 1728, Loss: 549.770751953125\n",
      "Epoch 4, Batch 1729, Loss: 501.745361328125\n",
      "Epoch 4, Batch 1730, Loss: 489.8561706542969\n",
      "Epoch 4, Batch 1731, Loss: 509.526123046875\n",
      "Epoch 4, Batch 1732, Loss: 508.22454833984375\n",
      "Epoch 4, Batch 1733, Loss: 525.18701171875\n",
      "Epoch 4, Batch 1734, Loss: 542.998046875\n",
      "Epoch 4, Batch 1735, Loss: 514.8678588867188\n",
      "Epoch 4, Batch 1736, Loss: 545.7298583984375\n",
      "Epoch 4, Batch 1737, Loss: 535.106201171875\n",
      "Epoch 4, Batch 1738, Loss: 514.991455078125\n",
      "Epoch 4, Batch 1739, Loss: 509.906982421875\n",
      "Epoch 4, Batch 1740, Loss: 517.08935546875\n",
      "Epoch 4, Batch 1741, Loss: 496.67041015625\n",
      "Epoch 4, Batch 1742, Loss: 515.229736328125\n",
      "Epoch 4, Batch 1743, Loss: 531.568115234375\n",
      "Epoch 4, Batch 1744, Loss: 525.8739013671875\n",
      "Epoch 4, Batch 1745, Loss: 501.738525390625\n",
      "Epoch 4, Batch 1746, Loss: 488.4139404296875\n",
      "Epoch 4, Batch 1747, Loss: 541.2647094726562\n",
      "Epoch 4, Batch 1748, Loss: 543.6171875\n",
      "Epoch 4, Batch 1749, Loss: 517.2111206054688\n",
      "Epoch 4, Batch 1750, Loss: 538.3641357421875\n",
      "Epoch 4, Batch 1751, Loss: 524.817138671875\n",
      "Epoch 4, Batch 1752, Loss: 513.5792846679688\n",
      "Epoch 4, Batch 1753, Loss: 529.5828857421875\n",
      "Epoch 4, Batch 1754, Loss: 517.7852783203125\n",
      "Epoch 4, Batch 1755, Loss: 520.0513916015625\n",
      "Epoch 4, Batch 1756, Loss: 544.6923217773438\n",
      "Epoch 4, Batch 1757, Loss: 518.6542358398438\n",
      "Epoch 4, Batch 1758, Loss: 509.9078063964844\n",
      "Epoch 4, Batch 1759, Loss: 523.9022216796875\n",
      "Epoch 4, Batch 1760, Loss: 560.741943359375\n",
      "Epoch 4, Batch 1761, Loss: 526.39453125\n",
      "Epoch 4, Batch 1762, Loss: 498.32373046875\n",
      "Epoch 4, Batch 1763, Loss: 500.9175720214844\n",
      "Epoch 4, Batch 1764, Loss: 511.989501953125\n",
      "Epoch 4, Batch 1765, Loss: 534.242431640625\n",
      "Epoch 4, Batch 1766, Loss: 506.8315124511719\n",
      "Epoch 4, Batch 1767, Loss: 526.2625122070312\n",
      "Epoch 4, Batch 1768, Loss: 498.34466552734375\n",
      "Epoch 4, Batch 1769, Loss: 508.4530029296875\n",
      "Epoch 4, Batch 1770, Loss: 529.1865234375\n",
      "Epoch 4, Batch 1771, Loss: 531.208251953125\n",
      "Epoch 4, Batch 1772, Loss: 505.60418701171875\n",
      "Epoch 4, Batch 1773, Loss: 509.6566162109375\n",
      "Epoch 4, Batch 1774, Loss: 523.7122802734375\n",
      "Epoch 4, Batch 1775, Loss: 495.7880554199219\n",
      "Epoch 4, Batch 1776, Loss: 566.4865112304688\n",
      "Epoch 4, Batch 1777, Loss: 512.7537841796875\n",
      "Epoch 4, Batch 1778, Loss: 523.992431640625\n",
      "Epoch 4, Batch 1779, Loss: 512.231201171875\n",
      "Epoch 4, Batch 1780, Loss: 517.0411376953125\n",
      "Epoch 4, Batch 1781, Loss: 504.30303955078125\n",
      "Epoch 4, Batch 1782, Loss: 491.83441162109375\n",
      "Epoch 4, Batch 1783, Loss: 524.5516967773438\n",
      "Epoch 4, Batch 1784, Loss: 533.8661499023438\n",
      "Epoch 4, Batch 1785, Loss: 544.5819702148438\n",
      "Epoch 4, Batch 1786, Loss: 514.2769165039062\n",
      "Epoch 4, Batch 1787, Loss: 513.6566772460938\n",
      "Epoch 4, Batch 1788, Loss: 493.8408203125\n",
      "Epoch 4, Batch 1789, Loss: 519.5516357421875\n",
      "Epoch 4, Batch 1790, Loss: 513.155029296875\n",
      "Epoch 4, Batch 1791, Loss: 528.756591796875\n",
      "Epoch 4, Batch 1792, Loss: 481.759765625\n",
      "Epoch 4, Batch 1793, Loss: 509.16912841796875\n",
      "Epoch 4, Batch 1794, Loss: 542.1149291992188\n",
      "Epoch 4, Batch 1795, Loss: 483.00286865234375\n",
      "Epoch 4, Batch 1796, Loss: 521.8706665039062\n",
      "Epoch 4, Batch 1797, Loss: 512.5496826171875\n",
      "Epoch 4, Batch 1798, Loss: 487.9313659667969\n",
      "Epoch 4, Batch 1799, Loss: 528.87158203125\n",
      "Epoch 4, Batch 1800, Loss: 522.8231201171875\n",
      "Epoch 4, Batch 1801, Loss: 520.042724609375\n",
      "Epoch 4, Batch 1802, Loss: 499.862060546875\n",
      "Epoch 4, Batch 1803, Loss: 502.36639404296875\n",
      "Epoch 4, Batch 1804, Loss: 533.1988525390625\n",
      "Epoch 4, Batch 1805, Loss: 525.6375732421875\n",
      "Epoch 4, Batch 1806, Loss: 538.3167114257812\n",
      "Epoch 4, Batch 1807, Loss: 526.9271850585938\n",
      "Epoch 4, Batch 1808, Loss: 524.0888671875\n",
      "Epoch 4, Batch 1809, Loss: 516.2582397460938\n",
      "Epoch 4, Batch 1810, Loss: 522.1219482421875\n",
      "Epoch 4, Batch 1811, Loss: 517.8355712890625\n",
      "Epoch 4, Batch 1812, Loss: 517.41650390625\n",
      "Epoch 4, Batch 1813, Loss: 506.544677734375\n",
      "Epoch 4, Batch 1814, Loss: 514.41162109375\n",
      "Epoch 4, Batch 1815, Loss: 510.734375\n",
      "Epoch 4, Batch 1816, Loss: 528.794921875\n",
      "Epoch 4, Batch 1817, Loss: 524.9389038085938\n",
      "Epoch 4, Batch 1818, Loss: 505.6888122558594\n",
      "Epoch 4, Batch 1819, Loss: 514.5272827148438\n",
      "Epoch 4, Batch 1820, Loss: 515.5148315429688\n",
      "Epoch 4, Batch 1821, Loss: 491.05291748046875\n",
      "Epoch 4, Batch 1822, Loss: 528.9610595703125\n",
      "Epoch 4, Batch 1823, Loss: 516.1129150390625\n",
      "Epoch 4, Batch 1824, Loss: 532.638916015625\n",
      "Epoch 4, Batch 1825, Loss: 514.091064453125\n",
      "Epoch 4, Batch 1826, Loss: 553.4510498046875\n",
      "Epoch 4, Batch 1827, Loss: 510.12322998046875\n",
      "Epoch 4, Batch 1828, Loss: 502.330810546875\n",
      "Epoch 4, Batch 1829, Loss: 522.1900634765625\n",
      "Epoch 4, Batch 1830, Loss: 509.74993896484375\n",
      "Epoch 4, Batch 1831, Loss: 501.9572448730469\n",
      "Epoch 4, Batch 1832, Loss: 517.0015258789062\n",
      "Epoch 4, Batch 1833, Loss: 545.8560791015625\n",
      "Epoch 4, Batch 1834, Loss: 497.29241943359375\n",
      "Epoch 4, Batch 1835, Loss: 487.1084899902344\n",
      "Epoch 4, Batch 1836, Loss: 517.70068359375\n",
      "Epoch 4, Batch 1837, Loss: 528.97900390625\n",
      "Epoch 4, Batch 1838, Loss: 521.786376953125\n",
      "Epoch 4, Batch 1839, Loss: 546.82080078125\n",
      "Epoch 4, Batch 1840, Loss: 490.0274353027344\n",
      "Epoch 4, Batch 1841, Loss: 523.3970947265625\n",
      "Epoch 4, Batch 1842, Loss: 492.13671875\n",
      "Epoch 4, Batch 1843, Loss: 507.3355712890625\n",
      "Epoch 4, Batch 1844, Loss: 528.6129150390625\n",
      "Epoch 4, Batch 1845, Loss: 486.33612060546875\n",
      "Epoch 4, Batch 1846, Loss: 540.3272094726562\n",
      "Epoch 4, Batch 1847, Loss: 507.47747802734375\n",
      "Epoch 4, Batch 1848, Loss: 531.871826171875\n",
      "Epoch 4, Batch 1849, Loss: 522.3507080078125\n",
      "Epoch 4, Batch 1850, Loss: 538.25390625\n",
      "Epoch 4, Batch 1851, Loss: 499.5364990234375\n",
      "Epoch 4, Batch 1852, Loss: 513.4556884765625\n",
      "Epoch 4, Batch 1853, Loss: 526.5274658203125\n",
      "Epoch 4, Batch 1854, Loss: 516.8583374023438\n",
      "Epoch 4, Batch 1855, Loss: 549.2284545898438\n",
      "Epoch 4, Batch 1856, Loss: 537.7626342773438\n",
      "Epoch 4, Batch 1857, Loss: 518.3595581054688\n",
      "Epoch 4, Batch 1858, Loss: 517.6126708984375\n",
      "Epoch 4, Batch 1859, Loss: 544.2606811523438\n",
      "Epoch 4, Batch 1860, Loss: 498.2538757324219\n",
      "Epoch 4, Batch 1861, Loss: 534.9469604492188\n",
      "Epoch 4, Batch 1862, Loss: 516.5399169921875\n",
      "Epoch 4, Batch 1863, Loss: 510.1194763183594\n",
      "Epoch 4, Batch 1864, Loss: 508.98089599609375\n",
      "Epoch 4, Batch 1865, Loss: 525.5950927734375\n",
      "Epoch 4, Batch 1866, Loss: 514.381591796875\n",
      "Epoch 4, Batch 1867, Loss: 486.83062744140625\n",
      "Epoch 4, Batch 1868, Loss: 498.38623046875\n",
      "Epoch 4, Batch 1869, Loss: 502.31976318359375\n",
      "Epoch 4, Batch 1870, Loss: 523.5407104492188\n",
      "Epoch 4, Batch 1871, Loss: 511.2211608886719\n",
      "Epoch 4, Batch 1872, Loss: 535.260009765625\n",
      "Epoch 4, Batch 1873, Loss: 524.4456787109375\n",
      "Epoch 4, Batch 1874, Loss: 515.3211669921875\n",
      "Epoch 4, Batch 1875, Loss: 547.0469970703125\n",
      "Epoch 4, Batch 1876, Loss: 501.86785888671875\n",
      "Epoch 4, Batch 1877, Loss: 541.447265625\n",
      "Epoch 4, Batch 1878, Loss: 529.238037109375\n",
      "Epoch 4, Batch 1879, Loss: 534.23486328125\n",
      "Epoch 4, Batch 1880, Loss: 504.2049560546875\n",
      "Epoch 4, Batch 1881, Loss: 524.7543334960938\n",
      "Epoch 4, Batch 1882, Loss: 525.9957275390625\n",
      "Epoch 4, Batch 1883, Loss: 489.1120300292969\n",
      "Epoch 4, Batch 1884, Loss: 505.08740234375\n",
      "Epoch 4, Batch 1885, Loss: 510.5242614746094\n",
      "Epoch 4, Batch 1886, Loss: 530.43115234375\n",
      "Epoch 4, Batch 1887, Loss: 520.5426025390625\n",
      "Epoch 4, Batch 1888, Loss: 529.1666259765625\n",
      "Epoch 4, Batch 1889, Loss: 520.4608154296875\n",
      "Epoch 4, Batch 1890, Loss: 509.732177734375\n",
      "Epoch 4, Batch 1891, Loss: 521.056640625\n",
      "Epoch 4, Batch 1892, Loss: 498.52117919921875\n",
      "Epoch 4, Batch 1893, Loss: 520.48779296875\n",
      "Epoch 4, Batch 1894, Loss: 509.8221130371094\n",
      "Epoch 4, Batch 1895, Loss: 515.736572265625\n",
      "Epoch 4, Batch 1896, Loss: 478.65936279296875\n",
      "Epoch 4, Batch 1897, Loss: 528.3363647460938\n",
      "Epoch 4, Batch 1898, Loss: 494.2795104980469\n",
      "Epoch 4, Batch 1899, Loss: 531.361083984375\n",
      "Epoch 4, Batch 1900, Loss: 526.11572265625\n",
      "Epoch 4, Batch 1901, Loss: 513.0008544921875\n",
      "Epoch 4, Batch 1902, Loss: 520.0780639648438\n",
      "Epoch 4, Batch 1903, Loss: 517.8419799804688\n",
      "Epoch 4, Batch 1904, Loss: 504.6081237792969\n",
      "Epoch 4, Batch 1905, Loss: 529.9716186523438\n",
      "Epoch 4, Batch 1906, Loss: 537.09619140625\n",
      "Epoch 4, Batch 1907, Loss: 499.1217041015625\n",
      "Epoch 4, Batch 1908, Loss: 508.9639587402344\n",
      "Epoch 4, Batch 1909, Loss: 525.5941162109375\n",
      "Epoch 4, Batch 1910, Loss: 515.2725219726562\n",
      "Epoch 4, Batch 1911, Loss: 529.5489501953125\n",
      "Epoch 4, Batch 1912, Loss: 533.4059448242188\n",
      "Epoch 4, Batch 1913, Loss: 531.4630737304688\n",
      "Epoch 4, Batch 1914, Loss: 547.1310424804688\n",
      "Epoch 4, Batch 1915, Loss: 480.480224609375\n",
      "Epoch 4, Batch 1916, Loss: 527.427490234375\n",
      "Epoch 4, Batch 1917, Loss: 493.2225036621094\n",
      "Epoch 4, Batch 1918, Loss: 540.5618896484375\n",
      "Epoch 4, Batch 1919, Loss: 513.014404296875\n",
      "Epoch 4, Batch 1920, Loss: 535.0242309570312\n",
      "Epoch 4, Batch 1921, Loss: 514.955078125\n",
      "Epoch 4, Batch 1922, Loss: 510.157958984375\n",
      "Epoch 4, Batch 1923, Loss: 499.14251708984375\n",
      "Epoch 4, Batch 1924, Loss: 504.9684753417969\n",
      "Epoch 4, Batch 1925, Loss: 506.83123779296875\n",
      "Epoch 4, Batch 1926, Loss: 501.7861328125\n",
      "Epoch 4, Batch 1927, Loss: 523.79736328125\n",
      "Epoch 4, Batch 1928, Loss: 510.2381591796875\n",
      "Epoch 4, Batch 1929, Loss: 528.7979736328125\n",
      "Epoch 4, Batch 1930, Loss: 492.7248229980469\n",
      "Epoch 4, Batch 1931, Loss: 522.734130859375\n",
      "Epoch 4, Batch 1932, Loss: 498.6328125\n",
      "Epoch 4, Batch 1933, Loss: 512.929443359375\n",
      "Epoch 4, Batch 1934, Loss: 506.6531677246094\n",
      "Epoch 4, Batch 1935, Loss: 493.22900390625\n",
      "Epoch 4, Batch 1936, Loss: 497.8699645996094\n",
      "Epoch 4, Batch 1937, Loss: 531.437744140625\n",
      "Epoch 4, Batch 1938, Loss: 520.1353759765625\n",
      "Epoch 4, Batch 1939, Loss: 503.2818298339844\n",
      "Epoch 4, Batch 1940, Loss: 515.18896484375\n",
      "Epoch 4, Batch 1941, Loss: 528.5338134765625\n",
      "Epoch 4, Batch 1942, Loss: 534.3974609375\n",
      "Epoch 4, Batch 1943, Loss: 503.570068359375\n",
      "Epoch 4, Batch 1944, Loss: 498.2242126464844\n",
      "Epoch 4, Batch 1945, Loss: 533.5667114257812\n",
      "Epoch 4, Batch 1946, Loss: 484.10357666015625\n",
      "Epoch 4, Batch 1947, Loss: 504.66387939453125\n",
      "Epoch 4, Batch 1948, Loss: 515.6637573242188\n",
      "Epoch 4, Batch 1949, Loss: 507.91595458984375\n",
      "Epoch 4, Batch 1950, Loss: 514.2313842773438\n",
      "Epoch 4, Batch 1951, Loss: 516.0849609375\n",
      "Epoch 4, Batch 1952, Loss: 505.3984375\n",
      "Epoch 4, Batch 1953, Loss: 504.15521240234375\n",
      "Epoch 4, Batch 1954, Loss: 529.6051025390625\n",
      "Epoch 4, Batch 1955, Loss: 537.9017944335938\n",
      "Epoch 4, Batch 1956, Loss: 506.870361328125\n",
      "Epoch 4, Batch 1957, Loss: 519.461669921875\n",
      "Epoch 4, Batch 1958, Loss: 504.7376708984375\n",
      "Epoch 4, Batch 1959, Loss: 521.6077270507812\n",
      "Epoch 4, Batch 1960, Loss: 499.59320068359375\n",
      "Epoch 4, Batch 1961, Loss: 498.7950134277344\n",
      "Epoch 4, Batch 1962, Loss: 513.429931640625\n",
      "Epoch 4, Batch 1963, Loss: 498.9949951171875\n",
      "Epoch 4, Batch 1964, Loss: 519.3770141601562\n",
      "Epoch 4, Batch 1965, Loss: 509.4072265625\n",
      "Epoch 4, Batch 1966, Loss: 523.5927124023438\n",
      "Epoch 4, Batch 1967, Loss: 534.37158203125\n",
      "Epoch 4, Batch 1968, Loss: 515.7412109375\n",
      "Epoch 4, Batch 1969, Loss: 506.1368103027344\n",
      "Epoch 4, Batch 1970, Loss: 506.7675476074219\n",
      "Epoch 4, Batch 1971, Loss: 521.11181640625\n",
      "Epoch 4, Batch 1972, Loss: 546.0324096679688\n",
      "Epoch 4, Batch 1973, Loss: 502.9899597167969\n",
      "Epoch 4, Batch 1974, Loss: 529.0014038085938\n",
      "Epoch 4, Batch 1975, Loss: 524.7333984375\n",
      "Epoch 4, Batch 1976, Loss: 501.84881591796875\n",
      "Epoch 4, Batch 1977, Loss: 519.5704956054688\n",
      "Epoch 4, Batch 1978, Loss: 512.7532958984375\n",
      "Epoch 4, Batch 1979, Loss: 482.33001708984375\n",
      "Epoch 4, Batch 1980, Loss: 538.853271484375\n",
      "Epoch 4, Batch 1981, Loss: 509.8384704589844\n",
      "Epoch 4, Batch 1982, Loss: 486.7073669433594\n",
      "Epoch 4, Batch 1983, Loss: 512.2069702148438\n",
      "Epoch 4, Batch 1984, Loss: 511.1863708496094\n",
      "Epoch 4, Batch 1985, Loss: 506.46942138671875\n",
      "Epoch 4, Batch 1986, Loss: 525.159423828125\n",
      "Epoch 4, Batch 1987, Loss: 523.0192260742188\n",
      "Epoch 4, Batch 1988, Loss: 507.42974853515625\n",
      "Epoch 4, Batch 1989, Loss: 536.310302734375\n",
      "Epoch 4, Batch 1990, Loss: 498.39886474609375\n",
      "Epoch 4, Batch 1991, Loss: 504.27777099609375\n",
      "Epoch 4, Batch 1992, Loss: 547.8956909179688\n",
      "Epoch 4, Batch 1993, Loss: 498.68060302734375\n",
      "Epoch 4, Batch 1994, Loss: 525.62646484375\n",
      "Epoch 4, Batch 1995, Loss: 523.13623046875\n",
      "Epoch 4, Batch 1996, Loss: 509.4261169433594\n",
      "Epoch 4, Batch 1997, Loss: 516.5783081054688\n",
      "Epoch 4, Batch 1998, Loss: 509.9539489746094\n",
      "Epoch 4, Batch 1999, Loss: 515.8623657226562\n",
      "Epoch 4, Batch 2000, Loss: 507.40911865234375\n",
      "Epoch 4, Batch 2001, Loss: 514.2783203125\n",
      "Epoch 4, Batch 2002, Loss: 539.014892578125\n",
      "Epoch 4, Batch 2003, Loss: 495.56494140625\n",
      "Epoch 4, Batch 2004, Loss: 519.6870727539062\n",
      "Epoch 4, Batch 2005, Loss: 502.58203125\n",
      "Epoch 4, Batch 2006, Loss: 516.0613403320312\n",
      "Epoch 4, Batch 2007, Loss: 501.4517517089844\n",
      "Epoch 4, Batch 2008, Loss: 483.5730895996094\n",
      "Epoch 4, Batch 2009, Loss: 499.684326171875\n",
      "Epoch 4, Batch 2010, Loss: 537.092041015625\n",
      "Epoch 4, Batch 2011, Loss: 526.988037109375\n",
      "Epoch 4, Batch 2012, Loss: 509.4646911621094\n",
      "Epoch 4, Batch 2013, Loss: 513.6080322265625\n",
      "Epoch 4, Batch 2014, Loss: 517.3327026367188\n",
      "Epoch 4, Batch 2015, Loss: 473.73052978515625\n",
      "Epoch 4, Batch 2016, Loss: 526.7855224609375\n",
      "Epoch 4, Batch 2017, Loss: 500.8045349121094\n",
      "Epoch 4, Batch 2018, Loss: 548.5714721679688\n",
      "Epoch 4, Batch 2019, Loss: 508.9620056152344\n",
      "Epoch 4, Batch 2020, Loss: 534.873291015625\n",
      "Epoch 4, Batch 2021, Loss: 531.8884887695312\n",
      "Epoch 4, Batch 2022, Loss: 493.23193359375\n",
      "Epoch 4, Batch 2023, Loss: 559.8139038085938\n",
      "Epoch 4, Batch 2024, Loss: 540.1055908203125\n",
      "Epoch 4, Batch 2025, Loss: 509.24493408203125\n",
      "Epoch 4, Batch 2026, Loss: 529.39306640625\n",
      "Epoch 4, Batch 2027, Loss: 500.06414794921875\n",
      "Epoch 4, Batch 2028, Loss: 482.72918701171875\n",
      "Epoch 4, Batch 2029, Loss: 518.7582397460938\n",
      "Epoch 4, Batch 2030, Loss: 477.0721130371094\n",
      "Epoch 4, Batch 2031, Loss: 483.9072265625\n",
      "Epoch 4, Batch 2032, Loss: 535.3316650390625\n",
      "Epoch 4, Batch 2033, Loss: 492.9666748046875\n",
      "Epoch 4, Batch 2034, Loss: 492.60833740234375\n",
      "Epoch 4, Batch 2035, Loss: 512.005126953125\n",
      "Epoch 4, Batch 2036, Loss: 541.2717895507812\n",
      "Epoch 4, Batch 2037, Loss: 546.325439453125\n",
      "Epoch 4, Batch 2038, Loss: 500.144287109375\n",
      "Epoch 4, Batch 2039, Loss: 490.37646484375\n",
      "Epoch 4, Batch 2040, Loss: 530.3964233398438\n",
      "Epoch 4, Batch 2041, Loss: 508.61962890625\n",
      "Epoch 4, Batch 2042, Loss: 523.2969970703125\n",
      "Epoch 4, Batch 2043, Loss: 551.0358276367188\n",
      "Epoch 4, Batch 2044, Loss: 499.0694274902344\n",
      "Epoch 4, Batch 2045, Loss: 515.2593994140625\n",
      "Epoch 4, Batch 2046, Loss: 536.3612670898438\n",
      "Epoch 4, Batch 2047, Loss: 529.8831787109375\n",
      "Epoch 4, Batch 2048, Loss: 524.4276123046875\n",
      "Epoch 4, Batch 2049, Loss: 539.8389282226562\n",
      "Epoch 4, Batch 2050, Loss: 546.452392578125\n",
      "Epoch 4, Batch 2051, Loss: 549.915283203125\n",
      "Epoch 4, Batch 2052, Loss: 525.6212158203125\n",
      "Epoch 4, Batch 2053, Loss: 507.66595458984375\n",
      "Epoch 4, Batch 2054, Loss: 516.2838134765625\n",
      "Epoch 4, Batch 2055, Loss: 492.43157958984375\n",
      "Epoch 4, Batch 2056, Loss: 507.577392578125\n",
      "Epoch 4, Batch 2057, Loss: 532.9888916015625\n",
      "Epoch 4, Batch 2058, Loss: 528.80419921875\n",
      "Epoch 4, Batch 2059, Loss: 515.88232421875\n",
      "Epoch 4, Batch 2060, Loss: 498.3328552246094\n",
      "Epoch 4, Batch 2061, Loss: 501.9552001953125\n",
      "Epoch 4, Batch 2062, Loss: 532.877197265625\n",
      "Epoch 4, Batch 2063, Loss: 506.34222412109375\n",
      "Epoch 4, Batch 2064, Loss: 494.0076599121094\n",
      "Epoch 4, Batch 2065, Loss: 530.1515502929688\n",
      "Epoch 4, Batch 2066, Loss: 528.1864013671875\n",
      "Epoch 4, Batch 2067, Loss: 504.298095703125\n",
      "Epoch 4, Batch 2068, Loss: 512.7205810546875\n",
      "Epoch 4, Batch 2069, Loss: 520.1449584960938\n",
      "Epoch 4, Batch 2070, Loss: 519.38330078125\n",
      "Epoch 4, Batch 2071, Loss: 511.3050537109375\n",
      "Epoch 4, Batch 2072, Loss: 527.14111328125\n",
      "Epoch 4, Batch 2073, Loss: 526.6041259765625\n",
      "Epoch 4, Batch 2074, Loss: 524.4159545898438\n",
      "Epoch 4, Batch 2075, Loss: 506.04278564453125\n",
      "Epoch 4, Batch 2076, Loss: 510.9380798339844\n",
      "Epoch 4, Batch 2077, Loss: 533.877197265625\n",
      "Epoch 4, Batch 2078, Loss: 488.2044677734375\n",
      "Epoch 4, Batch 2079, Loss: 531.2980346679688\n",
      "Epoch 4, Batch 2080, Loss: 519.4168090820312\n",
      "Epoch 4, Batch 2081, Loss: 522.0147705078125\n",
      "Epoch 4, Batch 2082, Loss: 515.1681518554688\n",
      "Epoch 4, Batch 2083, Loss: 495.6233825683594\n",
      "Epoch 4, Batch 2084, Loss: 508.887451171875\n",
      "Epoch 4, Batch 2085, Loss: 508.72381591796875\n",
      "Epoch 4, Batch 2086, Loss: 564.0257568359375\n",
      "Epoch 4, Batch 2087, Loss: 528.3449096679688\n",
      "Epoch 4, Batch 2088, Loss: 517.6000366210938\n",
      "Epoch 4, Batch 2089, Loss: 528.394287109375\n",
      "Epoch 4, Batch 2090, Loss: 495.00555419921875\n",
      "Epoch 4, Batch 2091, Loss: 504.615234375\n",
      "Epoch 4, Batch 2092, Loss: 507.9248962402344\n",
      "Epoch 4, Batch 2093, Loss: 514.6171264648438\n",
      "Epoch 4, Batch 2094, Loss: 541.1542358398438\n",
      "Epoch 4, Batch 2095, Loss: 508.43072509765625\n",
      "Epoch 4, Batch 2096, Loss: 499.8284606933594\n",
      "Epoch 4, Batch 2097, Loss: 535.8700561523438\n",
      "Epoch 4, Batch 2098, Loss: 522.5936889648438\n",
      "Epoch 4, Batch 2099, Loss: 512.6429443359375\n",
      "Epoch 4, Batch 2100, Loss: 549.5166015625\n",
      "Epoch 4, Batch 2101, Loss: 530.4537353515625\n",
      "Epoch 4, Batch 2102, Loss: 509.9351501464844\n",
      "Epoch 4, Batch 2103, Loss: 521.3515625\n",
      "Epoch 4, Batch 2104, Loss: 498.0923767089844\n",
      "Epoch 4, Batch 2105, Loss: 511.15325927734375\n",
      "Epoch 4, Batch 2106, Loss: 530.2355346679688\n",
      "Epoch 4, Batch 2107, Loss: 532.0393676757812\n",
      "Epoch 4, Batch 2108, Loss: 499.1568298339844\n",
      "Epoch 4, Batch 2109, Loss: 533.1282348632812\n",
      "Epoch 4, Batch 2110, Loss: 512.5233154296875\n",
      "Epoch 4, Batch 2111, Loss: 493.641845703125\n",
      "Epoch 4, Batch 2112, Loss: 523.8004760742188\n",
      "Epoch 4, Batch 2113, Loss: 541.7977294921875\n",
      "Epoch 4, Batch 2114, Loss: 514.9630737304688\n",
      "Epoch 4, Batch 2115, Loss: 519.2781982421875\n",
      "Epoch 4, Batch 2116, Loss: 534.030029296875\n",
      "Epoch 4, Batch 2117, Loss: 532.51416015625\n",
      "Epoch 4, Batch 2118, Loss: 528.7152099609375\n",
      "Epoch 4, Batch 2119, Loss: 512.521728515625\n",
      "Epoch 4, Batch 2120, Loss: 513.0650634765625\n",
      "Epoch 4, Batch 2121, Loss: 522.1253662109375\n",
      "Epoch 4, Batch 2122, Loss: 509.7697448730469\n",
      "Epoch 4, Batch 2123, Loss: 517.76513671875\n",
      "Epoch 4, Batch 2124, Loss: 499.7261962890625\n",
      "Epoch 4, Batch 2125, Loss: 516.9993896484375\n",
      "Epoch 4, Batch 2126, Loss: 508.4432067871094\n",
      "Epoch 4, Batch 2127, Loss: 526.8260498046875\n",
      "Epoch 4, Batch 2128, Loss: 472.016357421875\n",
      "Epoch 4, Batch 2129, Loss: 514.0933227539062\n",
      "Epoch 4, Batch 2130, Loss: 517.6544799804688\n",
      "Epoch 4, Batch 2131, Loss: 540.6927490234375\n",
      "Epoch 4, Batch 2132, Loss: 480.9842224121094\n",
      "Epoch 4, Batch 2133, Loss: 503.3892517089844\n",
      "Epoch 4, Batch 2134, Loss: 521.0897827148438\n",
      "Epoch 4, Batch 2135, Loss: 531.2844848632812\n",
      "Epoch 4, Batch 2136, Loss: 498.62811279296875\n",
      "Epoch 4, Batch 2137, Loss: 551.8118896484375\n",
      "Epoch 4, Batch 2138, Loss: 493.8433532714844\n",
      "Epoch 4, Batch 2139, Loss: 522.1058349609375\n",
      "Epoch 4, Batch 2140, Loss: 521.2413330078125\n",
      "Epoch 4, Batch 2141, Loss: 512.898681640625\n",
      "Epoch 4, Batch 2142, Loss: 507.46087646484375\n",
      "Epoch 4, Batch 2143, Loss: 548.169921875\n",
      "Epoch 4, Batch 2144, Loss: 524.8033447265625\n",
      "Epoch 4, Batch 2145, Loss: 516.962158203125\n",
      "Epoch 4, Batch 2146, Loss: 555.4706420898438\n",
      "Epoch 4, Batch 2147, Loss: 534.7901000976562\n",
      "Epoch 4, Batch 2148, Loss: 494.5791931152344\n",
      "Epoch 4, Batch 2149, Loss: 500.8389587402344\n",
      "Epoch 4, Batch 2150, Loss: 524.5021362304688\n",
      "Epoch 4, Batch 2151, Loss: 502.39862060546875\n",
      "Epoch 4, Batch 2152, Loss: 530.2653198242188\n",
      "Epoch 4, Batch 2153, Loss: 509.76513671875\n",
      "Epoch 4, Batch 2154, Loss: 521.6998901367188\n",
      "Epoch 4, Batch 2155, Loss: 514.6029663085938\n",
      "Epoch 4, Batch 2156, Loss: 524.4271240234375\n",
      "Epoch 4, Batch 2157, Loss: 541.5172729492188\n",
      "Epoch 4, Batch 2158, Loss: 516.57177734375\n",
      "Epoch 4, Batch 2159, Loss: 483.5981140136719\n",
      "Epoch 4, Batch 2160, Loss: 499.771240234375\n",
      "Epoch 4, Batch 2161, Loss: 534.3067626953125\n",
      "Epoch 4, Batch 2162, Loss: 534.3062744140625\n",
      "Epoch 4, Batch 2163, Loss: 498.20538330078125\n",
      "Epoch 4, Batch 2164, Loss: 550.2396240234375\n",
      "Epoch 4, Batch 2165, Loss: 495.10687255859375\n",
      "Epoch 4, Batch 2166, Loss: 521.3667602539062\n",
      "Epoch 4, Batch 2167, Loss: 511.01153564453125\n",
      "Epoch 4, Batch 2168, Loss: 532.8600463867188\n",
      "Epoch 4, Batch 2169, Loss: 499.73480224609375\n",
      "Epoch 4, Batch 2170, Loss: 533.648193359375\n",
      "Epoch 4, Batch 2171, Loss: 528.7711181640625\n",
      "Epoch 4, Batch 2172, Loss: 488.204345703125\n",
      "Epoch 4, Batch 2173, Loss: 523.6257934570312\n",
      "Epoch 4, Batch 2174, Loss: 494.41845703125\n",
      "Epoch 4, Batch 2175, Loss: 493.32135009765625\n",
      "Epoch 4, Batch 2176, Loss: 497.5321960449219\n",
      "Epoch 4, Batch 2177, Loss: 524.8616943359375\n",
      "Epoch 4, Batch 2178, Loss: 515.3424072265625\n",
      "Epoch 4, Batch 2179, Loss: 533.0016479492188\n",
      "Epoch 4, Batch 2180, Loss: 500.6548767089844\n",
      "Epoch 4, Batch 2181, Loss: 479.235107421875\n",
      "Epoch 4, Batch 2182, Loss: 518.2057495117188\n",
      "Epoch 4, Batch 2183, Loss: 534.07666015625\n",
      "Epoch 4, Batch 2184, Loss: 523.117919921875\n",
      "Epoch 4, Batch 2185, Loss: 548.846923828125\n",
      "Epoch 4, Batch 2186, Loss: 519.9760131835938\n",
      "Epoch 4, Batch 2187, Loss: 526.6519775390625\n",
      "Epoch 4, Batch 2188, Loss: 514.1224365234375\n",
      "Epoch 4, Batch 2189, Loss: 510.35821533203125\n",
      "Epoch 4, Batch 2190, Loss: 516.4056396484375\n",
      "Epoch 4, Batch 2191, Loss: 508.968017578125\n",
      "Epoch 4, Batch 2192, Loss: 519.328125\n",
      "Epoch 4, Batch 2193, Loss: 516.3018798828125\n",
      "Epoch 4, Batch 2194, Loss: 516.0805053710938\n",
      "Epoch 4, Batch 2195, Loss: 488.72796630859375\n",
      "Epoch 4, Batch 2196, Loss: 500.8404541015625\n",
      "Epoch 4, Batch 2197, Loss: 546.0578002929688\n",
      "Epoch 4, Batch 2198, Loss: 510.5525817871094\n",
      "Epoch 4, Batch 2199, Loss: 507.37939453125\n",
      "Epoch 4, Batch 2200, Loss: 504.4562683105469\n",
      "Epoch 4, Batch 2201, Loss: 523.6719970703125\n",
      "Epoch 4, Batch 2202, Loss: 498.3134765625\n",
      "Epoch 4, Batch 2203, Loss: 506.9073181152344\n",
      "Epoch 4, Batch 2204, Loss: 531.7101440429688\n",
      "Epoch 4, Batch 2205, Loss: 559.9728393554688\n",
      "Epoch 4, Batch 2206, Loss: 497.18756103515625\n",
      "Epoch 4, Batch 2207, Loss: 521.7992553710938\n",
      "Epoch 4, Batch 2208, Loss: 529.5396728515625\n",
      "Epoch 4, Batch 2209, Loss: 537.539794921875\n",
      "Epoch 4, Batch 2210, Loss: 524.3471069335938\n",
      "Epoch 4, Batch 2211, Loss: 533.9796752929688\n",
      "Epoch 4, Batch 2212, Loss: 528.2996826171875\n",
      "Epoch 4, Batch 2213, Loss: 537.7077026367188\n",
      "Epoch 4, Batch 2214, Loss: 499.8536071777344\n",
      "Epoch 4, Batch 2215, Loss: 514.7662963867188\n",
      "Epoch 4, Batch 2216, Loss: 525.355712890625\n",
      "Epoch 4, Batch 2217, Loss: 503.58905029296875\n",
      "Epoch 4, Batch 2218, Loss: 532.7158813476562\n",
      "Epoch 4, Batch 2219, Loss: 492.3184509277344\n",
      "Epoch 4, Batch 2220, Loss: 496.29925537109375\n",
      "Epoch 4, Batch 2221, Loss: 515.6809692382812\n",
      "Epoch 4, Batch 2222, Loss: 513.6539306640625\n",
      "Epoch 4, Batch 2223, Loss: 507.9272155761719\n",
      "Epoch 4, Batch 2224, Loss: 521.1551513671875\n",
      "Epoch 4, Batch 2225, Loss: 501.1293640136719\n",
      "Epoch 4, Batch 2226, Loss: 557.9794311523438\n",
      "Epoch 4, Batch 2227, Loss: 523.7587890625\n",
      "Epoch 4, Batch 2228, Loss: 520.8541259765625\n",
      "Epoch 4, Batch 2229, Loss: 516.3024291992188\n",
      "Epoch 4, Batch 2230, Loss: 512.00439453125\n",
      "Epoch 4, Batch 2231, Loss: 530.4766845703125\n",
      "Epoch 4, Batch 2232, Loss: 529.9848022460938\n",
      "Epoch 4, Batch 2233, Loss: 508.12542724609375\n",
      "Epoch 4, Batch 2234, Loss: 499.6614685058594\n",
      "Epoch 4, Batch 2235, Loss: 481.4102478027344\n",
      "Epoch 4, Batch 2236, Loss: 558.5409545898438\n",
      "Epoch 4, Batch 2237, Loss: 503.57177734375\n",
      "Epoch 4, Batch 2238, Loss: 551.7847900390625\n",
      "Epoch 4, Batch 2239, Loss: 493.66796875\n",
      "Epoch 4, Batch 2240, Loss: 491.303466796875\n",
      "Epoch 4, Batch 2241, Loss: 537.3497924804688\n",
      "Epoch 4, Batch 2242, Loss: 513.23388671875\n",
      "Epoch 4, Batch 2243, Loss: 522.939453125\n",
      "Epoch 4, Batch 2244, Loss: 542.7783203125\n",
      "Epoch 4, Batch 2245, Loss: 542.0460815429688\n",
      "Epoch 4, Batch 2246, Loss: 513.5360717773438\n",
      "Epoch 4, Batch 2247, Loss: 538.24853515625\n",
      "Epoch 4, Batch 2248, Loss: 508.31689453125\n",
      "Epoch 4, Batch 2249, Loss: 551.4539794921875\n",
      "Epoch 4, Batch 2250, Loss: 503.77154541015625\n",
      "Epoch 4, Batch 2251, Loss: 530.2847290039062\n",
      "Epoch 4, Batch 2252, Loss: 527.7705078125\n",
      "Epoch 4, Batch 2253, Loss: 511.22698974609375\n",
      "Epoch 4, Batch 2254, Loss: 514.2401123046875\n",
      "Epoch 4, Batch 2255, Loss: 488.6053161621094\n",
      "Epoch 4, Batch 2256, Loss: 508.704345703125\n",
      "Epoch 4, Batch 2257, Loss: 505.6854248046875\n",
      "Epoch 4, Batch 2258, Loss: 508.4248046875\n",
      "Epoch 4, Batch 2259, Loss: 524.6600952148438\n",
      "Epoch 4, Batch 2260, Loss: 513.6422119140625\n",
      "Epoch 4, Batch 2261, Loss: 498.2560729980469\n",
      "Epoch 4, Batch 2262, Loss: 512.593994140625\n",
      "Epoch 4, Batch 2263, Loss: 529.0950927734375\n",
      "Epoch 4, Batch 2264, Loss: 521.974853515625\n",
      "Epoch 4, Batch 2265, Loss: 518.5675048828125\n",
      "Epoch 4, Batch 2266, Loss: 522.1619873046875\n",
      "Epoch 4, Batch 2267, Loss: 501.3870849609375\n",
      "Epoch 4, Batch 2268, Loss: 534.4116821289062\n",
      "Epoch 4, Batch 2269, Loss: 522.2904663085938\n",
      "Epoch 4, Batch 2270, Loss: 530.4805908203125\n",
      "Epoch 4, Batch 2271, Loss: 496.2404479980469\n",
      "Epoch 4, Batch 2272, Loss: 541.909912109375\n",
      "Epoch 4, Batch 2273, Loss: 535.675048828125\n",
      "Epoch 4, Batch 2274, Loss: 513.4940795898438\n",
      "Epoch 4, Batch 2275, Loss: 534.3341064453125\n",
      "Epoch 4, Batch 2276, Loss: 531.529541015625\n",
      "Epoch 4, Batch 2277, Loss: 496.7955017089844\n",
      "Epoch 4, Batch 2278, Loss: 496.8753356933594\n",
      "Epoch 4, Batch 2279, Loss: 508.118896484375\n",
      "Epoch 4, Batch 2280, Loss: 515.6356201171875\n",
      "Epoch 4, Batch 2281, Loss: 503.46734619140625\n",
      "Epoch 4, Batch 2282, Loss: 554.290771484375\n",
      "Epoch 4, Batch 2283, Loss: 525.7900390625\n",
      "Epoch 4, Batch 2284, Loss: 542.6900634765625\n",
      "Epoch 4, Batch 2285, Loss: 499.378662109375\n",
      "Epoch 4, Batch 2286, Loss: 534.7494506835938\n",
      "Epoch 4, Batch 2287, Loss: 510.24530029296875\n",
      "Epoch 4, Batch 2288, Loss: 550.98193359375\n",
      "Epoch 4, Batch 2289, Loss: 533.2073974609375\n",
      "Epoch 4, Batch 2290, Loss: 504.9207458496094\n",
      "Epoch 4, Batch 2291, Loss: 520.7928466796875\n",
      "Epoch 4, Batch 2292, Loss: 517.98779296875\n",
      "Epoch 4, Batch 2293, Loss: 490.22723388671875\n",
      "Epoch 4, Batch 2294, Loss: 502.31695556640625\n",
      "Epoch 4, Batch 2295, Loss: 509.60736083984375\n",
      "Epoch 4, Batch 2296, Loss: 498.60003662109375\n",
      "Epoch 4, Batch 2297, Loss: 519.0994873046875\n",
      "Epoch 4, Batch 2298, Loss: 505.078125\n",
      "Epoch 4, Batch 2299, Loss: 508.67291259765625\n",
      "Epoch 4, Batch 2300, Loss: 536.110107421875\n",
      "Epoch 4, Batch 2301, Loss: 521.2181396484375\n",
      "Epoch 4, Batch 2302, Loss: 490.9489440917969\n",
      "Epoch 4, Batch 2303, Loss: 506.0341796875\n",
      "Epoch 4, Batch 2304, Loss: 539.1553344726562\n",
      "Epoch 4, Batch 2305, Loss: 540.9161376953125\n",
      "Epoch 4, Batch 2306, Loss: 507.95574951171875\n",
      "Epoch 4, Batch 2307, Loss: 493.2064208984375\n",
      "Epoch 4, Batch 2308, Loss: 524.7965698242188\n",
      "Epoch 4, Batch 2309, Loss: 523.7583618164062\n",
      "Epoch 4, Batch 2310, Loss: 502.36572265625\n",
      "Epoch 4, Batch 2311, Loss: 522.0921630859375\n",
      "Epoch 4, Batch 2312, Loss: 512.1448974609375\n",
      "Epoch 4, Batch 2313, Loss: 538.1373901367188\n",
      "Epoch 4, Batch 2314, Loss: 522.197265625\n",
      "Epoch 4, Batch 2315, Loss: 509.7908630371094\n",
      "Epoch 4, Batch 2316, Loss: 512.507080078125\n",
      "Epoch 4, Batch 2317, Loss: 480.2515869140625\n",
      "Epoch 4, Batch 2318, Loss: 541.8389892578125\n",
      "Epoch 4, Batch 2319, Loss: 517.6654663085938\n",
      "Epoch 4, Batch 2320, Loss: 528.2369384765625\n",
      "Epoch 4, Batch 2321, Loss: 521.058837890625\n",
      "Epoch 4, Batch 2322, Loss: 514.443603515625\n",
      "Epoch 4, Batch 2323, Loss: 510.42401123046875\n",
      "Epoch 4, Batch 2324, Loss: 501.66497802734375\n",
      "Epoch 4, Batch 2325, Loss: 506.33184814453125\n",
      "Epoch 4, Batch 2326, Loss: 497.66790771484375\n",
      "Epoch 4, Batch 2327, Loss: 513.3595581054688\n",
      "Epoch 4, Batch 2328, Loss: 527.4486083984375\n",
      "Epoch 4, Batch 2329, Loss: 518.7606811523438\n",
      "Epoch 4, Batch 2330, Loss: 526.9107666015625\n",
      "Epoch 4, Batch 2331, Loss: 500.35748291015625\n",
      "Epoch 4, Batch 2332, Loss: 501.06072998046875\n",
      "Epoch 4, Batch 2333, Loss: 503.16943359375\n",
      "Epoch 4, Batch 2334, Loss: 516.8502197265625\n",
      "Epoch 4, Batch 2335, Loss: 537.9238891601562\n",
      "Epoch 4, Batch 2336, Loss: 487.6259765625\n",
      "Epoch 4, Batch 2337, Loss: 529.3634033203125\n",
      "Epoch 4, Batch 2338, Loss: 532.60205078125\n",
      "Epoch 4, Batch 2339, Loss: 528.6207885742188\n",
      "Epoch 4, Batch 2340, Loss: 528.1082763671875\n",
      "Epoch 4, Batch 2341, Loss: 503.92242431640625\n",
      "Epoch 4, Batch 2342, Loss: 516.2432861328125\n",
      "Epoch 4, Batch 2343, Loss: 546.5592041015625\n",
      "Epoch 4, Batch 2344, Loss: 515.6043701171875\n",
      "Epoch 4, Batch 2345, Loss: 532.9069213867188\n",
      "Epoch 4, Batch 2346, Loss: 531.348876953125\n",
      "Epoch 4, Batch 2347, Loss: 493.4581298828125\n",
      "Epoch 4, Batch 2348, Loss: 509.9501647949219\n",
      "Epoch 4, Batch 2349, Loss: 526.3947143554688\n",
      "Epoch 4, Batch 2350, Loss: 493.0667419433594\n",
      "Epoch 4, Batch 2351, Loss: 546.524658203125\n",
      "Epoch 4, Batch 2352, Loss: 511.31817626953125\n",
      "Epoch 4, Batch 2353, Loss: 527.9334716796875\n",
      "Epoch 4, Batch 2354, Loss: 533.6292114257812\n",
      "Epoch 4, Batch 2355, Loss: 499.43817138671875\n",
      "Epoch 4, Batch 2356, Loss: 486.98345947265625\n",
      "Epoch 4, Batch 2357, Loss: 523.2218017578125\n",
      "Epoch 4, Batch 2358, Loss: 515.926025390625\n",
      "Epoch 4, Batch 2359, Loss: 507.6209411621094\n",
      "Epoch 4, Batch 2360, Loss: 526.7149658203125\n",
      "Epoch 4, Batch 2361, Loss: 524.2127685546875\n",
      "Epoch 4, Batch 2362, Loss: 509.8605651855469\n",
      "Epoch 4, Batch 2363, Loss: 519.444580078125\n",
      "Epoch 4, Batch 2364, Loss: 517.635498046875\n",
      "Epoch 4, Batch 2365, Loss: 532.127197265625\n",
      "Epoch 4, Batch 2366, Loss: 525.8402099609375\n",
      "Epoch 4, Batch 2367, Loss: 510.83013916015625\n",
      "Epoch 4, Batch 2368, Loss: 527.880126953125\n",
      "Epoch 4, Batch 2369, Loss: 495.66375732421875\n",
      "Epoch 4, Batch 2370, Loss: 540.5389404296875\n",
      "Epoch 4, Batch 2371, Loss: 517.0277099609375\n",
      "Epoch 4, Batch 2372, Loss: 516.528076171875\n",
      "Epoch 4, Batch 2373, Loss: 532.8635864257812\n",
      "Epoch 4, Batch 2374, Loss: 539.5023193359375\n",
      "Epoch 4, Batch 2375, Loss: 521.07568359375\n",
      "Epoch 4, Batch 2376, Loss: 553.6586303710938\n",
      "Epoch 4, Batch 2377, Loss: 501.59906005859375\n",
      "Epoch 4, Batch 2378, Loss: 525.265625\n",
      "Epoch 4, Batch 2379, Loss: 525.8134765625\n",
      "Epoch 4, Batch 2380, Loss: 487.9306640625\n",
      "Epoch 4, Batch 2381, Loss: 523.2498779296875\n",
      "Epoch 4, Batch 2382, Loss: 524.439453125\n",
      "Epoch 4, Batch 2383, Loss: 531.9420166015625\n",
      "Epoch 4, Batch 2384, Loss: 518.9329223632812\n",
      "Epoch 4, Batch 2385, Loss: 527.4400634765625\n",
      "Epoch 4, Batch 2386, Loss: 502.302978515625\n",
      "Epoch 4, Batch 2387, Loss: 508.7703552246094\n",
      "Epoch 4, Batch 2388, Loss: 498.1723327636719\n",
      "Epoch 4, Batch 2389, Loss: 499.2711181640625\n",
      "Epoch 4, Batch 2390, Loss: 534.0074462890625\n",
      "Epoch 4, Batch 2391, Loss: 518.326904296875\n",
      "Epoch 4, Batch 2392, Loss: 504.1253662109375\n",
      "Epoch 4, Batch 2393, Loss: 496.3968200683594\n",
      "Epoch 4, Batch 2394, Loss: 523.4859619140625\n",
      "Epoch 4, Batch 2395, Loss: 511.300048828125\n",
      "Epoch 4, Batch 2396, Loss: 514.8446044921875\n",
      "Epoch 4, Batch 2397, Loss: 540.2545166015625\n",
      "Epoch 4, Batch 2398, Loss: 484.96368408203125\n",
      "Epoch 4, Batch 2399, Loss: 498.7688903808594\n",
      "Epoch 4, Batch 2400, Loss: 510.0747985839844\n",
      "Epoch 4, Batch 2401, Loss: 543.2421875\n",
      "Epoch 4, Batch 2402, Loss: 525.4982299804688\n",
      "Epoch 4, Batch 2403, Loss: 510.59912109375\n",
      "Epoch 4, Batch 2404, Loss: 503.7122802734375\n",
      "Epoch 4, Batch 2405, Loss: 493.830078125\n",
      "Epoch 4, Batch 2406, Loss: 526.819091796875\n",
      "Epoch 4, Batch 2407, Loss: 520.3134155273438\n",
      "Epoch 4, Batch 2408, Loss: 514.55224609375\n",
      "Epoch 4, Batch 2409, Loss: 493.1604919433594\n",
      "Epoch 4, Batch 2410, Loss: 497.69427490234375\n",
      "Epoch 4, Batch 2411, Loss: 519.0821533203125\n",
      "Epoch 4, Batch 2412, Loss: 521.0496826171875\n",
      "Epoch 4, Batch 2413, Loss: 534.451904296875\n",
      "Epoch 4, Batch 2414, Loss: 523.7760009765625\n",
      "Epoch 4, Batch 2415, Loss: 503.89794921875\n",
      "Epoch 4, Batch 2416, Loss: 489.21649169921875\n",
      "Epoch 4, Batch 2417, Loss: 518.0787963867188\n",
      "Epoch 4, Batch 2418, Loss: 509.2237243652344\n",
      "Epoch 4, Batch 2419, Loss: 514.154541015625\n",
      "Epoch 4, Batch 2420, Loss: 531.3988037109375\n",
      "Epoch 4, Batch 2421, Loss: 503.87469482421875\n",
      "Epoch 4, Batch 2422, Loss: 516.4554443359375\n",
      "Epoch 4, Batch 2423, Loss: 507.533203125\n",
      "Epoch 4, Batch 2424, Loss: 513.4739990234375\n",
      "Epoch 4, Batch 2425, Loss: 514.3350830078125\n",
      "Epoch 4, Batch 2426, Loss: 496.0931091308594\n",
      "Epoch 4, Batch 2427, Loss: 497.06243896484375\n",
      "Epoch 4, Batch 2428, Loss: 508.5079040527344\n",
      "Epoch 4, Batch 2429, Loss: 526.9942016601562\n",
      "Epoch 4, Batch 2430, Loss: 513.0952758789062\n",
      "Epoch 4, Batch 2431, Loss: 508.37982177734375\n",
      "Epoch 4, Batch 2432, Loss: 525.5928955078125\n",
      "Epoch 4, Batch 2433, Loss: 521.8517456054688\n",
      "Epoch 4, Batch 2434, Loss: 524.814453125\n",
      "Epoch 4, Batch 2435, Loss: 498.0160827636719\n",
      "Epoch 4, Batch 2436, Loss: 505.3780517578125\n",
      "Epoch 4, Batch 2437, Loss: 520.840576171875\n",
      "Epoch 4, Batch 2438, Loss: 515.4368286132812\n",
      "Epoch 4, Batch 2439, Loss: 501.52532958984375\n",
      "Epoch 4, Batch 2440, Loss: 519.3438720703125\n",
      "Epoch 4, Batch 2441, Loss: 504.2566833496094\n",
      "Epoch 4, Batch 2442, Loss: 523.9366455078125\n",
      "Epoch 4, Batch 2443, Loss: 545.2318115234375\n",
      "Epoch 4, Batch 2444, Loss: 506.03607177734375\n",
      "Epoch 4, Batch 2445, Loss: 517.1009521484375\n",
      "Epoch 4, Batch 2446, Loss: 497.5267028808594\n",
      "Epoch 4, Batch 2447, Loss: 522.7057495117188\n",
      "Epoch 4, Batch 2448, Loss: 511.3671569824219\n",
      "Epoch 4, Batch 2449, Loss: 529.1201171875\n",
      "Epoch 4, Batch 2450, Loss: 495.7848815917969\n",
      "Epoch 4, Batch 2451, Loss: 525.5995483398438\n",
      "Epoch 4, Batch 2452, Loss: 501.648681640625\n",
      "Epoch 4, Batch 2453, Loss: 511.5882263183594\n",
      "Epoch 4, Batch 2454, Loss: 520.135009765625\n",
      "Epoch 4, Batch 2455, Loss: 526.573974609375\n",
      "Epoch 4, Batch 2456, Loss: 522.5983276367188\n",
      "Epoch 4, Batch 2457, Loss: 523.0255737304688\n",
      "Epoch 4, Batch 2458, Loss: 533.7847290039062\n",
      "Epoch 4, Batch 2459, Loss: 495.5501403808594\n",
      "Epoch 4, Batch 2460, Loss: 496.8074645996094\n",
      "Epoch 4, Batch 2461, Loss: 511.3654479980469\n",
      "Epoch 4, Batch 2462, Loss: 516.8121948242188\n",
      "Epoch 4, Batch 2463, Loss: 507.2413024902344\n",
      "Epoch 4, Batch 2464, Loss: 504.56787109375\n",
      "Epoch 4, Batch 2465, Loss: 522.7455444335938\n",
      "Epoch 4, Batch 2466, Loss: 513.4591674804688\n",
      "Epoch 4, Batch 2467, Loss: 501.228515625\n",
      "Epoch 4, Batch 2468, Loss: 532.9047241210938\n",
      "Epoch 4, Batch 2469, Loss: 509.5048828125\n",
      "Epoch 4, Batch 2470, Loss: 501.9091796875\n",
      "Epoch 4, Batch 2471, Loss: 500.8095397949219\n",
      "Epoch 4, Batch 2472, Loss: 518.3304443359375\n",
      "Epoch 4, Batch 2473, Loss: 508.16400146484375\n",
      "Epoch 4, Batch 2474, Loss: 517.0225830078125\n",
      "Epoch 4, Batch 2475, Loss: 505.24853515625\n",
      "Epoch 4, Batch 2476, Loss: 511.9739685058594\n",
      "Epoch 4, Batch 2477, Loss: 539.62060546875\n",
      "Epoch 4, Batch 2478, Loss: 487.14801025390625\n",
      "Epoch 4, Batch 2479, Loss: 520.6339111328125\n",
      "Epoch 4, Batch 2480, Loss: 514.9962768554688\n",
      "Epoch 4, Batch 2481, Loss: 498.2491455078125\n",
      "Epoch 4, Batch 2482, Loss: 532.8617553710938\n",
      "Epoch 4, Batch 2483, Loss: 519.6589965820312\n",
      "Epoch 4, Batch 2484, Loss: 530.5164794921875\n",
      "Epoch 4, Batch 2485, Loss: 498.52313232421875\n",
      "Epoch 4, Batch 2486, Loss: 490.3506774902344\n",
      "Epoch 4, Batch 2487, Loss: 535.113525390625\n",
      "Epoch 4, Batch 2488, Loss: 507.30389404296875\n",
      "Epoch 4, Batch 2489, Loss: 495.416015625\n",
      "Epoch 4, Batch 2490, Loss: 528.7266235351562\n",
      "Epoch 4, Batch 2491, Loss: 538.1146850585938\n",
      "Epoch 4, Batch 2492, Loss: 521.7291870117188\n",
      "Epoch 4, Batch 2493, Loss: 523.3139038085938\n",
      "Epoch 4, Batch 2494, Loss: 506.8059997558594\n",
      "Epoch 4, Batch 2495, Loss: 506.4759521484375\n",
      "Epoch 4, Batch 2496, Loss: 519.4854736328125\n",
      "Epoch 4, Batch 2497, Loss: 511.23358154296875\n",
      "Epoch 4, Batch 2498, Loss: 498.42919921875\n",
      "Epoch 4, Batch 2499, Loss: 524.9068603515625\n",
      "Epoch 4, Batch 2500, Loss: 502.0486755371094\n",
      "Epoch 4, Batch 2501, Loss: 525.2770385742188\n",
      "Epoch 4, Batch 2502, Loss: 532.6460571289062\n",
      "Epoch 4, Batch 2503, Loss: 503.6574401855469\n",
      "Epoch 4, Batch 2504, Loss: 536.025146484375\n",
      "Epoch 4, Batch 2505, Loss: 509.977783203125\n",
      "Epoch 4, Batch 2506, Loss: 516.4085693359375\n",
      "Epoch 4, Batch 2507, Loss: 538.3106689453125\n",
      "Epoch 4, Batch 2508, Loss: 513.9722290039062\n",
      "Epoch 4, Batch 2509, Loss: 529.041748046875\n",
      "Epoch 4, Batch 2510, Loss: 510.9971618652344\n",
      "Epoch 4, Batch 2511, Loss: 496.3912048339844\n",
      "Epoch 4, Batch 2512, Loss: 513.025634765625\n",
      "Epoch 4, Batch 2513, Loss: 527.4660034179688\n",
      "Epoch 4, Batch 2514, Loss: 506.75146484375\n",
      "Epoch 4, Batch 2515, Loss: 512.0480346679688\n",
      "Epoch 4, Batch 2516, Loss: 525.8487548828125\n",
      "Epoch 4, Batch 2517, Loss: 514.9081420898438\n",
      "Epoch 4, Batch 2518, Loss: 524.2720947265625\n",
      "Epoch 4, Batch 2519, Loss: 504.81317138671875\n",
      "Epoch 4, Batch 2520, Loss: 485.7919921875\n",
      "Epoch 4, Batch 2521, Loss: 503.6670837402344\n",
      "Epoch 4, Batch 2522, Loss: 535.4569091796875\n",
      "Epoch 4, Batch 2523, Loss: 516.9654541015625\n",
      "Epoch 4, Batch 2524, Loss: 507.621337890625\n",
      "Epoch 4, Batch 2525, Loss: 511.08001708984375\n",
      "Epoch 4, Batch 2526, Loss: 503.0489196777344\n",
      "Epoch 4, Batch 2527, Loss: 502.9633483886719\n",
      "Epoch 4, Batch 2528, Loss: 539.44482421875\n",
      "Epoch 4, Batch 2529, Loss: 505.4642333984375\n",
      "Epoch 4, Batch 2530, Loss: 526.14501953125\n",
      "Epoch 4, Batch 2531, Loss: 536.1494140625\n",
      "Epoch 4, Batch 2532, Loss: 547.9337158203125\n",
      "Epoch 4, Batch 2533, Loss: 519.218505859375\n",
      "Epoch 4, Batch 2534, Loss: 527.8404541015625\n",
      "Epoch 4, Batch 2535, Loss: 506.62158203125\n",
      "Epoch 4, Batch 2536, Loss: 504.146728515625\n",
      "Epoch 4, Batch 2537, Loss: 544.0220947265625\n",
      "Epoch 4, Batch 2538, Loss: 498.27801513671875\n",
      "Epoch 4, Batch 2539, Loss: 511.5985107421875\n",
      "Epoch 4, Batch 2540, Loss: 516.0205078125\n",
      "Epoch 4, Batch 2541, Loss: 524.49658203125\n",
      "Epoch 4, Batch 2542, Loss: 534.6618041992188\n",
      "Epoch 4, Batch 2543, Loss: 526.5375366210938\n",
      "Epoch 4, Batch 2544, Loss: 532.2607421875\n",
      "Epoch 4, Batch 2545, Loss: 528.7367553710938\n",
      "Epoch 4, Batch 2546, Loss: 510.5896911621094\n",
      "Epoch 4, Batch 2547, Loss: 502.04608154296875\n",
      "Epoch 4, Batch 2548, Loss: 495.070068359375\n",
      "Epoch 4, Batch 2549, Loss: 536.1585083007812\n",
      "Epoch 4, Batch 2550, Loss: 497.3858642578125\n",
      "Epoch 4, Batch 2551, Loss: 535.2302856445312\n",
      "Epoch 4, Batch 2552, Loss: 496.95745849609375\n",
      "Epoch 4, Batch 2553, Loss: 535.439697265625\n",
      "Epoch 4, Batch 2554, Loss: 483.92254638671875\n",
      "Epoch 4, Batch 2555, Loss: 524.4462280273438\n",
      "Epoch 4, Batch 2556, Loss: 542.952392578125\n",
      "Epoch 4, Batch 2557, Loss: 514.4547729492188\n",
      "Epoch 4, Batch 2558, Loss: 514.4092407226562\n",
      "Epoch 4, Batch 2559, Loss: 541.8466796875\n",
      "Epoch 4, Batch 2560, Loss: 515.1054077148438\n",
      "Epoch 4, Batch 2561, Loss: 504.3416748046875\n",
      "Epoch 4, Batch 2562, Loss: 500.6691589355469\n",
      "Epoch 4, Batch 2563, Loss: 518.9925537109375\n",
      "Epoch 4, Batch 2564, Loss: 520.2275390625\n",
      "Epoch 4, Batch 2565, Loss: 520.4735107421875\n",
      "Epoch 4, Batch 2566, Loss: 504.97625732421875\n",
      "Epoch 4, Batch 2567, Loss: 515.6673583984375\n",
      "Epoch 4, Batch 2568, Loss: 531.9959716796875\n",
      "Epoch 4, Batch 2569, Loss: 545.4024658203125\n",
      "Epoch 4, Batch 2570, Loss: 472.47698974609375\n",
      "Epoch 4, Batch 2571, Loss: 522.686279296875\n",
      "Epoch 4, Batch 2572, Loss: 533.069580078125\n",
      "Epoch 4, Batch 2573, Loss: 490.40753173828125\n",
      "Epoch 4, Batch 2574, Loss: 546.19287109375\n",
      "Epoch 4, Batch 2575, Loss: 504.0079040527344\n",
      "Epoch 4, Batch 2576, Loss: 530.970947265625\n",
      "Epoch 4, Batch 2577, Loss: 520.8639526367188\n",
      "Epoch 4, Batch 2578, Loss: 536.6585083007812\n",
      "Epoch 4, Batch 2579, Loss: 511.83514404296875\n",
      "Epoch 4, Batch 2580, Loss: 501.6707458496094\n",
      "Epoch 4, Batch 2581, Loss: 526.5376586914062\n",
      "Epoch 4, Batch 2582, Loss: 521.0910034179688\n",
      "Epoch 4, Batch 2583, Loss: 515.2238159179688\n",
      "Epoch 4, Batch 2584, Loss: 523.8990478515625\n",
      "Epoch 4, Batch 2585, Loss: 523.5136108398438\n",
      "Epoch 4, Batch 2586, Loss: 521.9671630859375\n",
      "Epoch 4, Batch 2587, Loss: 496.7783203125\n",
      "Epoch 4, Batch 2588, Loss: 493.4622497558594\n",
      "Epoch 4, Batch 2589, Loss: 519.3856811523438\n",
      "Epoch 4, Batch 2590, Loss: 527.169189453125\n",
      "Epoch 4, Batch 2591, Loss: 508.5005798339844\n",
      "Epoch 4, Batch 2592, Loss: 483.5378112792969\n",
      "Epoch 4, Batch 2593, Loss: 515.451416015625\n",
      "Epoch 4, Batch 2594, Loss: 521.538818359375\n",
      "Epoch 4, Batch 2595, Loss: 527.02978515625\n",
      "Epoch 4, Batch 2596, Loss: 550.5966796875\n",
      "Epoch 4, Batch 2597, Loss: 536.0093994140625\n",
      "Epoch 4, Batch 2598, Loss: 544.8283081054688\n",
      "Epoch 4, Batch 2599, Loss: 507.5491638183594\n",
      "Epoch 4, Batch 2600, Loss: 502.029052734375\n",
      "Epoch 4, Batch 2601, Loss: 534.4686889648438\n",
      "Epoch 4, Batch 2602, Loss: 489.96331787109375\n",
      "Epoch 4, Batch 2603, Loss: 562.7490234375\n",
      "Epoch 4, Batch 2604, Loss: 521.4684448242188\n",
      "Epoch 4, Batch 2605, Loss: 518.993896484375\n",
      "Epoch 4, Batch 2606, Loss: 517.3349609375\n",
      "Epoch 4, Batch 2607, Loss: 539.48583984375\n",
      "Epoch 4, Batch 2608, Loss: 487.03875732421875\n",
      "Epoch 4, Batch 2609, Loss: 505.66156005859375\n",
      "Epoch 4, Batch 2610, Loss: 498.80853271484375\n",
      "Epoch 4, Batch 2611, Loss: 526.711181640625\n",
      "Epoch 4, Batch 2612, Loss: 505.30828857421875\n",
      "Epoch 4, Batch 2613, Loss: 505.7135925292969\n",
      "Epoch 4, Batch 2614, Loss: 525.9663696289062\n",
      "Epoch 4, Batch 2615, Loss: 531.66650390625\n",
      "Epoch 4, Batch 2616, Loss: 514.1793823242188\n",
      "Epoch 4, Batch 2617, Loss: 496.857666015625\n",
      "Epoch 4, Batch 2618, Loss: 507.4576110839844\n",
      "Epoch 4, Batch 2619, Loss: 497.79412841796875\n",
      "Epoch 4, Batch 2620, Loss: 512.392822265625\n",
      "Epoch 4, Batch 2621, Loss: 495.5555114746094\n",
      "Epoch 4, Batch 2622, Loss: 522.643310546875\n",
      "Epoch 4, Batch 2623, Loss: 531.625244140625\n",
      "Epoch 4, Batch 2624, Loss: 540.5586547851562\n",
      "Epoch 4, Batch 2625, Loss: 518.9686279296875\n",
      "Epoch 4, Batch 2626, Loss: 493.80120849609375\n",
      "Epoch 4, Batch 2627, Loss: 526.3621826171875\n",
      "Epoch 4, Batch 2628, Loss: 512.085693359375\n",
      "Epoch 4, Batch 2629, Loss: 529.6192016601562\n",
      "Epoch 4, Batch 2630, Loss: 493.3648376464844\n",
      "Epoch 4, Batch 2631, Loss: 547.8170776367188\n",
      "Epoch 4, Batch 2632, Loss: 481.8897399902344\n",
      "Epoch 4, Batch 2633, Loss: 548.5265502929688\n",
      "Epoch 4, Batch 2634, Loss: 497.1846618652344\n",
      "Epoch 4, Batch 2635, Loss: 515.0557250976562\n",
      "Epoch 4, Batch 2636, Loss: 531.0914306640625\n",
      "Epoch 4, Batch 2637, Loss: 499.1914978027344\n",
      "Epoch 4, Batch 2638, Loss: 504.5872802734375\n",
      "Epoch 4, Batch 2639, Loss: 518.042724609375\n",
      "Epoch 4, Batch 2640, Loss: 534.8837280273438\n",
      "Epoch 4, Batch 2641, Loss: 526.9708251953125\n",
      "Epoch 4, Batch 2642, Loss: 515.1915893554688\n",
      "Epoch 4, Batch 2643, Loss: 513.0507202148438\n",
      "Epoch 4, Batch 2644, Loss: 496.8672790527344\n",
      "Epoch 4, Batch 2645, Loss: 510.4962158203125\n",
      "Epoch 4, Batch 2646, Loss: 488.127197265625\n",
      "Epoch 4, Batch 2647, Loss: 490.5877685546875\n",
      "Epoch 4, Batch 2648, Loss: 528.043701171875\n",
      "Epoch 4, Batch 2649, Loss: 485.9576110839844\n",
      "Epoch 4, Batch 2650, Loss: 543.396484375\n",
      "Epoch 4, Batch 2651, Loss: 506.13824462890625\n",
      "Epoch 4, Batch 2652, Loss: 502.4998474121094\n",
      "Epoch 4, Batch 2653, Loss: 530.6883544921875\n",
      "Epoch 4, Batch 2654, Loss: 520.3444213867188\n",
      "Epoch 4, Batch 2655, Loss: 507.76458740234375\n",
      "Epoch 4, Batch 2656, Loss: 508.3976745605469\n",
      "Epoch 4, Batch 2657, Loss: 527.2100830078125\n",
      "Epoch 4, Batch 2658, Loss: 535.8924560546875\n",
      "Epoch 4, Batch 2659, Loss: 498.5752868652344\n",
      "Epoch 4, Batch 2660, Loss: 557.3552856445312\n",
      "Epoch 4, Batch 2661, Loss: 490.3935241699219\n",
      "Epoch 4, Batch 2662, Loss: 519.7611694335938\n",
      "Epoch 4, Batch 2663, Loss: 505.0174560546875\n",
      "Epoch 4, Batch 2664, Loss: 540.2537841796875\n",
      "Epoch 4, Batch 2665, Loss: 514.396240234375\n",
      "Epoch 4, Batch 2666, Loss: 507.2167663574219\n",
      "Epoch 4, Batch 2667, Loss: 507.7807312011719\n",
      "Epoch 4, Batch 2668, Loss: 525.9750366210938\n",
      "Epoch 4, Batch 2669, Loss: 483.29364013671875\n",
      "Epoch 4, Batch 2670, Loss: 514.0042724609375\n",
      "Epoch 4, Batch 2671, Loss: 505.1138610839844\n",
      "Epoch 4, Batch 2672, Loss: 521.4205322265625\n",
      "Epoch 4, Batch 2673, Loss: 542.2698974609375\n",
      "Epoch 4, Batch 2674, Loss: 530.224609375\n",
      "Epoch 4, Batch 2675, Loss: 532.8077392578125\n",
      "Epoch 4, Batch 2676, Loss: 529.8491821289062\n",
      "Epoch 4, Batch 2677, Loss: 537.856201171875\n",
      "Epoch 4, Batch 2678, Loss: 503.9888610839844\n",
      "Epoch 4, Batch 2679, Loss: 503.18267822265625\n",
      "Epoch 4, Batch 2680, Loss: 528.493896484375\n",
      "Epoch 4, Batch 2681, Loss: 521.2247314453125\n",
      "Epoch 4, Batch 2682, Loss: 502.6700134277344\n",
      "Epoch 4, Batch 2683, Loss: 503.4234313964844\n",
      "Epoch 4, Batch 2684, Loss: 534.6981201171875\n",
      "Epoch 4, Batch 2685, Loss: 519.4322509765625\n",
      "Epoch 4, Batch 2686, Loss: 507.0794372558594\n",
      "Epoch 4, Batch 2687, Loss: 495.3330078125\n",
      "Epoch 4, Batch 2688, Loss: 500.47003173828125\n",
      "Epoch 4, Batch 2689, Loss: 511.7730407714844\n",
      "Epoch 4, Batch 2690, Loss: 497.1162109375\n",
      "Epoch 4, Batch 2691, Loss: 513.8426513671875\n",
      "Epoch 4, Batch 2692, Loss: 522.7130126953125\n",
      "Epoch 4, Batch 2693, Loss: 528.9916381835938\n",
      "Epoch 4, Batch 2694, Loss: 496.00872802734375\n",
      "Epoch 4, Batch 2695, Loss: 537.1406860351562\n",
      "Epoch 4, Batch 2696, Loss: 518.2950439453125\n",
      "Epoch 4, Batch 2697, Loss: 518.0014038085938\n",
      "Epoch 4, Batch 2698, Loss: 530.47509765625\n",
      "Epoch 4, Batch 2699, Loss: 524.2933959960938\n",
      "Epoch 4, Batch 2700, Loss: 513.733154296875\n",
      "Epoch 4, Batch 2701, Loss: 504.72967529296875\n",
      "Epoch 4, Batch 2702, Loss: 527.9535522460938\n",
      "Epoch 4, Batch 2703, Loss: 512.0001831054688\n",
      "Epoch 4, Batch 2704, Loss: 515.8851318359375\n",
      "Epoch 4, Batch 2705, Loss: 502.9241638183594\n",
      "Epoch 4, Batch 2706, Loss: 535.3876953125\n",
      "Epoch 4, Batch 2707, Loss: 531.3610229492188\n",
      "Epoch 4, Batch 2708, Loss: 526.8187866210938\n",
      "Epoch 4, Batch 2709, Loss: 511.1482849121094\n",
      "Epoch 4, Batch 2710, Loss: 518.721923828125\n",
      "Epoch 4, Batch 2711, Loss: 506.431396484375\n",
      "Epoch 4, Batch 2712, Loss: 505.7308044433594\n",
      "Epoch 4, Batch 2713, Loss: 493.6333923339844\n",
      "Epoch 4, Batch 2714, Loss: 512.0972290039062\n",
      "Epoch 4, Batch 2715, Loss: 525.287841796875\n",
      "Epoch 4, Batch 2716, Loss: 520.8964233398438\n",
      "Epoch 4, Batch 2717, Loss: 514.7197265625\n",
      "Epoch 4, Batch 2718, Loss: 515.9714965820312\n",
      "Epoch 4, Batch 2719, Loss: 513.7595825195312\n",
      "Epoch 4, Batch 2720, Loss: 535.5941772460938\n",
      "Epoch 4, Batch 2721, Loss: 541.4374389648438\n",
      "Epoch 4, Batch 2722, Loss: 501.84002685546875\n",
      "Epoch 4, Batch 2723, Loss: 501.3052673339844\n",
      "Epoch 4, Batch 2724, Loss: 524.573486328125\n",
      "Epoch 4, Batch 2725, Loss: 493.5988464355469\n",
      "Epoch 4, Batch 2726, Loss: 524.3798828125\n",
      "Epoch 4, Batch 2727, Loss: 519.0123291015625\n",
      "Epoch 4, Batch 2728, Loss: 502.4017028808594\n",
      "Epoch 4, Batch 2729, Loss: 509.5069580078125\n",
      "Epoch 4, Batch 2730, Loss: 513.869140625\n",
      "Epoch 4, Batch 2731, Loss: 524.2680053710938\n",
      "Epoch 4, Batch 2732, Loss: 494.4687194824219\n",
      "Epoch 4, Batch 2733, Loss: 500.80535888671875\n",
      "Epoch 4, Batch 2734, Loss: 524.6416015625\n",
      "Epoch 4, Batch 2735, Loss: 498.241943359375\n",
      "Epoch 4, Batch 2736, Loss: 518.57958984375\n",
      "Epoch 4, Batch 2737, Loss: 530.546630859375\n",
      "Epoch 4, Batch 2738, Loss: 498.1484375\n",
      "Epoch 4, Batch 2739, Loss: 549.5567626953125\n",
      "Epoch 4, Batch 2740, Loss: 541.8734741210938\n",
      "Epoch 4, Batch 2741, Loss: 498.0953063964844\n",
      "Epoch 4, Batch 2742, Loss: 518.0299682617188\n",
      "Epoch 4, Batch 2743, Loss: 491.2161865234375\n",
      "Epoch 4, Batch 2744, Loss: 557.8997802734375\n",
      "Epoch 4, Batch 2745, Loss: 527.261474609375\n",
      "Epoch 4, Batch 2746, Loss: 484.41400146484375\n",
      "Epoch 4, Batch 2747, Loss: 531.9324951171875\n",
      "Epoch 4, Batch 2748, Loss: 540.4097900390625\n",
      "Epoch 4, Batch 2749, Loss: 514.875244140625\n",
      "Epoch 4, Batch 2750, Loss: 500.731689453125\n",
      "Epoch 4, Batch 2751, Loss: 512.124267578125\n",
      "Epoch 4, Batch 2752, Loss: 498.164306640625\n",
      "Epoch 4, Batch 2753, Loss: 540.702392578125\n",
      "Epoch 4, Batch 2754, Loss: 515.242919921875\n",
      "Epoch 4, Batch 2755, Loss: 512.8028564453125\n",
      "Epoch 4, Batch 2756, Loss: 523.1278076171875\n",
      "Epoch 4, Batch 2757, Loss: 506.2072448730469\n",
      "Epoch 4, Batch 2758, Loss: 526.5169067382812\n",
      "Epoch 4, Batch 2759, Loss: 508.2840576171875\n",
      "Epoch 4, Batch 2760, Loss: 499.440673828125\n",
      "Epoch 4, Batch 2761, Loss: 513.4679565429688\n",
      "Epoch 4, Batch 2762, Loss: 540.7415771484375\n",
      "Epoch 4, Batch 2763, Loss: 516.345458984375\n",
      "Epoch 4, Batch 2764, Loss: 507.07537841796875\n",
      "Epoch 4, Batch 2765, Loss: 527.09716796875\n",
      "Epoch 4, Batch 2766, Loss: 513.2616577148438\n",
      "Epoch 4, Batch 2767, Loss: 495.189697265625\n",
      "Epoch 4, Batch 2768, Loss: 533.2374877929688\n",
      "Epoch 4, Batch 2769, Loss: 499.34326171875\n",
      "Epoch 4, Batch 2770, Loss: 507.7649841308594\n",
      "Epoch 4, Batch 2771, Loss: 539.516845703125\n",
      "Epoch 4, Batch 2772, Loss: 506.923583984375\n",
      "Epoch 4, Batch 2773, Loss: 526.4669189453125\n",
      "Epoch 4, Batch 2774, Loss: 531.7041015625\n",
      "Epoch 4, Batch 2775, Loss: 526.1358642578125\n",
      "Epoch 4, Batch 2776, Loss: 513.138671875\n",
      "Epoch 4, Batch 2777, Loss: 519.6095581054688\n",
      "Epoch 4, Batch 2778, Loss: 542.5028076171875\n",
      "Epoch 4, Batch 2779, Loss: 524.2291870117188\n",
      "Epoch 4, Batch 2780, Loss: 538.220947265625\n",
      "Epoch 4, Batch 2781, Loss: 526.2586669921875\n",
      "Epoch 4, Batch 2782, Loss: 528.8446044921875\n",
      "Epoch 4, Batch 2783, Loss: 529.1964111328125\n",
      "Epoch 4, Batch 2784, Loss: 505.43310546875\n",
      "Epoch 4, Batch 2785, Loss: 530.6957397460938\n",
      "Epoch 4, Batch 2786, Loss: 509.9163513183594\n",
      "Epoch 4, Batch 2787, Loss: 509.11248779296875\n",
      "Epoch 4, Batch 2788, Loss: 514.306640625\n",
      "Epoch 4, Batch 2789, Loss: 508.7359313964844\n",
      "Epoch 4, Batch 2790, Loss: 505.35150146484375\n",
      "Epoch 4, Batch 2791, Loss: 533.046875\n",
      "Epoch 4, Batch 2792, Loss: 507.08782958984375\n",
      "Epoch 4, Batch 2793, Loss: 522.3089599609375\n",
      "Epoch 4, Batch 2794, Loss: 485.7868347167969\n",
      "Epoch 4, Batch 2795, Loss: 501.73553466796875\n",
      "Epoch 4, Batch 2796, Loss: 512.3226318359375\n",
      "Epoch 4, Batch 2797, Loss: 532.4017333984375\n",
      "Epoch 4, Batch 2798, Loss: 506.87451171875\n",
      "Epoch 4, Batch 2799, Loss: 510.6925964355469\n",
      "Epoch 4, Batch 2800, Loss: 529.0838623046875\n",
      "Epoch 4, Batch 2801, Loss: 512.8985595703125\n",
      "Epoch 4, Batch 2802, Loss: 501.87908935546875\n",
      "Epoch 4, Batch 2803, Loss: 527.04541015625\n",
      "Epoch 4, Batch 2804, Loss: 498.4967956542969\n",
      "Epoch 4, Batch 2805, Loss: 519.040771484375\n",
      "Epoch 4, Batch 2806, Loss: 511.29052734375\n",
      "Epoch 4, Batch 2807, Loss: 498.0762634277344\n",
      "Epoch 4, Batch 2808, Loss: 515.0910034179688\n",
      "Epoch 4, Batch 2809, Loss: 516.1878662109375\n",
      "Epoch 4, Batch 2810, Loss: 515.0372314453125\n",
      "Epoch 4, Batch 2811, Loss: 552.2472534179688\n",
      "Epoch 4, Batch 2812, Loss: 543.3743286132812\n",
      "Epoch 4, Batch 2813, Loss: 528.2462158203125\n",
      "Epoch 4, Batch 2814, Loss: 509.15838623046875\n",
      "Epoch 4, Batch 2815, Loss: 517.84228515625\n",
      "Epoch 4, Batch 2816, Loss: 537.134033203125\n",
      "Epoch 4, Batch 2817, Loss: 508.46826171875\n",
      "Epoch 4, Batch 2818, Loss: 496.9794921875\n",
      "Epoch 4, Batch 2819, Loss: 508.01776123046875\n",
      "Epoch 4, Batch 2820, Loss: 533.6343383789062\n",
      "Epoch 4, Batch 2821, Loss: 526.920654296875\n",
      "Epoch 4, Batch 2822, Loss: 517.8734741210938\n",
      "Epoch 4, Batch 2823, Loss: 502.480712890625\n",
      "Epoch 4, Batch 2824, Loss: 505.5748291015625\n",
      "Epoch 4, Batch 2825, Loss: 514.4680786132812\n",
      "Epoch 4, Batch 2826, Loss: 516.018310546875\n",
      "Epoch 4, Batch 2827, Loss: 550.641357421875\n",
      "Epoch 4, Batch 2828, Loss: 537.8850708007812\n",
      "Epoch 4, Batch 2829, Loss: 528.80908203125\n",
      "Epoch 4, Batch 2830, Loss: 519.514892578125\n",
      "Epoch 4, Batch 2831, Loss: 512.4730834960938\n",
      "Epoch 4, Batch 2832, Loss: 509.07940673828125\n",
      "Epoch 4, Batch 2833, Loss: 543.5045776367188\n",
      "Epoch 4, Batch 2834, Loss: 542.0729370117188\n",
      "Epoch 4, Batch 2835, Loss: 506.96075439453125\n",
      "Epoch 4, Batch 2836, Loss: 532.941650390625\n",
      "Epoch 4, Batch 2837, Loss: 522.94189453125\n",
      "Epoch 4, Batch 2838, Loss: 492.840576171875\n",
      "Epoch 4, Batch 2839, Loss: 505.7668151855469\n",
      "Epoch 4, Batch 2840, Loss: 539.1610107421875\n",
      "Epoch 4, Batch 2841, Loss: 524.845458984375\n",
      "Epoch 4, Batch 2842, Loss: 511.69146728515625\n",
      "Epoch 4, Batch 2843, Loss: 539.26171875\n",
      "Epoch 4, Batch 2844, Loss: 500.7936706542969\n",
      "Epoch 4, Batch 2845, Loss: 500.85150146484375\n",
      "Epoch 4, Batch 2846, Loss: 492.37762451171875\n",
      "Epoch 4, Batch 2847, Loss: 514.6319580078125\n",
      "Epoch 4, Batch 2848, Loss: 534.49951171875\n",
      "Epoch 4, Batch 2849, Loss: 517.7604370117188\n",
      "Epoch 4, Batch 2850, Loss: 530.969970703125\n",
      "Epoch 4, Batch 2851, Loss: 527.200439453125\n",
      "Epoch 4, Batch 2852, Loss: 520.5369873046875\n",
      "Epoch 4, Batch 2853, Loss: 511.9942626953125\n",
      "Epoch 4, Batch 2854, Loss: 539.1690063476562\n",
      "Epoch 4, Batch 2855, Loss: 516.382080078125\n",
      "Epoch 4, Batch 2856, Loss: 525.3753662109375\n",
      "Epoch 4, Batch 2857, Loss: 520.41357421875\n",
      "Epoch 4, Batch 2858, Loss: 525.0563354492188\n",
      "Epoch 4, Batch 2859, Loss: 518.7296752929688\n",
      "Epoch 4, Batch 2860, Loss: 489.61248779296875\n",
      "Epoch 4, Batch 2861, Loss: 517.9508056640625\n",
      "Epoch 4, Batch 2862, Loss: 512.5383911132812\n",
      "Epoch 4, Batch 2863, Loss: 519.7096557617188\n",
      "Epoch 4, Batch 2864, Loss: 498.2144470214844\n",
      "Epoch 4, Batch 2865, Loss: 496.9208679199219\n",
      "Epoch 4, Batch 2866, Loss: 537.3631591796875\n",
      "Epoch 4, Batch 2867, Loss: 516.5160522460938\n",
      "Epoch 4, Batch 2868, Loss: 538.0121459960938\n",
      "Epoch 4, Batch 2869, Loss: 520.5679931640625\n",
      "Epoch 4, Batch 2870, Loss: 520.124267578125\n",
      "Epoch 4, Batch 2871, Loss: 489.1065368652344\n",
      "Epoch 4, Batch 2872, Loss: 533.2828979492188\n",
      "Epoch 4, Batch 2873, Loss: 510.99371337890625\n",
      "Epoch 4, Batch 2874, Loss: 541.4205322265625\n",
      "Epoch 4, Batch 2875, Loss: 524.4795532226562\n",
      "Epoch 4, Batch 2876, Loss: 506.5379638671875\n",
      "Epoch 4, Batch 2877, Loss: 544.271484375\n",
      "Epoch 4, Batch 2878, Loss: 520.1605224609375\n",
      "Epoch 4, Batch 2879, Loss: 518.316650390625\n",
      "Epoch 4, Batch 2880, Loss: 506.85919189453125\n",
      "Epoch 4, Batch 2881, Loss: 513.5011596679688\n",
      "Epoch 4, Batch 2882, Loss: 523.7307739257812\n",
      "Epoch 4, Batch 2883, Loss: 503.898681640625\n",
      "Epoch 4, Batch 2884, Loss: 498.75714111328125\n",
      "Epoch 4, Batch 2885, Loss: 531.2423706054688\n",
      "Epoch 4, Batch 2886, Loss: 495.00054931640625\n",
      "Epoch 4, Batch 2887, Loss: 509.81390380859375\n",
      "Epoch 4, Batch 2888, Loss: 533.9069213867188\n",
      "Epoch 4, Batch 2889, Loss: 482.9693298339844\n",
      "Epoch 4, Batch 2890, Loss: 524.6585693359375\n",
      "Epoch 4, Batch 2891, Loss: 518.8228149414062\n",
      "Epoch 4, Batch 2892, Loss: 524.3441162109375\n",
      "Epoch 4, Batch 2893, Loss: 532.7425537109375\n",
      "Epoch 4, Batch 2894, Loss: 531.922607421875\n",
      "Epoch 4, Batch 2895, Loss: 506.9297790527344\n",
      "Epoch 4, Batch 2896, Loss: 480.5806884765625\n",
      "Epoch 4, Batch 2897, Loss: 502.50634765625\n",
      "Epoch 4, Batch 2898, Loss: 522.0160522460938\n",
      "Epoch 4, Batch 2899, Loss: 501.3870544433594\n",
      "Epoch 4, Batch 2900, Loss: 532.0687866210938\n",
      "Epoch 4, Batch 2901, Loss: 514.098876953125\n",
      "Epoch 4, Batch 2902, Loss: 528.4645385742188\n",
      "Epoch 4, Batch 2903, Loss: 501.24310302734375\n",
      "Epoch 4, Batch 2904, Loss: 509.8781433105469\n",
      "Epoch 4, Batch 2905, Loss: 486.1727294921875\n",
      "Epoch 4, Batch 2906, Loss: 531.978271484375\n",
      "Epoch 4, Batch 2907, Loss: 536.0902099609375\n",
      "Epoch 4, Batch 2908, Loss: 502.2173767089844\n",
      "Epoch 4, Batch 2909, Loss: 523.9343872070312\n",
      "Epoch 4, Batch 2910, Loss: 533.6329956054688\n",
      "Epoch 4, Batch 2911, Loss: 501.926025390625\n",
      "Epoch 4, Batch 2912, Loss: 528.6627197265625\n",
      "Epoch 4, Batch 2913, Loss: 531.988525390625\n",
      "Epoch 4, Batch 2914, Loss: 519.8450927734375\n",
      "Epoch 4, Batch 2915, Loss: 536.1764526367188\n",
      "Epoch 4, Batch 2916, Loss: 531.1483764648438\n",
      "Epoch 4, Batch 2917, Loss: 503.01397705078125\n",
      "Epoch 4, Batch 2918, Loss: 540.3809814453125\n",
      "Epoch 4, Batch 2919, Loss: 525.2060546875\n",
      "Epoch 4, Batch 2920, Loss: 516.9170532226562\n",
      "Epoch 4, Batch 2921, Loss: 544.5713500976562\n",
      "Epoch 4, Batch 2922, Loss: 527.293701171875\n",
      "Epoch 4, Batch 2923, Loss: 507.51953125\n",
      "Epoch 4, Batch 2924, Loss: 525.6474609375\n",
      "Epoch 4, Batch 2925, Loss: 491.63800048828125\n",
      "Epoch 4, Batch 2926, Loss: 519.28076171875\n",
      "Epoch 4, Batch 2927, Loss: 508.829833984375\n",
      "Epoch 4, Batch 2928, Loss: 520.7405395507812\n",
      "Epoch 4, Batch 2929, Loss: 550.7039184570312\n",
      "Epoch 4, Batch 2930, Loss: 524.5488891601562\n",
      "Epoch 4, Batch 2931, Loss: 508.559814453125\n",
      "Epoch 4, Batch 2932, Loss: 520.1610107421875\n",
      "Epoch 4, Batch 2933, Loss: 517.102783203125\n",
      "Epoch 4, Batch 2934, Loss: 510.21539306640625\n",
      "Epoch 4, Batch 2935, Loss: 514.7763671875\n",
      "Epoch 4, Batch 2936, Loss: 541.9910888671875\n",
      "Epoch 4, Batch 2937, Loss: 498.6480712890625\n",
      "Epoch 4, Batch 2938, Loss: 498.3411560058594\n",
      "Epoch 4, Batch 2939, Loss: 514.2860107421875\n",
      "Epoch 4, Batch 2940, Loss: 537.95263671875\n",
      "Epoch 4, Batch 2941, Loss: 519.3390502929688\n",
      "Epoch 4, Batch 2942, Loss: 529.2042236328125\n",
      "Epoch 4, Batch 2943, Loss: 526.263427734375\n",
      "Epoch 4, Batch 2944, Loss: 539.2828369140625\n",
      "Epoch 4, Batch 2945, Loss: 510.4226989746094\n",
      "Epoch 4, Batch 2946, Loss: 505.42352294921875\n",
      "Epoch 4, Batch 2947, Loss: 485.1963806152344\n",
      "Epoch 4, Batch 2948, Loss: 510.8114929199219\n",
      "Epoch 4, Batch 2949, Loss: 520.15625\n",
      "Epoch 4, Batch 2950, Loss: 512.6549682617188\n",
      "Epoch 4, Batch 2951, Loss: 485.82293701171875\n",
      "Epoch 4, Batch 2952, Loss: 504.01611328125\n",
      "Epoch 4, Batch 2953, Loss: 537.227783203125\n",
      "Epoch 4, Batch 2954, Loss: 511.5988464355469\n",
      "Epoch 4, Batch 2955, Loss: 525.9161376953125\n",
      "Epoch 4, Batch 2956, Loss: 513.7020874023438\n",
      "Epoch 4, Batch 2957, Loss: 511.43878173828125\n",
      "Epoch 4, Batch 2958, Loss: 480.86614990234375\n",
      "Epoch 4, Batch 2959, Loss: 526.233642578125\n",
      "Epoch 4, Batch 2960, Loss: 528.058837890625\n",
      "Epoch 4, Batch 2961, Loss: 507.2748718261719\n",
      "Epoch 4, Batch 2962, Loss: 498.1228942871094\n",
      "Epoch 4, Batch 2963, Loss: 502.398681640625\n",
      "Epoch 4, Batch 2964, Loss: 518.6866455078125\n",
      "Epoch 4, Batch 2965, Loss: 508.82879638671875\n",
      "Epoch 4, Batch 2966, Loss: 524.3551635742188\n",
      "Epoch 4, Batch 2967, Loss: 499.6363220214844\n",
      "Epoch 4, Batch 2968, Loss: 530.8363037109375\n",
      "Epoch 4, Batch 2969, Loss: 546.800537109375\n",
      "Epoch 4, Batch 2970, Loss: 501.4500427246094\n",
      "Epoch 4, Batch 2971, Loss: 508.3819274902344\n",
      "Epoch 4, Batch 2972, Loss: 521.6505126953125\n",
      "Epoch 4, Batch 2973, Loss: 523.6710815429688\n",
      "Epoch 4, Batch 2974, Loss: 539.5789184570312\n",
      "Epoch 4, Batch 2975, Loss: 545.0824584960938\n",
      "Epoch 4, Batch 2976, Loss: 532.603515625\n",
      "Epoch 4, Batch 2977, Loss: 517.93115234375\n",
      "Epoch 4, Batch 2978, Loss: 507.204345703125\n",
      "Epoch 4, Batch 2979, Loss: 521.4530639648438\n",
      "Epoch 4, Batch 2980, Loss: 530.1344604492188\n",
      "Epoch 4, Batch 2981, Loss: 525.4219970703125\n",
      "Epoch 4, Batch 2982, Loss: 523.418212890625\n",
      "Epoch 4, Batch 2983, Loss: 537.29638671875\n",
      "Epoch 4, Batch 2984, Loss: 504.67852783203125\n",
      "Epoch 4, Batch 2985, Loss: 520.56884765625\n",
      "Epoch 4, Batch 2986, Loss: 538.7569580078125\n",
      "Epoch 4, Batch 2987, Loss: 537.664306640625\n",
      "Epoch 4, Batch 2988, Loss: 484.6808776855469\n",
      "Epoch 4, Batch 2989, Loss: 507.3348083496094\n",
      "Epoch 4, Batch 2990, Loss: 530.4417724609375\n",
      "Epoch 4, Batch 2991, Loss: 546.1180419921875\n",
      "Epoch 4, Batch 2992, Loss: 510.3544921875\n",
      "Epoch 4, Batch 2993, Loss: 494.6224060058594\n",
      "Epoch 4, Batch 2994, Loss: 545.8203125\n",
      "Epoch 4, Batch 2995, Loss: 518.318603515625\n",
      "Epoch 4, Batch 2996, Loss: 524.4065551757812\n",
      "Epoch 4, Batch 2997, Loss: 548.5433349609375\n",
      "Epoch 4, Batch 2998, Loss: 498.00653076171875\n",
      "Epoch 4, Batch 2999, Loss: 528.12060546875\n",
      "Epoch 4, Batch 3000, Loss: 498.65692138671875\n",
      "Epoch 4, Batch 3001, Loss: 546.90478515625\n",
      "Epoch 4, Batch 3002, Loss: 513.9223022460938\n",
      "Epoch 4, Batch 3003, Loss: 538.6917724609375\n",
      "Epoch 4, Batch 3004, Loss: 525.098388671875\n",
      "Epoch 4, Batch 3005, Loss: 539.3839111328125\n",
      "Epoch 4, Batch 3006, Loss: 503.0529479980469\n",
      "Epoch 4, Batch 3007, Loss: 501.515869140625\n",
      "Epoch 4, Batch 3008, Loss: 536.3131103515625\n",
      "Epoch 4, Batch 3009, Loss: 517.866455078125\n",
      "Epoch 4, Batch 3010, Loss: 541.131591796875\n",
      "Epoch 4, Batch 3011, Loss: 500.63201904296875\n",
      "Epoch 4, Batch 3012, Loss: 549.888427734375\n",
      "Epoch 4, Batch 3013, Loss: 530.1918334960938\n",
      "Epoch 4, Batch 3014, Loss: 518.316162109375\n",
      "Epoch 4, Batch 3015, Loss: 513.1561279296875\n",
      "Epoch 4, Batch 3016, Loss: 488.60284423828125\n",
      "Epoch 4, Batch 3017, Loss: 516.8798828125\n",
      "Epoch 4, Batch 3018, Loss: 503.69891357421875\n",
      "Epoch 4, Batch 3019, Loss: 528.62646484375\n",
      "Epoch 4, Batch 3020, Loss: 498.3803405761719\n",
      "Epoch 4, Batch 3021, Loss: 493.1370544433594\n",
      "Epoch 4, Batch 3022, Loss: 499.2159118652344\n",
      "Epoch 4, Batch 3023, Loss: 497.1800537109375\n",
      "Epoch 4, Batch 3024, Loss: 504.79608154296875\n",
      "Epoch 4, Batch 3025, Loss: 530.4122314453125\n",
      "Epoch 4, Batch 3026, Loss: 507.2245788574219\n",
      "Epoch 4, Batch 3027, Loss: 500.78997802734375\n",
      "Epoch 4, Batch 3028, Loss: 530.8217163085938\n",
      "Epoch 4, Batch 3029, Loss: 497.39471435546875\n",
      "Epoch 4, Batch 3030, Loss: 550.7356567382812\n",
      "Epoch 4, Batch 3031, Loss: 508.0494384765625\n",
      "Epoch 4, Batch 3032, Loss: 524.7958374023438\n",
      "Epoch 4, Batch 3033, Loss: 534.8095703125\n",
      "Epoch 4, Batch 3034, Loss: 546.0578002929688\n",
      "Epoch 4, Batch 3035, Loss: 511.6661071777344\n",
      "Epoch 4, Batch 3036, Loss: 529.7001342773438\n",
      "Epoch 4, Batch 3037, Loss: 514.900634765625\n",
      "Epoch 4, Batch 3038, Loss: 501.9462890625\n",
      "Epoch 4, Batch 3039, Loss: 497.885986328125\n",
      "Epoch 4, Batch 3040, Loss: 512.13525390625\n",
      "Epoch 4, Batch 3041, Loss: 493.1090087890625\n",
      "Epoch 4, Batch 3042, Loss: 529.986572265625\n",
      "Epoch 4, Batch 3043, Loss: 531.6019287109375\n",
      "Epoch 4, Batch 3044, Loss: 505.76580810546875\n",
      "Epoch 4, Batch 3045, Loss: 521.340576171875\n",
      "Epoch 4, Batch 3046, Loss: 513.0882568359375\n",
      "Epoch 4, Batch 3047, Loss: 504.0814208984375\n",
      "Epoch 4, Batch 3048, Loss: 510.4589538574219\n",
      "Epoch 4, Batch 3049, Loss: 506.268310546875\n",
      "Epoch 4, Batch 3050, Loss: 517.8170776367188\n",
      "Epoch 4, Batch 3051, Loss: 527.4663696289062\n",
      "Epoch 4, Batch 3052, Loss: 506.288818359375\n",
      "Epoch 4, Batch 3053, Loss: 493.95281982421875\n",
      "Epoch 4, Batch 3054, Loss: 514.5745239257812\n",
      "Epoch 4, Batch 3055, Loss: 505.7880859375\n",
      "Epoch 4, Batch 3056, Loss: 506.97796630859375\n",
      "Epoch 4, Batch 3057, Loss: 492.7207946777344\n",
      "Epoch 4, Batch 3058, Loss: 509.558837890625\n",
      "Epoch 4, Batch 3059, Loss: 518.453857421875\n",
      "Epoch 4, Batch 3060, Loss: 519.3324584960938\n",
      "Epoch 4, Batch 3061, Loss: 488.27734375\n",
      "Epoch 4, Batch 3062, Loss: 503.3883972167969\n",
      "Epoch 4, Batch 3063, Loss: 524.3087768554688\n",
      "Epoch 4, Batch 3064, Loss: 511.840087890625\n",
      "Epoch 4, Batch 3065, Loss: 513.7240600585938\n",
      "Epoch 4, Batch 3066, Loss: 512.5513916015625\n",
      "Epoch 4, Batch 3067, Loss: 516.9075317382812\n",
      "Epoch 4, Batch 3068, Loss: 513.79248046875\n",
      "Epoch 4, Batch 3069, Loss: 531.1986694335938\n",
      "Epoch 4, Batch 3070, Loss: 478.6541442871094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ssd4t/anaconda3/envs/traffic/lib/python3.9/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([701])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, log_interval=10, checkpoint_dir='./logs'):\n",
    "    # Move model to GPU\n",
    "    device = torch.device('cuda:1')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "    start_epoch = 1\n",
    "    flag = True\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pth')  # specify the filename for checkpoint\n",
    "    \n",
    "    # Ensure checkpoint directory exists\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    # Check if checkpoint exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1  # start from next epoch\n",
    "        \n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if flag:\n",
    "                print(next(model.parameters()).is_cuda)  \n",
    "                print(inputs.is_cuda)                    \n",
    "                flag = False\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Log loss to TensorBoard\n",
    "            if batch_idx % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            print(f'Epoch {epoch}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "            # Save checkpoint periodically\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                }, checkpoint_path)\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "# Example usage (assuming model, train_loader, criterion, optimizer are defined)\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_model(model, eval_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in eval_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "        print(f'Evaluation Loss: {total_loss / len(eval_loader)}')\n",
    "\n",
    "evaluate_model(model, eval_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'predictions_only.csv', with IDs starting from 1.\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for features, in test_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        predictions.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "# Create a new DataFrame with only predictions, set index starting from 1\n",
    "predictions_df = pd.DataFrame(predictions, columns=['predicted_q'])\n",
    "predictions_df.index = predictions_df.index + 1  # Adjust index to start from 1\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(inputdir+'predictions_only.csv', index_label='ID')\n",
    "\n",
    "print(\"Predictions saved to 'predictions_only.csv', with IDs starting from 1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the baseline data\n",
    "baseline_data = pd.read_csv(inputdir+'loop_sensor_test_baseline.csv')\n",
    "\n",
    "# Prepare test data\n",
    "test_data_x['t_1h'] = pd.to_datetime(test_data_x['t_1h'])\n",
    "ref_time = pd.Timestamp('2022-01-01 00:00:00')  # Adjust this if different\n",
    "test_data_x['t_1h'] = (test_data_x['t_1h'] - ref_time).dt.total_seconds() / 3600.0\n",
    "\n",
    "# Convert test data to tensor\n",
    "test_features = torch.tensor(test_data_x[['iu_ac', 't_1h', 'etat_barre']].values).float().to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # test_predictions = model(test_features).flatten().cpu()\n",
    "    test_predictions = model(test_features)\n",
    "\n",
    "test_predictions\n",
    "# print(len(test_data_x['iu_ac']))\n",
    "# print(len(test_predictions.numpy()))\n",
    "\n",
    "# Assuming lengths are now confirmed to match, proceed to create DataFrame\n",
    "# if len(test_predictions):\n",
    "#     predictions_df = pd.DataFrame({\n",
    "#         'iu_ac': test_data_x['iu_ac'],\n",
    "#         'predicted_q': test_predictions.numpy()\n",
    "#     })\n",
    "#     # Merge and compute MAE as before\n",
    "#     result = pd.merge(baseline_data, predictions_df, on='iu_ac')\n",
    "#     mae = torch.nn.functional.l1_loss(torch.tensor(result['predicted_q'].values).float(),\n",
    "#                                       torch.tensor(result['q'].values).float())\n",
    "#     print(f'Mean Absolute Error: {mae.item()}')\n",
    "# else:\n",
    "#     print(\"Error: Mismatch in data lengths.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
